<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Neural Networks using NumPy, r-base, rTorch and PyTorch | A Minimal rTorch Book</title>
  <meta name="description" content="This is a minimal tutorial about using the rTorch package to have fun while doing machine learning. This book was written with bookdown." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Neural Networks using NumPy, r-base, rTorch and PyTorch | A Minimal rTorch Book" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal tutorial about using the rTorch package to have fun while doing machine learning. This book was written with bookdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Neural Networks using NumPy, r-base, rTorch and PyTorch | A Minimal rTorch Book" />
  
  <meta name="twitter:description" content="This is a minimal tutorial about using the rTorch package to have fun while doing machine learning. This book was written with bookdown." />
  

<meta name="author" content="Alfonso R. Reyes" />


<meta name="date" content="2020-10-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="rainfall-prediction-with-linear-regression.html"/>
<link rel="next" href="a-step-by-step-neural-network-in-rtorch.html"/>
<script src="libs/header-attrs-2.4.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.13/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>

<script>
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    // close the dropdown menu when an option is clicked
    $("#allCodeButton").dropdown("toggle");
      // R show code
      $('div.r-code-collapse').each(function() {
        $(this).collapse('show');
      });
      // Python show code
      $('div.py-code-collapse').each(function() {
        $(this).collapse('show');
      }); 
      // Bash show code
      $('div.sh-code-collapse').each(function() {
        $(this).collapse('show');
      }); 
      
  });
  $("#rmd-hide-all-code").click(function() {
      // close the dropdown menu when an option is clicked
      $("#allCodeButton").dropdown("toggle");
      // Hide R code
      $('div.r-code-collapse').each(function() {
        $(this).collapse('hide');
      });
      // Hide Python code
      $('div.py-code-collapse').each(function() {
        $(this).collapse('hide');
      });
      // Hide Bash code
      $('div.sh-code-collapse').each(function() {
        $(this).collapse('hide');
      });
  });


  // index for unique code element ids
  var r_currentIndex  = 1;   // for R code
  var py_currentIndex = 1;   // for Python code
  var sh_currentIndex  = 1;   // for shell code

  // select Python chunks
  var pyCodeBlocks = $('pre.python');
  pyCodeBlocks.each(function() {
    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse py-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'pycode-643E0F36' + py_currentIndex++;
    div.attr('id', id);
    // "this" refers the code chunk
    $(this).before(div);
    $(this).detach().appendTo(div);
    $(this).css('background-color','#ebfaeb');  // change color of chunk background
    
    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide Python code' : 'Python code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);    
        
    // change the background color of the button
    showCodeButton.css('background-color','#009900');
        
    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');
    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);
    div.before(buttonRow);    
    
    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Python code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide Python code');
    });  
   });
  
  

  // select Bash shell chunks
  var shCodeBlocks = $('pre.bash');
  shCodeBlocks.each(function() {
    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse sh-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'shcode-643E0F36' + sh_currentIndex++;
    div.attr('id', id);
    // "this" refers the code chunk
    $(this).before(div);
    $(this).detach().appendTo(div);
    $(this).css('background-color','#A0A0A0');  // change color of chunk background
    
    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide Bash code' : 'Bash code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);    
        
    // change the background color of the button
    showCodeButton.css('background-color','#cc7a00');
        
    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');
    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);
    div.before(buttonRow);    
    
    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Bash code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide Bash code');
    });  
   });  


  // select all R code blocks
  // var rCodeBlocks = $('pre.sourceCode, pre.r, pre.bash, pre.sql, pre.cpp, pre.stan');
  // adding pre.sourceCode confuses the Python button
  var rCodeBlocks = $('pre.r, pre.sql, pre.cpp, pre.stan');
  rCodeBlocks.each(function() {
    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + r_currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);
    $(this).css('background-color','#e6faff'); // change color of chunk background

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide R code' : 'R code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);
    
    // change the background color of the button        
    showCodeButton.css('background-color','#0000ff');
    
    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');
    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);
    div.before(buttonRow);

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('R code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide R code');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn {
  margin-bottom: 4px;
}

.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}

.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}

.open > .dropdown-menu {
    display: block;
    color: #ffffff;
    background-color: #ffffff;
    background-image: none;
    border-color: #92897e;
}

.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: 400;
  line-height: 1.42857143;
  color: #000000;
  white-space: nowrap;
}

.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
}

.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #ffffff;
  text-decoration: none;
  background-color: #e95420;
  outline: 0;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #aea79f;
}

.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  cursor: not-allowed;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
}

.btn {
  display: inline-block;
  margin-bottom: 1;
  font-weight: normal;
  text-align: center;
  white-space: nowrap;
  vertical-align: middle;
  -ms-touch-action: manipulation;
      touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  padding: 4px 8px;
  font-size: 14px;
  line-height: 1.42857143;
  border-radius: 4px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #ffffff;
  text-decoration: none;
}
.btn:active,
.btn.active {
  background-image: none;
  outline: 0;
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  filter: alpha(opacity=65);
  opacity: 0.65;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #ffffff;
  background-color: #aea79f; #important
  border-color: #aea79f;
}

.btn-default:focus,
.btn-default.focus {
  color: #ffffff;
  background-color: #978e83;
  border-color: #6f675e;
}

.btn-default:hover {
  color: #ffffff;
  background-color: #978e83;
  border-color: #92897e;
}
.btn-default:active,
.btn-default.active,
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-top-right-radius: 0;
  border-bottom-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-left-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-right: 8px;
  padding-left: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-right: 12px;
  padding-left: 12px;
}
.btn-group.open .dropdown-toggle {
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  box-shadow: none;
}

</style>
<script>
var str = '<div class="btn-group pull-right" style="position: fixed; right: 50px; top: 10px; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" id="allCodeButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>';
document.write(str);
</script>
<script>
$(document).ready(function () {
  // show code by default. Use "show" === "hide" to hide
  window.initializeCodeFolding("show" === "show");
});
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The rTorch Minimal Book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#installation"><i class="fa fa-check"></i>Installation</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#python-anaconda"><i class="fa fa-check"></i>Python Anaconda</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#example"><i class="fa fa-check"></i>Example</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#automatic-installation"><i class="fa fa-check"></i>Automatic installation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Getting Started</b></span></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#motivation"><i class="fa fa-check"></i><b>1.1</b> Motivation</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#how-do-we-start-using-rtorch"><i class="fa fa-check"></i><b>1.2</b> How do we start using <code>rTorch</code></a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#getting-the-pytorch-version"><i class="fa fa-check"></i><b>1.2.1</b> Getting the PyTorch version</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro.html"><a href="intro.html#pytorch-configuration"><i class="fa fa-check"></i><b>1.2.2</b> PyTorch configuration</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#what-can-you-do-with-rtorch"><i class="fa fa-check"></i><b>1.3</b> What can you do with <code>rTorch</code></a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#getting-help"><i class="fa fa-check"></i><b>1.4</b> Getting help</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html"><i class="fa fa-check"></i><b>2</b> PyTorch and NumPy</a>
<ul>
<li class="chapter" data-level="2.1" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#callable-pytorch-modules-from-rtorch"><i class="fa fa-check"></i><b>2.1</b> Callable PyTorch modules from rTorch</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#the-torchvision-module"><i class="fa fa-check"></i><b>2.1.1</b> The <code>torchvision</code> module</a></li>
<li class="chapter" data-level="2.1.2" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#the-numpy-module"><i class="fa fa-check"></i><b>2.1.2</b> The <code>numpy</code> module</a>
<ul>
<li class="chapter" data-level="" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#create-an-array"><i class="fa fa-check"></i>Create an array</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#common-array-and-tensor-operations-in-numpy-and-pytorch"><i class="fa fa-check"></i><b>2.2</b> Common array and tensor operations in NumPy and PyTorch</a>
<ul>
<li class="chapter" data-level="" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#reshape-an-array"><i class="fa fa-check"></i>Reshape an array</a></li>
<li class="chapter" data-level="" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#generate-a-random-array-in-numpy"><i class="fa fa-check"></i>Generate a random array in NumPy</a></li>
<li class="chapter" data-level="" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#generate-a-random-array-in-pytorch"><i class="fa fa-check"></i>Generate a random array in PyTorch</a></li>
<li><a href="pytorch-and-numpy.html#convert-a-numpy-array-to-a-pytorch-tensor">Convert a <code>numpy</code> array to a PyTorch tensor</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#python-built-in-functions"><i class="fa fa-check"></i><b>2.3</b> Python built-in functions</a>
<ul>
<li class="chapter" data-level="" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#length-of-a-dataset"><i class="fa fa-check"></i>Length of a dataset</a></li>
<li class="chapter" data-level="" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#iterators"><i class="fa fa-check"></i>Iterators</a></li>
<li class="chapter" data-level="" data-path="pytorch-and-numpy.html"><a href="pytorch-and-numpy.html#types-and-instances"><i class="fa fa-check"></i>Types and instances</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html"><i class="fa fa-check"></i><b>3</b> rTorch vs PyTorch: What’s different</a>
<ul>
<li class="chapter" data-level="3.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#calling-objects-from-pytorch"><i class="fa fa-check"></i><b>3.1</b> Calling objects from PyTorch</a></li>
<li class="chapter" data-level="3.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#call-modules-and-functions-from-torch"><i class="fa fa-check"></i><b>3.2</b> Call modules and functions from <code>torch</code></a></li>
<li class="chapter" data-level="3.3" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#show-the-attributes-methods-of-a-class-or-pytorch-object"><i class="fa fa-check"></i><b>3.3</b> Show the attributes (methods) of a class or PyTorch object</a></li>
<li class="chapter" data-level="3.4" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#how-to-iterate-through-datasets"><i class="fa fa-check"></i><b>3.4</b> How to iterate through datasets</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#enumeration"><i class="fa fa-check"></i><b>3.4.1</b> Enumeration</a></li>
<li class="chapter" data-level="3.4.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#using-enumerate-and-iterate"><i class="fa fa-check"></i><b>3.4.2</b> Using <code>enumerate</code> and <code>iterate</code></a></li>
<li class="chapter" data-level="3.4.3" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#using-a-for-loop-to-iterate"><i class="fa fa-check"></i><b>3.4.3</b> Using a <code>for-loop</code> to iterate</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#zero-gradient"><i class="fa fa-check"></i><b>3.5</b> Zero gradient</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#version-in-python"><i class="fa fa-check"></i><b>3.5.1</b> Version in Python</a></li>
<li class="chapter" data-level="3.5.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#version-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Version in R</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#r-generics-for-pytorch-functions"><i class="fa fa-check"></i><b>3.6</b> R generics for PyTorch functions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="converting-tensors.html"><a href="converting-tensors.html"><i class="fa fa-check"></i><b>4</b> Converting tensors</a>
<ul>
<li class="chapter" data-level="4.1" data-path="converting-tensors.html"><a href="converting-tensors.html#transforming-a-tensor-from-numpy-and-viceversa"><i class="fa fa-check"></i><b>4.1</b> Transforming a tensor from <code>numpy</code> and viceversa</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="converting-tensors.html"><a href="converting-tensors.html#convert-a-tensor-to-numpy-object"><i class="fa fa-check"></i><b>4.1.1</b> Convert a tensor to <code>numpy</code> object</a></li>
<li class="chapter" data-level="4.1.2" data-path="converting-tensors.html"><a href="converting-tensors.html#convert-a-numpy-object-to-an-r-object"><i class="fa fa-check"></i><b>4.1.2</b> Convert a <code>numpy</code> object to an <code>R</code> object</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="converting-tensors.html"><a href="converting-tensors.html#transforming-a-tensor-from-pytorch-to-r-and-viceversa"><i class="fa fa-check"></i><b>4.2</b> Transforming a tensor from PyTorch to R and viceversa</a></li>
</ul></li>
<li class="part"><span><b>II Basic Tensor Operations</b></span></li>
<li class="chapter" data-level="5" data-path="tensors.html"><a href="tensors.html"><i class="fa fa-check"></i><b>5</b> Tensors</a>
<ul>
<li class="chapter" data-level="5.1" data-path="tensors.html"><a href="tensors.html#tensor-data-types"><i class="fa fa-check"></i><b>5.1</b> Tensor data types</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="tensors.html"><a href="tensors.html#major-tensor-types"><i class="fa fa-check"></i><b>5.1.1</b> Major tensor types</a></li>
<li class="chapter" data-level="5.1.2" data-path="tensors.html"><a href="tensors.html#example-basic-attributes-of-a-4d-tensor"><i class="fa fa-check"></i><b>5.1.2</b> Example: Basic attributes of a 4D tensor</a></li>
<li class="chapter" data-level="5.1.3" data-path="tensors.html"><a href="tensors.html#example-attributes-of-a-3d-tensor"><i class="fa fa-check"></i><b>5.1.3</b> Example: Attributes of a 3D tensor</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="tensors.html"><a href="tensors.html#arithmetic-of-tensors"><i class="fa fa-check"></i><b>5.2</b> Arithmetic of tensors</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="tensors.html"><a href="tensors.html#add-tensors"><i class="fa fa-check"></i><b>5.2.1</b> Add tensors</a></li>
<li class="chapter" data-level="5.2.2" data-path="tensors.html"><a href="tensors.html#add-an-element-of-a-tensor-to-another-tensor"><i class="fa fa-check"></i><b>5.2.2</b> Add an element of a tensor to another tensor</a></li>
<li class="chapter" data-level="5.2.3" data-path="tensors.html"><a href="tensors.html#multiply-a-tensor-by-a-scalar"><i class="fa fa-check"></i><b>5.2.3</b> Multiply a tensor by a scalar</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="tensors.html"><a href="tensors.html#numpy-and-pytorch"><i class="fa fa-check"></i><b>5.3</b> NumPy and PyTorch</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="tensors.html"><a href="tensors.html#in-python-we-use-tuples-in-r-we-use-vectors"><i class="fa fa-check"></i><b>5.3.1</b> In Python we use Tuples, in R we use vectors</a></li>
<li class="chapter" data-level="5.3.2" data-path="tensors.html"><a href="tensors.html#build-a-numpy-array-from-three-r-vectors"><i class="fa fa-check"></i><b>5.3.2</b> Build a numpy array from three R vectors</a></li>
<li class="chapter" data-level="5.3.3" data-path="tensors.html"><a href="tensors.html#convert-a-numpy-array-to-a-tensor-with-as_tensor"><i class="fa fa-check"></i><b>5.3.3</b> Convert a numpy array to a tensor with <code>as_tensor()</code></a></li>
<li class="chapter" data-level="5.3.4" data-path="tensors.html"><a href="tensors.html#create-and-fill-a-tensor"><i class="fa fa-check"></i><b>5.3.4</b> Create and fill a tensor</a></li>
<li class="chapter" data-level="5.3.5" data-path="tensors.html"><a href="tensors.html#tensor-to-array-and-viceversa"><i class="fa fa-check"></i><b>5.3.5</b> Tensor to array, and viceversa</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="tensors.html"><a href="tensors.html#create-tensors"><i class="fa fa-check"></i><b>5.4</b> Create tensors</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="tensors.html"><a href="tensors.html#tensor-fill"><i class="fa fa-check"></i><b>5.4.1</b> Tensor fill</a></li>
<li class="chapter" data-level="5.4.2" data-path="tensors.html"><a href="tensors.html#initialize-tensor-with-a-range-of-values"><i class="fa fa-check"></i><b>5.4.2</b> Initialize Tensor with a range of values</a></li>
<li class="chapter" data-level="5.4.3" data-path="tensors.html"><a href="tensors.html#initialize-a-linear-or-log-scale-tensor"><i class="fa fa-check"></i><b>5.4.3</b> Initialize a linear or log scale Tensor</a></li>
<li class="chapter" data-level="5.4.4" data-path="tensors.html"><a href="tensors.html#fill-a-tensor-in-place-out-of-place"><i class="fa fa-check"></i><b>5.4.4</b> Fill a tensor In-place / Out-of-place</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="tensors.html"><a href="tensors.html#tensor-resizing"><i class="fa fa-check"></i><b>5.5</b> Tensor resizing</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="tensors.html"><a href="tensors.html#exercise"><i class="fa fa-check"></i><b>5.5.1</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="tensors.html"><a href="tensors.html#concatenate-tensors"><i class="fa fa-check"></i><b>5.6</b> Concatenate tensors</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="tensors.html"><a href="tensors.html#concatenate-tensors-by-dim0-rows"><i class="fa fa-check"></i><b>5.6.1</b> Concatenate tensors by <code>dim=0</code> (rows)</a></li>
<li class="chapter" data-level="5.6.2" data-path="tensors.html"><a href="tensors.html#concatenate-tensors-by-dim1-columns"><i class="fa fa-check"></i><b>5.6.2</b> Concatenate tensors by <code>dim=1</code> (columns)</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="tensors.html"><a href="tensors.html#reshape-tensors"><i class="fa fa-check"></i><b>5.7</b> Reshape tensors</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="tensors.html"><a href="tensors.html#with-chunk"><i class="fa fa-check"></i><b>5.7.1</b> With <code>chunk()</code>:</a>
<ul>
<li class="chapter" data-level="5.7.1.1" data-path="tensors.html"><a href="tensors.html#exercise-1"><i class="fa fa-check"></i><b>5.7.1.1</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="5.7.2" data-path="tensors.html"><a href="tensors.html#with-index_select"><i class="fa fa-check"></i><b>5.7.2</b> With <code>index_select()</code>:</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="tensors.html"><a href="tensors.html#special-tensors"><i class="fa fa-check"></i><b>5.8</b> Special tensors</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="tensors.html"><a href="tensors.html#identity-matrix"><i class="fa fa-check"></i><b>5.8.1</b> Identity matrix</a></li>
<li class="chapter" data-level="5.8.2" data-path="tensors.html"><a href="tensors.html#ones"><i class="fa fa-check"></i><b>5.8.2</b> Ones</a></li>
<li class="chapter" data-level="5.8.3" data-path="tensors.html"><a href="tensors.html#zeros"><i class="fa fa-check"></i><b>5.8.3</b> Zeros</a></li>
<li class="chapter" data-level="5.8.4" data-path="tensors.html"><a href="tensors.html#diagonal-operations"><i class="fa fa-check"></i><b>5.8.4</b> Diagonal operations</a>
<ul>
<li class="chapter" data-level="5.8.4.1" data-path="tensors.html"><a href="tensors.html#diagonal-matrix"><i class="fa fa-check"></i><b>5.8.4.1</b> Diagonal matrix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="tensors.html"><a href="tensors.html#access-to-tensor-elements"><i class="fa fa-check"></i><b>5.9</b> Access to tensor elements</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="tensors.html"><a href="tensors.html#using-indices-to-access-elements"><i class="fa fa-check"></i><b>5.9.1</b> Using indices to access elements</a></li>
<li class="chapter" data-level="5.9.2" data-path="tensors.html"><a href="tensors.html#using-the-take-function"><i class="fa fa-check"></i><b>5.9.2</b> Using the <code>take</code> function</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="tensors.html"><a href="tensors.html#other-tensor-operations"><i class="fa fa-check"></i><b>5.10</b> Other tensor operations</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="tensors.html"><a href="tensors.html#cross-product"><i class="fa fa-check"></i><b>5.10.1</b> Cross product</a></li>
<li class="chapter" data-level="5.10.2" data-path="tensors.html"><a href="tensors.html#dot-product"><i class="fa fa-check"></i><b>5.10.2</b> Dot product</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="tensors.html"><a href="tensors.html#logical-operations"><i class="fa fa-check"></i><b>5.11</b> Logical operations</a>
<ul>
<li class="chapter" data-level="5.11.1" data-path="tensors.html"><a href="tensors.html#using-a-function-to-extract-a-unique-logical-result"><i class="fa fa-check"></i><b>5.11.1</b> Using a function to extract a unique logical result</a></li>
<li class="chapter" data-level="5.11.2" data-path="tensors.html"><a href="tensors.html#greater-than-gt"><i class="fa fa-check"></i><b>5.11.2</b> Greater than (<code>gt</code>)</a></li>
<li class="chapter" data-level="5.11.3" data-path="tensors.html"><a href="tensors.html#less-than-or-equal-le"><i class="fa fa-check"></i><b>5.11.3</b> Less than or equal (<code>le</code>)</a></li>
<li class="chapter" data-level="5.11.4" data-path="tensors.html"><a href="tensors.html#logical-not"><i class="fa fa-check"></i><b>5.11.4</b> Logical NOT (!`)</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="tensors.html"><a href="tensors.html#distributions"><i class="fa fa-check"></i><b>5.12</b> Distributions</a>
<ul>
<li class="chapter" data-level="5.12.1" data-path="tensors.html"><a href="tensors.html#uniform-matrix"><i class="fa fa-check"></i><b>5.12.1</b> Uniform matrix</a></li>
<li class="chapter" data-level="5.12.2" data-path="tensors.html"><a href="tensors.html#binomial-distribution"><i class="fa fa-check"></i><b>5.12.2</b> Binomial distribution</a></li>
<li class="chapter" data-level="5.12.3" data-path="tensors.html"><a href="tensors.html#exponential-distribution"><i class="fa fa-check"></i><b>5.12.3</b> Exponential distribution</a></li>
<li class="chapter" data-level="5.12.4" data-path="tensors.html"><a href="tensors.html#weibull-distribution"><i class="fa fa-check"></i><b>5.12.4</b> Weibull distribution</a>
<ul>
<li class="chapter" data-level="5.12.4.1" data-path="tensors.html"><a href="tensors.html#weibull-at-constant-scale"><i class="fa fa-check"></i><b>5.12.4.1</b> Weibull at constant <code>scale</code></a></li>
<li class="chapter" data-level="5.12.4.2" data-path="tensors.html"><a href="tensors.html#weibull-at-constant-concentration"><i class="fa fa-check"></i><b>5.12.4.2</b> Weibull at constant <code>concentration</code></a></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linearalgebra.html"><a href="linearalgebra.html"><i class="fa fa-check"></i><b>6</b> Linear Algebra with Torch</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linearalgebra.html"><a href="linearalgebra.html#scalars"><i class="fa fa-check"></i><b>6.1</b> Scalars</a></li>
<li class="chapter" data-level="6.2" data-path="linearalgebra.html"><a href="linearalgebra.html#vectors"><i class="fa fa-check"></i><b>6.2</b> Vectors</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="linearalgebra.html"><a href="linearalgebra.html#vector-to-matrix-matrix-to-tensor"><i class="fa fa-check"></i><b>6.2.1</b> Vector to matrix, matrix to tensor</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="linearalgebra.html"><a href="linearalgebra.html#matrices"><i class="fa fa-check"></i><b>6.3</b> Matrices</a></li>
<li class="chapter" data-level="6.4" data-path="linearalgebra.html"><a href="linearalgebra.html#d-tensors"><i class="fa fa-check"></i><b>6.4</b> 3D+ tensors</a></li>
<li class="chapter" data-level="6.5" data-path="linearalgebra.html"><a href="linearalgebra.html#transpose-of-a-matrix"><i class="fa fa-check"></i><b>6.5</b> Transpose of a matrix</a></li>
<li class="chapter" data-level="6.6" data-path="linearalgebra.html"><a href="linearalgebra.html#vectors-special-case-of-a-matrix"><i class="fa fa-check"></i><b>6.6</b> Vectors, special case of a matrix</a></li>
<li class="chapter" data-level="6.7" data-path="linearalgebra.html"><a href="linearalgebra.html#tensor-arithmetic"><i class="fa fa-check"></i><b>6.7</b> Tensor arithmetic</a></li>
<li class="chapter" data-level="6.8" data-path="linearalgebra.html"><a href="linearalgebra.html#add-a-scalar-to-a-tensor"><i class="fa fa-check"></i><b>6.8</b> Add a scalar to a tensor</a></li>
<li class="chapter" data-level="6.9" data-path="linearalgebra.html"><a href="linearalgebra.html#multiplying-tensors"><i class="fa fa-check"></i><b>6.9</b> Multiplying tensors</a></li>
<li class="chapter" data-level="6.10" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product-1"><i class="fa fa-check"></i><b>6.10</b> Dot product</a>
<ul>
<li class="chapter" data-level="6.10.1" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product-of-2d-array-using-python"><i class="fa fa-check"></i><b>6.10.1</b> Dot product of 2D array using Python</a></li>
<li class="chapter" data-level="6.10.2" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product-of-2d-array-using-r"><i class="fa fa-check"></i><b>6.10.2</b> Dot product of 2D array using R</a></li>
<li class="chapter" data-level="6.10.3" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product-with-mm-and-matmul-functions"><i class="fa fa-check"></i><b>6.10.3</b> Dot product with <code>mm</code> and <code>matmul</code> functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="creating-pytorch-classes.html"><a href="creating-pytorch-classes.html"><i class="fa fa-check"></i><b>7</b> Creating PyTorch classes</a>
<ul>
<li class="chapter" data-level="7.1" data-path="creating-pytorch-classes.html"><a href="creating-pytorch-classes.html#build-a-pytorch-model-class"><i class="fa fa-check"></i><b>7.1</b> Build a PyTorch model class</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="creating-pytorch-classes.html"><a href="creating-pytorch-classes.html#example-1-a-neural-network-with-one-layer"><i class="fa fa-check"></i><b>7.1.1</b> Example 1: a neural network with one layer</a></li>
<li class="chapter" data-level="7.1.2" data-path="creating-pytorch-classes.html"><a href="creating-pytorch-classes.html#example-2-logistic-regression"><i class="fa fa-check"></i><b>7.1.2</b> Example 2: Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Logistic Regression</b></span></li>
<li class="chapter" data-level="8" data-path="example-1-a-classification-problem-with-logistic-regression.html"><a href="example-1-a-classification-problem-with-logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Example 1: A classification problem with Logistic Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="example-1-a-classification-problem-with-logistic-regression.html"><a href="example-1-a-classification-problem-with-logistic-regression.html#code-in-python"><i class="fa fa-check"></i><b>8.1</b> Code in Python</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="mnistdigits.html"><a href="mnistdigits.html"><i class="fa fa-check"></i><b>9</b> Example 2: MNIST handwritten digits</a>
<ul>
<li class="chapter" data-level="9.1" data-path="mnistdigits.html"><a href="mnistdigits.html#code-in-r"><i class="fa fa-check"></i><b>9.1</b> Code in R</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="mnistdigits.html"><a href="mnistdigits.html#hyperparameters"><i class="fa fa-check"></i><b>9.1.1</b> Hyperparameters</a></li>
<li class="chapter" data-level="9.1.2" data-path="mnistdigits.html"><a href="mnistdigits.html#read-datasets"><i class="fa fa-check"></i><b>9.1.2</b> Read datasets</a></li>
<li class="chapter" data-level="9.1.3" data-path="mnistdigits.html"><a href="mnistdigits.html#define-the-model"><i class="fa fa-check"></i><b>9.1.3</b> Define the model</a></li>
<li class="chapter" data-level="9.1.4" data-path="mnistdigits.html"><a href="mnistdigits.html#training"><i class="fa fa-check"></i><b>9.1.4</b> Training</a></li>
<li class="chapter" data-level="9.1.5" data-path="mnistdigits.html"><a href="mnistdigits.html#prediction"><i class="fa fa-check"></i><b>9.1.5</b> Prediction</a></li>
<li class="chapter" data-level="9.1.6" data-path="mnistdigits.html"><a href="mnistdigits.html#save-the-model"><i class="fa fa-check"></i><b>9.1.6</b> Save the model</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="mnistdigits.html"><a href="mnistdigits.html#code-in-python-1"><i class="fa fa-check"></i><b>9.2</b> Code in Python</a></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="10" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>10</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="linear-regression.html"><a href="linear-regression.html#introduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="linear-regression.html"><a href="linear-regression.html#generate-the-dataset"><i class="fa fa-check"></i><b>10.2</b> Generate the dataset</a></li>
<li class="chapter" data-level="10.3" data-path="linear-regression.html"><a href="linear-regression.html#convert-arrays-to-tensors"><i class="fa fa-check"></i><b>10.3</b> Convert arrays to tensors</a></li>
<li class="chapter" data-level="10.4" data-path="linear-regression.html"><a href="linear-regression.html#converting-from-numpy-to-tensor"><i class="fa fa-check"></i><b>10.4</b> Converting from numpy to tensor</a></li>
<li class="chapter" data-level="10.5" data-path="linear-regression.html"><a href="linear-regression.html#creating-the-network-model"><i class="fa fa-check"></i><b>10.5</b> Creating the network model</a></li>
<li class="chapter" data-level="10.6" data-path="linear-regression.html"><a href="linear-regression.html#optimizer-and-loss"><i class="fa fa-check"></i><b>10.6</b> Optimizer and Loss</a></li>
<li class="chapter" data-level="10.7" data-path="linear-regression.html"><a href="linear-regression.html#training-1"><i class="fa fa-check"></i><b>10.7</b> Training</a></li>
<li class="chapter" data-level="10.8" data-path="linear-regression.html"><a href="linear-regression.html#results"><i class="fa fa-check"></i><b>10.8</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html"><i class="fa fa-check"></i><b>11</b> Rainfall prediction with Linear Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#training-data"><i class="fa fa-check"></i><b>11.1</b> Training data</a></li>
<li class="chapter" data-level="11.2" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#convert-arrays-to-tensors-1"><i class="fa fa-check"></i><b>11.2</b> Convert arrays to tensors</a></li>
<li class="chapter" data-level="11.3" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#build-the-model"><i class="fa fa-check"></i><b>11.3</b> Build the model</a></li>
<li class="chapter" data-level="11.4" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#generate-predictions"><i class="fa fa-check"></i><b>11.4</b> Generate predictions</a></li>
<li class="chapter" data-level="11.5" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#loss-function"><i class="fa fa-check"></i><b>11.5</b> Loss Function</a></li>
<li class="chapter" data-level="11.6" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#step-by-step-process"><i class="fa fa-check"></i><b>11.6</b> Step by step process</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#compute-the-losses"><i class="fa fa-check"></i><b>11.6.1</b> Compute the losses</a></li>
<li class="chapter" data-level="11.6.2" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#compute-gradients"><i class="fa fa-check"></i><b>11.6.2</b> Compute Gradients</a></li>
<li class="chapter" data-level="11.6.3" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#reset-the-gradients"><i class="fa fa-check"></i><b>11.6.3</b> Reset the gradients</a>
<ul>
<li class="chapter" data-level="11.6.3.1" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#adjust-weights-and-biases-using-gradient-descent"><i class="fa fa-check"></i><b>11.6.3.1</b> Adjust weights and biases using gradient descent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="rainfall-prediction-with-linear-regression.html"><a href="rainfall-prediction-with-linear-regression.html#all-together-train-for-multiple-epochs"><i class="fa fa-check"></i><b>11.7</b> All together: train for multiple epochs</a></li>
</ul></li>
<li class="part"><span><b>V Neural Networks</b></span></li>
<li class="chapter" data-level="12" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><i class="fa fa-check"></i><b>12</b> Neural Networks using NumPy, r-base, rTorch and PyTorch</a>
<ul>
<li class="chapter" data-level="12.1" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#a-neural-network-with-numpy"><i class="fa fa-check"></i><b>12.1</b> A neural network with <code>numpy</code></a></li>
<li class="chapter" data-level="12.2" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#a-neural-network-with-r-base"><i class="fa fa-check"></i><b>12.2</b> A neural network with <code>r-base</code></a></li>
<li class="chapter" data-level="12.3" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#the-neural-network-written-in-pytorch"><i class="fa fa-check"></i><b>12.3</b> The neural network written in <code>PyTorch</code></a></li>
<li class="chapter" data-level="12.4" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#a-neural-network-written-in-rtorch"><i class="fa fa-check"></i><b>12.4</b> A neural network written in <code>rTorch</code></a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#load-the-libraries"><i class="fa fa-check"></i><b>12.4.1</b> Load the libraries</a></li>
<li class="chapter" data-level="12.4.2" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#dataset"><i class="fa fa-check"></i><b>12.4.2</b> Dataset</a></li>
<li class="chapter" data-level="12.4.3" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#initialize-the-weights"><i class="fa fa-check"></i><b>12.4.3</b> Initialize the weights</a></li>
<li class="chapter" data-level="12.4.4" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#iterate-through-the-dataset"><i class="fa fa-check"></i><b>12.4.4</b> Iterate through the dataset</a>
<ul>
<li class="chapter" data-level="12.4.4.1" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#iterate-50-times"><i class="fa fa-check"></i><b>12.4.4.1</b> Iterate 50 times</a></li>
<li class="chapter" data-level="12.4.4.2" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#a-function-to-train-the-neural-network"><i class="fa fa-check"></i><b>12.4.4.2</b> A function to train the neural network</a></li>
<li class="chapter" data-level="12.4.4.3" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#run-it-at-100-iterations"><i class="fa fa-check"></i><b>12.4.4.3</b> Run it at 100 iterations</a></li>
<li class="chapter" data-level="12.4.4.4" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#iterations"><i class="fa fa-check"></i><b>12.4.4.4</b> 250 iterations</a></li>
<li class="chapter" data-level="12.4.4.5" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#iterations-1"><i class="fa fa-check"></i><b>12.4.4.5</b> 500 iterations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#exercise-2"><i class="fa fa-check"></i><b>12.5</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html"><i class="fa fa-check"></i><b>13</b> A step-by-step neural network in rTorch</a>
<ul>
<li class="chapter" data-level="13.1" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#introduction-1"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#select-device"><i class="fa fa-check"></i><b>13.2</b> Select device</a></li>
<li class="chapter" data-level="13.3" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#create-the-dataset"><i class="fa fa-check"></i><b>13.3</b> Create the dataset</a></li>
<li class="chapter" data-level="13.4" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#define-the-model-1"><i class="fa fa-check"></i><b>13.4</b> Define the model</a></li>
<li class="chapter" data-level="13.5" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#the-loss-function"><i class="fa fa-check"></i><b>13.5</b> The Loss function</a></li>
<li class="chapter" data-level="13.6" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#iterate-through-dataset"><i class="fa fa-check"></i><b>13.6</b> Iterate through dataset</a></li>
<li class="chapter" data-level="13.7" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#using-r-generics-to-simplify-tensor-operations"><i class="fa fa-check"></i><b>13.7</b> Using R generics to simplify tensor operations</a></li>
<li class="chapter" data-level="13.8" data-path="a-step-by-step-neural-network-in-rtorch.html"><a href="a-step-by-step-neural-network-in-rtorch.html#a-more-elegant-way-of-writing-the-neural-network"><i class="fa fa-check"></i><b>13.8</b> A more elegant way of writing the neural network</a></li>
</ul></li>
<li class="part"><span><b>VI PyTorch and R data structures</b></span></li>
<li class="chapter" data-level="14" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html"><i class="fa fa-check"></i><b>14</b> Working with data.frame</a>
<ul>
<li class="chapter" data-level="14.1" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#load-pytorch-libraries"><i class="fa fa-check"></i><b>14.1</b> Load PyTorch libraries</a></li>
<li class="chapter" data-level="14.2" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#load-dataset"><i class="fa fa-check"></i><b>14.2</b> Load dataset</a></li>
<li class="chapter" data-level="14.3" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#summary-statistics-for-tensors"><i class="fa fa-check"></i><b>14.3</b> Summary statistics for tensors</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#using-data.frame"><i class="fa fa-check"></i><b>14.3.1</b> using <code>data.frame</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="working-with-data-table.html"><a href="working-with-data-table.html"><i class="fa fa-check"></i><b>15</b> Working with data.table</a>
<ul>
<li class="chapter" data-level="15.1" data-path="working-with-data-table.html"><a href="working-with-data-table.html#load-pytorch-libraries-1"><i class="fa fa-check"></i><b>15.1</b> Load PyTorch libraries</a></li>
<li class="chapter" data-level="15.2" data-path="working-with-data-table.html"><a href="working-with-data-table.html#load-dataset-1"><i class="fa fa-check"></i><b>15.2</b> Load dataset</a></li>
<li class="chapter" data-level="15.3" data-path="working-with-data-table.html"><a href="working-with-data-table.html#read-the-datasets-without-normalization"><i class="fa fa-check"></i><b>15.3</b> Read the datasets without normalization</a></li>
<li class="chapter" data-level="15.4" data-path="working-with-data-table.html"><a href="working-with-data-table.html#using-data.table"><i class="fa fa-check"></i><b>15.4</b> Using <code>data.table</code></a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixA.html"><a href="appendixA.html"><i class="fa fa-check"></i><b>A</b> Statistical Background</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appendixA.html"><a href="appendixA.html#basic-statistical-terms"><i class="fa fa-check"></i><b>A.1</b> Basic statistical terms</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appendixA.html"><a href="appendixA.html#five-number-summary"><i class="fa fa-check"></i><b>A.1.1</b> Five-number summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixB.html"><a href="appendixB.html"><i class="fa fa-check"></i><b>B</b> Activation Functions</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendixB.html"><a href="appendixB.html#the-sigmoid-function"><i class="fa fa-check"></i><b>B.1</b> The Sigmoid function</a></li>
<li class="chapter" data-level="B.2" data-path="appendixB.html"><a href="appendixB.html#the-relu-function"><i class="fa fa-check"></i><b>B.2</b> The ReLU function</a></li>
<li class="chapter" data-level="B.3" data-path="appendixB.html"><a href="appendixB.html#the-tanh-function"><i class="fa fa-check"></i><b>B.3</b> The tanh function</a></li>
<li class="chapter" data-level="B.4" data-path="appendixB.html"><a href="appendixB.html#the-softmax-activation-function"><i class="fa fa-check"></i><b>B.4</b> The Softmax Activation function</a></li>
<li class="chapter" data-level="B.5" data-path="appendixB.html"><a href="appendixB.html#coding-your-own-activation-functions-in-python"><i class="fa fa-check"></i><b>B.5</b> Coding your own activation functions in Python</a>
<ul>
<li class="chapter" data-level="" data-path="appendixB.html"><a href="appendixB.html#linear-activation"><i class="fa fa-check"></i>Linear activation</a></li>
<li class="chapter" data-level="" data-path="appendixB.html"><a href="appendixB.html#sigmoid-activation"><i class="fa fa-check"></i>Sigmoid activation</a></li>
<li class="chapter" data-level="" data-path="appendixB.html"><a href="appendixB.html#hyperbolic-tangent-activation"><i class="fa fa-check"></i>Hyperbolic Tangent activation</a></li>
<li class="chapter" data-level="" data-path="appendixB.html"><a href="appendixB.html#rectifier-linear-unit-relu"><i class="fa fa-check"></i>Rectifier linear unit (ReLU)</a></li>
<li><a href="appendixB.html#visualization-with-matplotlib">Visualization with <code>matplotlib</code></a></li>
</ul></li>
<li class="chapter" data-level="B.6" data-path="appendixB.html"><a href="appendixB.html#softmax-in-python"><i class="fa fa-check"></i><b>B.6</b> Softmax in Python</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Minimal rTorch Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="neural-networks-using-numpy-r-base-rtorch-and-pytorch" class="section level1" number="12">
<h1><span class="header-section-number">Chapter 12</span> Neural Networks using NumPy, r-base, rTorch and PyTorch</h1>
<p>We will compare three neural networks:</p>
<ul>
<li><p>a neural network written in <code>numpy</code></p></li>
<li><p>a neural network written in <code>r-base</code></p></li>
<li><p>a neural network written in <code>PyTorch</code></p></li>
<li><p>a neural network written in <code>rTorch</code></p></li>
</ul>
<div id="a-neural-network-with-numpy" class="section level2" number="12.1">
<h2><span class="header-section-number">12.1</span> A neural network with <code>numpy</code></h2>
<p>We start the neural network by simply using <code>numpy</code>:</p>
<div class="sourceCode" id="cb507"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb507-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A simple neural network using NumPy</span></span>
<span id="cb507-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Code in file tensor/two_layer_net_numpy.py</span></span>
<span id="cb507-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb507-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb507-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb507-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-6" aria-hidden="true" tabindex="-1"></a>tic <span class="op">=</span> time.process_time()</span>
<span id="cb507-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb507-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)   <span class="co"># set a seed for reproducibility</span></span>
<span id="cb507-9"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-9" aria-hidden="true" tabindex="-1"></a><span class="co"># N is batch size; D_in is input dimension;</span></span>
<span id="cb507-10"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-10" aria-hidden="true" tabindex="-1"></a><span class="co"># H is hidden dimension; D_out is output dimension.</span></span>
<span id="cb507-11"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-11" aria-hidden="true" tabindex="-1"></a>N, D_in, H, D_out <span class="op">=</span> <span class="dv">64</span>, <span class="dv">1000</span>, <span class="dv">100</span>, <span class="dv">10</span></span>
<span id="cb507-12"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb507-13"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random input and output data</span></span>
<span id="cb507-14"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-14" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.randn(N, D_in)</span>
<span id="cb507-15"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.randn(N, D_out)</span>
<span id="cb507-16"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-16" aria-hidden="true" tabindex="-1"></a><span class="co"># print(x.shape)</span></span>
<span id="cb507-17"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-17" aria-hidden="true" tabindex="-1"></a><span class="co"># print(y.shape)</span></span>
<span id="cb507-18"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb507-19"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-19" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> np.random.randn(D_in, H)</span>
<span id="cb507-20"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-20" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> np.random.randn(H, D_out)</span>
<span id="cb507-21"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-21" aria-hidden="true" tabindex="-1"></a><span class="co"># print(w1.shape)</span></span>
<span id="cb507-22"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-22" aria-hidden="true" tabindex="-1"></a><span class="co"># print(w2.shape)</span></span>
<span id="cb507-23"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb507-24"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-24" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb507-25"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb507-26"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Forward pass: compute predicted y</span></span>
<span id="cb507-27"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-27" aria-hidden="true" tabindex="-1"></a>  h <span class="op">=</span> x.dot(w1)</span>
<span id="cb507-28"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(t, h.max())</span></span>
<span id="cb507-29"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-29" aria-hidden="true" tabindex="-1"></a>  h_relu <span class="op">=</span> np.maximum(h, <span class="dv">0</span>)</span>
<span id="cb507-30"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-30" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="op">=</span> h_relu.dot(w2)</span>
<span id="cb507-31"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb507-32"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-32" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute and print loss</span></span>
<span id="cb507-33"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-33" aria-hidden="true" tabindex="-1"></a>  sq <span class="op">=</span> np.square(y_pred <span class="op">-</span> y)</span>
<span id="cb507-34"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-34" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> sq.<span class="bu">sum</span>()</span>
<span id="cb507-35"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-35" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(t, loss)</span>
<span id="cb507-36"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-36" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb507-37"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-37" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span>
<span id="cb507-38"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-38" aria-hidden="true" tabindex="-1"></a>  grad_y_pred <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> (y_pred <span class="op">-</span> y)</span>
<span id="cb507-39"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-39" aria-hidden="true" tabindex="-1"></a>  grad_w2 <span class="op">=</span> h_relu.T.dot(grad_y_pred)</span>
<span id="cb507-40"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-40" aria-hidden="true" tabindex="-1"></a>  grad_h_relu <span class="op">=</span> grad_y_pred.dot(w2.T)</span>
<span id="cb507-41"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-41" aria-hidden="true" tabindex="-1"></a>  grad_h <span class="op">=</span> grad_h_relu.copy()</span>
<span id="cb507-42"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-42" aria-hidden="true" tabindex="-1"></a>  grad_h[h <span class="op">&lt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb507-43"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-43" aria-hidden="true" tabindex="-1"></a>  grad_w1 <span class="op">=</span> x.T.dot(grad_h)</span>
<span id="cb507-44"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-44" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb507-45"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-45" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update weights</span></span>
<span id="cb507-46"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-46" aria-hidden="true" tabindex="-1"></a>  w1 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w1</span>
<span id="cb507-47"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-47" aria-hidden="true" tabindex="-1"></a>  w2 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w2</span>
<span id="cb507-48"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb507-48" aria-hidden="true" tabindex="-1"></a><span class="co"># processing time  </span></span></code></pre></div>
<pre><code>#&gt; 0 28624200.80093851
#&gt; 1 24402861.381040633
#&gt; 2 23157437.291475512
#&gt; 3 21617191.633971732
#&gt; 4 18598190.361558586
#&gt; 5 14198211.41969284
#&gt; 6 9786244.45261814
#&gt; 7 6233451.217340665
#&gt; 8 3862647.2678296
#&gt; 9 2412366.6327648377
#&gt; 10 1569915.4392193719
#&gt; 11 1078501.338148753
#&gt; 12 785163.9233288623
#&gt; 13 601495.2825043732
#&gt; 14 479906.0403613453
#&gt; 15 394555.1933174624
#&gt; 16 331438.6987273828
#&gt; 17 282679.6687873872
#&gt; 18 243807.84432087577
#&gt; 19 211970.1811070823
#&gt; 20 185451.68615142742
#&gt; 21 163078.20881862933
#&gt; 22 144011.80160918707
#&gt; 23 127662.96132466738
#&gt; 24 113546.29175681758
#&gt; 25 101291.55288493488
#&gt; 26 90623.2083365488
#&gt; 27 81307.3259069289
#&gt; 28 73135.24710426925
#&gt; 29 65937.50294095621
#&gt; 30 59570.264253680405
#&gt; 31 53923.82804264214
#&gt; 32 48909.69273028217
#&gt; 33 44438.89933807683
#&gt; 34 40445.340315697315
#&gt; 35 36873.300419894105
#&gt; 36 33664.990437423796
#&gt; 37 30781.198962949482
#&gt; 38 28184.24227268409
#&gt; 39 25843.997931081976
#&gt; 40 23727.282448406404
#&gt; 41 21810.062067327617
#&gt; 42 20071.326437572203
#&gt; 43 18492.63752543332
#&gt; 44 17056.72779714253
#&gt; 45 15749.299484025263
#&gt; 46 14557.324481207244
#&gt; 47 13468.469764337975
#&gt; 48 12473.575866914001
#&gt; 49 11562.485809665823
#&gt; 50 10727.865926563425
#&gt; 51 9962.4113728161
#&gt; 52 9259.619803682264
#&gt; 53 8613.269071227103
#&gt; 54 8018.523834750735
#&gt; 55 7471.080819104454
#&gt; 56 6966.0065184565265
#&gt; 57 6499.966854225839
#&gt; 58 6069.576425345407
#&gt; 59 5671.28212284083
#&gt; 60 5302.64498008629
#&gt; 61 4961.339043761746
#&gt; 62 4645.025414234535
#&gt; 63 4351.473575805076
#&gt; 64 4079.2165446062886
#&gt; 65 3826.148082088759
#&gt; 66 3590.887308956795
#&gt; 67 3372.0103280622634
#&gt; 68 3168.1734086507527
#&gt; 69 2978.3621000816793
#&gt; 70 2801.3026490979832
#&gt; 71 2636.037950790896
#&gt; 72 2481.7354010452723
#&gt; 73 2337.6093944873373
#&gt; 74 2202.825042568387
#&gt; 75 2076.8872560589457
#&gt; 76 1958.9976460120315
#&gt; 77 1848.5060338548442
#&gt; 78 1744.999338082472
#&gt; 79 1647.9807349258722
#&gt; 80 1556.9947585282348
#&gt; 81 1471.7081797400365
#&gt; 82 1391.6136870762475
#&gt; 83 1316.3329239757234
#&gt; 84 1245.5902641069808
#&gt; 85 1179.0691783286152
#&gt; 86 1116.509520952858
#&gt; 87 1057.6662051951444
#&gt; 88 1002.251968682358
#&gt; 89 950.0167505993277
#&gt; 90 900.7916929993592
#&gt; 91 854.3816389576887
#&gt; 92 810.6277767708937
#&gt; 93 769.359204134852
#&gt; 94 730.3836012940129
#&gt; 95 693.5644048073445
#&gt; 96 658.7807027999572
#&gt; 97 625.9238747325836
#&gt; 98 594.8758111694992
#&gt; 99 565.4973547949343
#&gt; 100 537.7012178149585
#&gt; 101 511.3901106843997
#&gt; 102 486.4837276215498
#&gt; 103 462.9074695545851
#&gt; 104 440.57876228874807
#&gt; 105 419.41212313923995
#&gt; 106 399.34612374956964
#&gt; 107 380.32217772728114
#&gt; 108 362.2821345456041
#&gt; 109 345.1804975712006
#&gt; 110 328.9402861597722
#&gt; 111 313.51912062711443
#&gt; 112 298.87547706727497
#&gt; 113 284.96926791620365
#&gt; 114 271.76429845268774
#&gt; 115 259.2246266311524
#&gt; 116 247.30122156532178
#&gt; 117 235.96203976771284
#&gt; 118 225.178741845231
#&gt; 119 214.92539698060864
#&gt; 120 205.1691616882672
#&gt; 121 195.8892001432394
#&gt; 122 187.052215013267
#&gt; 123 178.64288738757915
#&gt; 124 170.6347989732484
#&gt; 125 163.00806018890742
#&gt; 126 155.74401913460838
#&gt; 127 148.8335289811081
#&gt; 128 142.2496666996883
#&gt; 129 135.9750912283429
#&gt; 130 129.9898261242878
#&gt; 131 124.2841886577775
#&gt; 132 118.84482149781245
#&gt; 133 113.65645952102648
#&gt; 134 108.70543970080944
#&gt; 135 103.98144604071936
#&gt; 136 99.47512083366092
#&gt; 137 95.17318303450631
#&gt; 138 91.06775169947753
#&gt; 139 87.1495259294625
#&gt; 140 83.4075554849762
#&gt; 141 79.83335532838096
#&gt; 142 76.41993249926696
#&gt; 143 73.15953167860118
#&gt; 144 70.04535899921332
#&gt; 145 67.07000377138765
#&gt; 146 64.22536514818634
#&gt; 147 61.50715956099494
#&gt; 148 58.90970110703893
#&gt; 149 56.42818157299062
#&gt; 150 54.0534563439752
#&gt; 151 51.784098992504774
#&gt; 152 49.61304222205953
#&gt; 153 47.53708868183348
#&gt; 154 45.550739513747104
#&gt; 155 43.651385230775844
#&gt; 156 41.833382882033526
#&gt; 157 40.09449255768935
#&gt; 158 38.430465576899365
#&gt; 159 36.83773398481139
#&gt; 160 35.313368600585356
#&gt; 161 33.854369284339015
#&gt; 162 32.45799709272589
#&gt; 163 31.120973836570375
#&gt; 164 29.841057186482367
#&gt; 165 28.615366313659155
#&gt; 166 27.441646501923387
#&gt; 167 26.317677128114568
#&gt; 168 25.24106573435136
#&gt; 169 24.21056866875246
#&gt; 170 23.223366825889713
#&gt; 171 22.276914475964166
#&gt; 172 21.37056177702837
#&gt; 173 20.502013041054887
#&gt; 174 19.669605151003026
#&gt; 175 18.872156637146674
#&gt; 176 18.107932697664374
#&gt; 177 17.37534709306374
#&gt; 178 16.673297052412618
#&gt; 179 16.000313127916463
#&gt; 180 15.355056259808652
#&gt; 181 14.736642044314724
#&gt; 182 14.143657665389998
#&gt; 183 13.575482981168946
#&gt; 184 13.030557920726439
#&gt; 185 12.507813624902791
#&gt; 186 12.006508479643612
#&gt; 187 11.525873890625103
#&gt; 188 11.064924569594794
#&gt; 189 10.62284512860161
#&gt; 190 10.19922427874796
#&gt; 191 9.79248532294338
#&gt; 192 9.402215377695029
#&gt; 193 9.027996925838622
#&gt; 194 8.668895520242348
#&gt; 195 8.324385761674124
#&gt; 196 7.993908670660823
#&gt; 197 7.676665609325071
#&gt; 198 7.372299100127828
#&gt; 199 7.080233920966754
#&gt; 200 6.799940598001485
#&gt; 201 6.530984430179325
#&gt; 202 6.27288786879498
#&gt; 203 6.025197539285538
#&gt; 204 5.787473375781904
#&gt; 205 5.559253501791473
#&gt; 206 5.340172472449478
#&gt; 207 5.129896948041127
#&gt; 208 4.928007606816015
#&gt; 209 4.734225282678773
#&gt; 210 4.548186858907007
#&gt; 211 4.369651328446803
#&gt; 212 4.1982364576470985
#&gt; 213 4.03356501113859
#&gt; 214 3.8754625080282983
#&gt; 215 3.723691411552273
#&gt; 216 3.5779627242856717
#&gt; 217 3.4379821914237114
#&gt; 218 3.3035655875402
#&gt; 219 3.174454405800358
#&gt; 220 3.050474307039242
#&gt; 221 2.9313837093171027
#&gt; 222 2.8170418304756346
#&gt; 223 2.7072412196041817
#&gt; 224 2.601727700086947
#&gt; 225 2.500404091219077
#&gt; 226 2.403078781570885
#&gt; 227 2.3095944818352514
#&gt; 228 2.219794799730771
#&gt; 229 2.1335266786377156
#&gt; 230 2.050676042360689
#&gt; 231 1.9710453639291998
#&gt; 232 1.894559024311108
#&gt; 233 1.8211210547719694
#&gt; 234 1.7505340383434302
#&gt; 235 1.6826932948726598
#&gt; 236 1.6175070289512319
#&gt; 237 1.5549072300348497
#&gt; 238 1.4947316986694295
#&gt; 239 1.436912502601004
#&gt; 240 1.3813729879464858
#&gt; 241 1.32798542050412
#&gt; 242 1.2766884038686654
#&gt; 243 1.2273848146334736
#&gt; 244 1.1800217450319552
#&gt; 245 1.1344919105886015
#&gt; 246 1.0907369940976221
#&gt; 247 1.048682623569126
#&gt; 248 1.008265620640105
#&gt; 249 0.9694282665758402
#&gt; 250 0.9320976601575863
#&gt; 251 0.8962339607475193
#&gt; 252 0.861753386590558
#&gt; 253 0.8286151485835402
#&gt; 254 0.7967578289855202
#&gt; 255 0.7661404678427008
#&gt; 256 0.7367202044069405
#&gt; 257 0.7084227136677593
#&gt; 258 0.6812311487720661
#&gt; 259 0.6550822696784752
#&gt; 260 0.6299469090212146
#&gt; 261 0.6057869953554655
#&gt; 262 0.5825650778278454
#&gt; 263 0.560238214093596
#&gt; 264 0.5387735503108713
#&gt; 265 0.5181403816558078
#&gt; 266 0.49830590931288704
#&gt; 267 0.4792293730809791
#&gt; 268 0.46088901492658985
#&gt; 269 0.44325464817124605
#&gt; 270 0.4263040840612994
#&gt; 271 0.41000543380652404
#&gt; 272 0.3943367329584973
#&gt; 273 0.37927114581518934
#&gt; 274 0.36478176529467893
#&gt; 275 0.3508504444514621
#&gt; 276 0.3374578361160551
#&gt; 277 0.32457682402471333
#&gt; 278 0.31219123729926995
#&gt; 279 0.3002965861471759
#&gt; 280 0.28884848624103193
#&gt; 281 0.27783526470540487
#&gt; 282 0.2672448769700397
#&gt; 283 0.257061810692966
#&gt; 284 0.2472693951467931
#&gt; 285 0.23785306876444096
#&gt; 286 0.2287964823126848
#&gt; 287 0.2200890964310116
#&gt; 288 0.2117131852610878
#&gt; 289 0.2036578219832887
#&gt; 290 0.1959113399380713
#&gt; 291 0.18846041746510409
#&gt; 292 0.1812947700716973
#&gt; 293 0.17440531516160357
#&gt; 294 0.1677799812083831
#&gt; 295 0.16140610523823434
#&gt; 296 0.15527565017156653
#&gt; 297 0.14937904644540106
#&gt; 298 0.143707930394666
#&gt; 299 0.13825290527817413
#&gt; 300 0.13300640130437155
#&gt; 301 0.12796012311329652
#&gt; 302 0.12310750541654694
#&gt; 303 0.11844182274750006
#&gt; 304 0.11395158652039863
#&gt; 305 0.10963187686683722
#&gt; 306 0.10547640155931667
#&gt; 307 0.10148022089421871
#&gt; 308 0.09763637993288563
#&gt; 309 0.09393976586808896
#&gt; 310 0.09038186218006347
#&gt; 311 0.08696004033324446
#&gt; 312 0.08366808215680896
#&gt; 313 0.08050159133383622
#&gt; 314 0.07745565072654795
#&gt; 315 0.07452541616804072
#&gt; 316 0.07170677388799906
#&gt; 317 0.0689949238891718
#&gt; 318 0.06638632065316974
#&gt; 319 0.06387707772652872
#&gt; 320 0.06146291085118909
#&gt; 321 0.05914022943971027
#&gt; 322 0.05690662209830684
#&gt; 323 0.05475707395743522
#&gt; 324 0.052689449069882516
#&gt; 325 0.0506998454506518
#&gt; 326 0.04878568859795683
#&gt; 327 0.04694479519756138
#&gt; 328 0.04517396661890381
#&gt; 329 0.04346938274989057
#&gt; 330 0.04182932192093554
#&gt; 331 0.04025154186790747
#&gt; 332 0.038733588417535325
#&gt; 333 0.03727299017397479
#&gt; 334 0.03586799441056368
#&gt; 335 0.03451589218269463
#&gt; 336 0.03321501089196713
#&gt; 337 0.03196371785315193
#&gt; 338 0.030759357425255512
#&gt; 339 0.029600888472506824
#&gt; 340 0.028485919148247208
#&gt; 341 0.027413172250666702
#&gt; 342 0.026380963791971412
#&gt; 343 0.02538782827695241
#&gt; 344 0.024432256369794788
#&gt; 345 0.023512794719542034
#&gt; 346 0.022628153928014032
#&gt; 347 0.02177684408441299
#&gt; 348 0.02095765200802166
#&gt; 349 0.020169474661596593
#&gt; 350 0.01941096289570436
#&gt; 351 0.018681045066723634
#&gt; 352 0.017978879513485046
#&gt; 353 0.017303468563149377
#&gt; 354 0.016653437842283743
#&gt; 355 0.01602766278434931
#&gt; 356 0.015425464893053308
#&gt; 357 0.014845946789042726
#&gt; 358 0.01428824985022298
#&gt; 359 0.013751635754278416
#&gt; 360 0.013235286650491312
#&gt; 361 0.012738339025971655
#&gt; 362 0.0122601869182692
#&gt; 363 0.011799970856207088
#&gt; 364 0.011357085981177944
#&gt; 365 0.010930950268791513
#&gt; 366 0.010520842685042622
#&gt; 367 0.01012614583012008
#&gt; 368 0.009746393154866176
#&gt; 369 0.009380889339541617
#&gt; 370 0.009029161386732537
#&gt; 371 0.00869059833701307
#&gt; 372 0.008364772077009364
#&gt; 373 0.008051209390688818
#&gt; 374 0.007749432506980628
#&gt; 375 0.007459023266141868
#&gt; 376 0.007179590434342686
#&gt; 377 0.006910623445872005
#&gt; 378 0.0066517499415919105
#&gt; 379 0.006402648026668485
#&gt; 380 0.006162978285335637
#&gt; 381 0.005932194796397628
#&gt; 382 0.0057100850523133465
#&gt; 383 0.005496310244889655
#&gt; 384 0.005290628924128056
#&gt; 385 0.00509262416882405
#&gt; 386 0.004902076613035799
#&gt; 387 0.004718638851161897
#&gt; 388 0.004542078962055229
#&gt; 389 0.004372164586674141
#&gt; 390 0.0042086186268486744
#&gt; 391 0.004051226677918435
#&gt; 392 0.003899737449492815
#&gt; 393 0.003753918301517942
#&gt; 394 0.0036135618379381004
#&gt; 395 0.003478478691764194
#&gt; 396 0.0033484625756204194
#&gt; 397 0.0032233273622630336
#&gt; 398 0.003102863549092089
#&gt; 399 0.002986912218199622
#&gt; 400 0.0028753481463720115
#&gt; 401 0.0027679524720228606
#&gt; 402 0.0026645903413001857
#&gt; 403 0.002565067280110832
#&gt; 404 0.002469270189885967
#&gt; 405 0.002377067171876515
#&gt; 406 0.0022883091777458524
#&gt; 407 0.002202926988989529
#&gt; 408 0.002120737936892398
#&gt; 409 0.0020415781423083032
#&gt; 410 0.0019653808381894892
#&gt; 411 0.0018920388674690015
#&gt; 412 0.0018214489876562734
#&gt; 413 0.0017534990549491567
#&gt; 414 0.0016880979054327092
#&gt; 415 0.0016251364192797018
#&gt; 416 0.0015645343026935593
#&gt; 417 0.0015062064772054069
#&gt; 418 0.0014500530088167006
#&gt; 419 0.001395986809735361
#&gt; 420 0.0013439464213948061
#&gt; 421 0.0012938496041315591
#&gt; 422 0.0012456223977539646
#&gt; 423 0.0011992050880636534
#&gt; 424 0.0011545283489988243
#&gt; 425 0.00111150758568053
#&gt; 426 0.0010701006705381246
#&gt; 427 0.0010302364937631399
#&gt; 428 0.0009918591300854913
#&gt; 429 0.000954924393229692
#&gt; 430 0.0009193639132851613
#&gt; 431 0.000885130846793718
#&gt; 432 0.0008521777959402742
#&gt; 433 0.0008204570911806855
#&gt; 434 0.0007899223397663538
#&gt; 435 0.0007605278374248271
#&gt; 436 0.0007322343466923758
#&gt; 437 0.0007049830913988117
#&gt; 438 0.0006787512341427114
#&gt; 439 0.0006535021203749922
#&gt; 440 0.0006291921955325687
#&gt; 441 0.0006057856348304279
#&gt; 442 0.0005832525024862874
#&gt; 443 0.0005615598539463627
#&gt; 444 0.0005406761235212546
#&gt; 445 0.0005205750249308841
#&gt; 446 0.0005012184845930873
#&gt; 447 0.00048258480282314676
#&gt; 448 0.00046464475752567284
#&gt; 449 0.00044737394619139524
#&gt; 450 0.00043075137592208436
#&gt; 451 0.00041474810355465676
#&gt; 452 0.0003993358048136131
#&gt; 453 0.0003844970781217258
#&gt; 454 0.00037021092506796444
#&gt; 455 0.000356459486191319
#&gt; 456 0.00034322132236759437
#&gt; 457 0.0003304723731924654
#&gt; 458 0.00031819830164123673
#&gt; 459 0.0003063812179908789
#&gt; 460 0.00029500453534664166
#&gt; 461 0.00028405331305463857
#&gt; 462 0.00027350873727132553
#&gt; 463 0.00026335657398047466
#&gt; 464 0.0002535812583676579
#&gt; 465 0.00024416913722500146
#&gt; 466 0.0002351142689434037
#&gt; 467 0.0002263919313751717
#&gt; 468 0.0002179925767375779
#&gt; 469 0.0002099042754011281
#&gt; 470 0.00020211745069089414
#&gt; 471 0.0001946205404420093
#&gt; 472 0.00018740325426796242
#&gt; 473 0.0001804525224963492
#&gt; 474 0.00017375996054049412
#&gt; 475 0.00016731630060453707
#&gt; 476 0.00016111227107184903
#&gt; 477 0.00015513993832780905
#&gt; 478 0.00014938925941842613
#&gt; 479 0.00014385207870840213
#&gt; 480 0.00013852014130335906
#&gt; 481 0.00013338601187803598
#&gt; 482 0.00012844279329313077
#&gt; 483 0.0001236841045649647
#&gt; 484 0.00011910150087052596
#&gt; 485 0.00011468967274778241
#&gt; 486 0.0001104405800218931
#&gt; 487 0.00010634983744801036
#&gt; 488 0.00010241132940383641
#&gt; 489 9.861901302497896e-05
#&gt; 490 9.496682985463865e-05
#&gt; 491 9.144989846228939e-05
#&gt; 492 8.806354488360765e-05
#&gt; 493 8.480312707843102e-05
#&gt; 494 8.166404591760756e-05
#&gt; 495 7.864135637129015e-05
#&gt; 496 7.573027442833174e-05
#&gt; 497 7.29278760308855e-05
#&gt; 498 7.023030228292749e-05
#&gt; 499 6.763183953436163e-05</code></pre>
<div class="sourceCode" id="cb509"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb509-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb509-1" aria-hidden="true" tabindex="-1"></a>toc <span class="op">=</span> time.process_time()</span>
<span id="cb509-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb509-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(toc <span class="op">-</span> tic, <span class="st">&quot;seconds&quot;</span>)</span></code></pre></div>
<pre><code>#&gt; 4.439214920000005 seconds</code></pre>
</div>
<div id="a-neural-network-with-r-base" class="section level2" number="12.2">
<h2><span class="header-section-number">12.2</span> A neural network with <code>r-base</code></h2>
<p>It is the same algorithm above in <code>numpy</code> but written in R base.</p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tictoc)</span>
<span id="cb511-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb511-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-3" aria-hidden="true" tabindex="-1"></a><span class="fu">tic</span>()</span>
<span id="cb511-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb511-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-5" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">64</span>; D_in <span class="ot">&lt;-</span> <span class="dv">1000</span>; H <span class="ot">&lt;-</span> <span class="dv">100</span>; D_out <span class="ot">&lt;-</span> <span class="dv">10</span>;</span>
<span id="cb511-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random input and output data</span></span>
<span id="cb511-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">rnorm</span>(N <span class="sc">*</span> D_in),  <span class="at">dim =</span> <span class="fu">c</span>(N, D_in))</span>
<span id="cb511-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">rnorm</span>(N <span class="sc">*</span> D_out), <span class="at">dim =</span> <span class="fu">c</span>(N, D_out))</span>
<span id="cb511-9"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly initialize weights</span></span>
<span id="cb511-10"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-10" aria-hidden="true" tabindex="-1"></a>w1 <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">rnorm</span>(D_in <span class="sc">*</span> H),  <span class="at">dim =</span> <span class="fu">c</span>(D_in, H))</span>
<span id="cb511-11"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-11" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">rnorm</span>(H <span class="sc">*</span> D_out),  <span class="at">dim =</span> <span class="fu">c</span>(H, D_out))</span>
<span id="cb511-12"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-12" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="ot">&lt;-</span>  <span class="fl">1e-6</span></span>
<span id="cb511-13"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb511-14"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">500</span>)) {</span>
<span id="cb511-15"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Forward pass: compute predicted y</span></span>
<span id="cb511-16"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-16" aria-hidden="true" tabindex="-1"></a>  h <span class="ot">=</span> x <span class="sc">%*%</span> w1</span>
<span id="cb511-17"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-17" aria-hidden="true" tabindex="-1"></a>  h_relu <span class="ot">=</span> <span class="fu">pmax</span>(h, <span class="dv">0</span>)</span>
<span id="cb511-18"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-18" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="ot">=</span> h_relu <span class="sc">%*%</span> w2</span>
<span id="cb511-19"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb511-20"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute and print loss</span></span>
<span id="cb511-21"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-21" aria-hidden="true" tabindex="-1"></a>  sq <span class="ot">&lt;-</span> (y_pred <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb511-22"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-22" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">sum</span>(sq)</span>
<span id="cb511-23"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(t, loss, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb511-24"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb511-25"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span>
<span id="cb511-26"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-26" aria-hidden="true" tabindex="-1"></a>  grad_y_pred <span class="ot">=</span> <span class="fl">2.0</span> <span class="sc">*</span> (y_pred <span class="sc">-</span> y)</span>
<span id="cb511-27"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-27" aria-hidden="true" tabindex="-1"></a>  grad_w2 <span class="ot">=</span> <span class="fu">t</span>(h_relu) <span class="sc">%*%</span> grad_y_pred</span>
<span id="cb511-28"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-28" aria-hidden="true" tabindex="-1"></a>  grad_h_relu <span class="ot">=</span> grad_y_pred <span class="sc">%*%</span> <span class="fu">t</span>(w2)</span>
<span id="cb511-29"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># grad_h &lt;- sapply(grad_h_relu, function(i) i, simplify = FALSE )   # grad_h = grad_h_relu.copy()</span></span>
<span id="cb511-30"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-30" aria-hidden="true" tabindex="-1"></a>  grad_h <span class="ot">&lt;-</span> rlang<span class="sc">::</span><span class="fu">duplicate</span>(grad_h_relu)</span>
<span id="cb511-31"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-31" aria-hidden="true" tabindex="-1"></a>  grad_h[h <span class="sc">&lt;</span> <span class="dv">0</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb511-32"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-32" aria-hidden="true" tabindex="-1"></a>  grad_w1 <span class="ot">=</span> <span class="fu">t</span>(x) <span class="sc">%*%</span> grad_h</span>
<span id="cb511-33"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-33" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb511-34"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update weights</span></span>
<span id="cb511-35"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-35" aria-hidden="true" tabindex="-1"></a>  w1 <span class="ot">=</span> w1 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_w1</span>
<span id="cb511-36"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-36" aria-hidden="true" tabindex="-1"></a>  w2 <span class="ot">=</span> w2 <span class="sc">-</span> learning_rate <span class="sc">*</span> grad_w2</span>
<span id="cb511-37"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-37" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb511-38"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb511-38" aria-hidden="true" tabindex="-1"></a><span class="fu">toc</span>()</span></code></pre></div>
<pre><code>#&gt; 1 2.8e+07 
#&gt; 2 25505803 
#&gt; 3 29441299 
#&gt; 4 35797650 
#&gt; 5 39517126 
#&gt; 6 34884942 
#&gt; 7 23333535 
#&gt; 8 11927525 
#&gt; 9 5352787 
#&gt; 10 2496984 
#&gt; 11 1379780 
#&gt; 12 918213 
#&gt; 13 695760 
#&gt; 14 564974 
#&gt; 15 474479 
#&gt; 16 405370 
#&gt; 17 349747 
#&gt; 18 303724 
#&gt; 19 265075 
#&gt; 20 232325 
#&gt; 21 204394 
#&gt; 22 180414 
#&gt; 23 159752 
#&gt; 24 141895 
#&gt; 25 126374 
#&gt; 26 112820 
#&gt; 27 100959 
#&gt; 28 90536 
#&gt; 29 81352 
#&gt; 30 73244 
#&gt; 31 66058 
#&gt; 32 59675 
#&gt; 33 53993 
#&gt; 34 48921 
#&gt; 35 44388 
#&gt; 36 40328 
#&gt; 37 36687 
#&gt; 38 33414 
#&gt; 39 30469 
#&gt; 40 27816 
#&gt; 41 25419 
#&gt; 42 23251 
#&gt; 43 21288 
#&gt; 44 19508 
#&gt; 45 17893 
#&gt; 46 16426 
#&gt; 47 15092 
#&gt; 48 13877 
#&gt; 49 12769 
#&gt; 50 11758 
#&gt; 51 10835 
#&gt; 52 9991 
#&gt; 53 9218 
#&gt; 54 8510 
#&gt; 55 7862 
#&gt; 56 7267 
#&gt; 57 6719 
#&gt; 58 6217 
#&gt; 59 5754 
#&gt; 60 5329 
#&gt; 61 4938 
#&gt; 62 4577 
#&gt; 63 4245 
#&gt; 64 3938 
#&gt; 65 3655 
#&gt; 66 3394 
#&gt; 67 3153 
#&gt; 68 2930 
#&gt; 69 2724 
#&gt; 70 2533 
#&gt; 71 2357 
#&gt; 72 2193 
#&gt; 73 2042 
#&gt; 74 1902 
#&gt; 75 1772 
#&gt; 76 1651 
#&gt; 77 1539 
#&gt; 78 1435 
#&gt; 79 1338 
#&gt; 80 1249 
#&gt; 81 1165 
#&gt; 82 1088 
#&gt; 83 1016 
#&gt; 84 949 
#&gt; 85 886 
#&gt; 86 828 
#&gt; 87 774 
#&gt; 88 724 
#&gt; 89 677 
#&gt; 90 633 
#&gt; 91 592 
#&gt; 92 554 
#&gt; 93 519 
#&gt; 94 486 
#&gt; 95 455 
#&gt; 96 426 
#&gt; 97 399 
#&gt; 98 374 
#&gt; 99 350 
#&gt; 100 328 
#&gt; 101 308 
#&gt; 102 289 
#&gt; 103 271 
#&gt; 104 254 
#&gt; 105 238 
#&gt; 106 224 
#&gt; 107 210 
#&gt; 108 197 
#&gt; 109 185 
#&gt; 110 174 
#&gt; 111 163 
#&gt; 112 153 
#&gt; 113 144 
#&gt; 114 135 
#&gt; 115 127 
#&gt; 116 119 
#&gt; 117 112 
#&gt; 118 106 
#&gt; 119 99.2 
#&gt; 120 93.3 
#&gt; 121 87.8 
#&gt; 122 82.6 
#&gt; 123 77.7 
#&gt; 124 73.1 
#&gt; 125 68.8 
#&gt; 126 64.7 
#&gt; 127 60.9 
#&gt; 128 57.4 
#&gt; 129 54 
#&gt; 130 50.9 
#&gt; 131 47.9 
#&gt; 132 45.1 
#&gt; 133 42.5 
#&gt; 134 40.1 
#&gt; 135 37.8 
#&gt; 136 35.6 
#&gt; 137 33.5 
#&gt; 138 31.6 
#&gt; 139 29.8 
#&gt; 140 28.1 
#&gt; 141 26.5 
#&gt; 142 25 
#&gt; 143 23.6 
#&gt; 144 22.2 
#&gt; 145 21 
#&gt; 146 19.8 
#&gt; 147 18.7 
#&gt; 148 17.6 
#&gt; 149 16.6 
#&gt; 150 15.7 
#&gt; 151 14.8 
#&gt; 152 14 
#&gt; 153 13.2 
#&gt; 154 12.5 
#&gt; 155 11.8 
#&gt; 156 11.1 
#&gt; 157 10.5 
#&gt; 158 9.94 
#&gt; 159 9.39 
#&gt; 160 8.87 
#&gt; 161 8.38 
#&gt; 162 7.92 
#&gt; 163 7.49 
#&gt; 164 7.08 
#&gt; 165 6.69 
#&gt; 166 6.32 
#&gt; 167 5.98 
#&gt; 168 5.65 
#&gt; 169 5.35 
#&gt; 170 5.06 
#&gt; 171 4.78 
#&gt; 172 4.52 
#&gt; 173 4.28 
#&gt; 174 4.05 
#&gt; 175 3.83 
#&gt; 176 3.62 
#&gt; 177 3.43 
#&gt; 178 3.25 
#&gt; 179 3.07 
#&gt; 180 2.91 
#&gt; 181 2.75 
#&gt; 182 2.6 
#&gt; 183 2.47 
#&gt; 184 2.33 
#&gt; 185 2.21 
#&gt; 186 2.09 
#&gt; 187 1.98 
#&gt; 188 1.88 
#&gt; 189 1.78 
#&gt; 190 1.68 
#&gt; 191 1.6 
#&gt; 192 1.51 
#&gt; 193 1.43 
#&gt; 194 1.36 
#&gt; 195 1.29 
#&gt; 196 1.22 
#&gt; 197 1.15 
#&gt; 198 1.09 
#&gt; 199 1.04 
#&gt; 200 0.983 
#&gt; 201 0.932 
#&gt; 202 0.883 
#&gt; 203 0.837 
#&gt; 204 0.794 
#&gt; 205 0.753 
#&gt; 206 0.714 
#&gt; 207 0.677 
#&gt; 208 0.642 
#&gt; 209 0.609 
#&gt; 210 0.577 
#&gt; 211 0.548 
#&gt; 212 0.519 
#&gt; 213 0.493 
#&gt; 214 0.467 
#&gt; 215 0.443 
#&gt; 216 0.421 
#&gt; 217 0.399 
#&gt; 218 0.379 
#&gt; 219 0.359 
#&gt; 220 0.341 
#&gt; 221 0.324 
#&gt; 222 0.307 
#&gt; 223 0.292 
#&gt; 224 0.277 
#&gt; 225 0.263 
#&gt; 226 0.249 
#&gt; 227 0.237 
#&gt; 228 0.225 
#&gt; 229 0.213 
#&gt; 230 0.203 
#&gt; 231 0.192 
#&gt; 232 0.183 
#&gt; 233 0.173 
#&gt; 234 0.165 
#&gt; 235 0.156 
#&gt; 236 0.149 
#&gt; 237 0.141 
#&gt; 238 0.134 
#&gt; 239 0.127 
#&gt; 240 0.121 
#&gt; 241 0.115 
#&gt; 242 0.109 
#&gt; 243 0.104 
#&gt; 244 0.0985 
#&gt; 245 0.0936 
#&gt; 246 0.0889 
#&gt; 247 0.0845 
#&gt; 248 0.0803 
#&gt; 249 0.0763 
#&gt; 250 0.0725 
#&gt; 251 0.0689 
#&gt; 252 0.0655 
#&gt; 253 0.0623 
#&gt; 254 0.0592 
#&gt; 255 0.0563 
#&gt; 256 0.0535 
#&gt; 257 0.0508 
#&gt; 258 0.0483 
#&gt; 259 0.0459 
#&gt; 260 0.0437 
#&gt; 261 0.0415 
#&gt; 262 0.0395 
#&gt; 263 0.0375 
#&gt; 264 0.0357 
#&gt; 265 0.0339 
#&gt; 266 0.0323 
#&gt; 267 0.0307 
#&gt; 268 0.0292 
#&gt; 269 0.0278 
#&gt; 270 0.0264 
#&gt; 271 0.0251 
#&gt; 272 0.0239 
#&gt; 273 0.0227 
#&gt; 274 0.0216 
#&gt; 275 0.0206 
#&gt; 276 0.0196 
#&gt; 277 0.0186 
#&gt; 278 0.0177 
#&gt; 279 0.0168 
#&gt; 280 0.016 
#&gt; 281 0.0152 
#&gt; 282 0.0145 
#&gt; 283 0.0138 
#&gt; 284 0.0131 
#&gt; 285 0.0125 
#&gt; 286 0.0119 
#&gt; 287 0.0113 
#&gt; 288 0.0108 
#&gt; 289 0.0102 
#&gt; 290 0.00975 
#&gt; 291 0.00927 
#&gt; 292 0.00883 
#&gt; 293 0.0084 
#&gt; 294 0.008 
#&gt; 295 0.00761 
#&gt; 296 0.00724 
#&gt; 297 0.0069 
#&gt; 298 0.00656 
#&gt; 299 0.00625 
#&gt; 300 0.00595 
#&gt; 301 0.00566 
#&gt; 302 0.00539 
#&gt; 303 0.00513 
#&gt; 304 0.00489 
#&gt; 305 0.00465 
#&gt; 306 0.00443 
#&gt; 307 0.00422 
#&gt; 308 0.00401 
#&gt; 309 0.00382 
#&gt; 310 0.00364 
#&gt; 311 0.00347 
#&gt; 312 0.0033 
#&gt; 313 0.00314 
#&gt; 314 0.00299 
#&gt; 315 0.00285 
#&gt; 316 0.00271 
#&gt; 317 0.00259 
#&gt; 318 0.00246 
#&gt; 319 0.00234 
#&gt; 320 0.00223 
#&gt; 321 0.00213 
#&gt; 322 0.00203 
#&gt; 323 0.00193 
#&gt; 324 0.00184 
#&gt; 325 0.00175 
#&gt; 326 0.00167 
#&gt; 327 0.00159 
#&gt; 328 0.00151 
#&gt; 329 0.00144 
#&gt; 330 0.00137 
#&gt; 331 0.00131 
#&gt; 332 0.00125 
#&gt; 333 0.00119 
#&gt; 334 0.00113 
#&gt; 335 0.00108 
#&gt; 336 0.00103 
#&gt; 337 0.000979 
#&gt; 338 0.000932 
#&gt; 339 0.000888 
#&gt; 340 0.000846 
#&gt; 341 0.000807 
#&gt; 342 0.000768 
#&gt; 343 0.000732 
#&gt; 344 0.000698 
#&gt; 345 0.000665 
#&gt; 346 0.000634 
#&gt; 347 0.000604 
#&gt; 348 0.000575 
#&gt; 349 0.000548 
#&gt; 350 0.000523 
#&gt; 351 0.000498 
#&gt; 352 0.000475 
#&gt; 353 0.000452 
#&gt; 354 0.000431 
#&gt; 355 0.000411 
#&gt; 356 0.000392 
#&gt; 357 0.000373 
#&gt; 358 0.000356 
#&gt; 359 0.000339 
#&gt; 360 0.000323 
#&gt; 361 0.000308 
#&gt; 362 0.000294 
#&gt; 363 0.00028 
#&gt; 364 0.000267 
#&gt; 365 0.000254 
#&gt; 366 0.000243 
#&gt; 367 0.000231 
#&gt; 368 0.00022 
#&gt; 369 0.00021 
#&gt; 370 2e-04 
#&gt; 371 0.000191 
#&gt; 372 0.000182 
#&gt; 373 0.000174 
#&gt; 374 0.000165 
#&gt; 375 0.000158 
#&gt; 376 0.00015 
#&gt; 377 0.000143 
#&gt; 378 0.000137 
#&gt; 379 0.00013 
#&gt; 380 0.000124 
#&gt; 381 0.000119 
#&gt; 382 0.000113 
#&gt; 383 0.000108 
#&gt; 384 0.000103 
#&gt; 385 9.8e-05 
#&gt; 386 9.34e-05 
#&gt; 387 8.91e-05 
#&gt; 388 8.49e-05 
#&gt; 389 8.1e-05 
#&gt; 390 7.72e-05 
#&gt; 391 7.37e-05 
#&gt; 392 7.02e-05 
#&gt; 393 6.7e-05 
#&gt; 394 6.39e-05 
#&gt; 395 6.09e-05 
#&gt; 396 5.81e-05 
#&gt; 397 5.54e-05 
#&gt; 398 5.28e-05 
#&gt; 399 5.04e-05 
#&gt; 400 4.81e-05 
#&gt; 401 4.58e-05 
#&gt; 402 4.37e-05 
#&gt; 403 4.17e-05 
#&gt; 404 3.98e-05 
#&gt; 405 3.79e-05 
#&gt; 406 3.62e-05 
#&gt; 407 3.45e-05 
#&gt; 408 3.29e-05 
#&gt; 409 3.14e-05 
#&gt; 410 2.99e-05 
#&gt; 411 2.86e-05 
#&gt; 412 2.72e-05 
#&gt; 413 2.6e-05 
#&gt; 414 2.48e-05 
#&gt; 415 2.36e-05 
#&gt; 416 2.25e-05 
#&gt; 417 2.15e-05 
#&gt; 418 2.05e-05 
#&gt; 419 1.96e-05 
#&gt; 420 1.87e-05 
#&gt; 421 1.78e-05 
#&gt; 422 1.7e-05 
#&gt; 423 1.62e-05 
#&gt; 424 1.55e-05 
#&gt; 425 1.48e-05 
#&gt; 426 1.41e-05 
#&gt; 427 1.34e-05 
#&gt; 428 1.28e-05 
#&gt; 429 1.22e-05 
#&gt; 430 1.17e-05 
#&gt; 431 1.11e-05 
#&gt; 432 1.06e-05 
#&gt; 433 1.01e-05 
#&gt; 434 9.66e-06 
#&gt; 435 9.22e-06 
#&gt; 436 8.79e-06 
#&gt; 437 8.39e-06 
#&gt; 438 8e-06 
#&gt; 439 7.64e-06 
#&gt; 440 7.29e-06 
#&gt; 441 6.95e-06 
#&gt; 442 6.63e-06 
#&gt; 443 6.33e-06 
#&gt; 444 6.04e-06 
#&gt; 445 5.76e-06 
#&gt; 446 5.5e-06 
#&gt; 447 5.25e-06 
#&gt; 448 5.01e-06 
#&gt; 449 4.78e-06 
#&gt; 450 4.56e-06 
#&gt; 451 4.35e-06 
#&gt; 452 4.15e-06 
#&gt; 453 3.96e-06 
#&gt; 454 3.78e-06 
#&gt; 455 3.61e-06 
#&gt; 456 3.44e-06 
#&gt; 457 3.28e-06 
#&gt; 458 3.13e-06 
#&gt; 459 2.99e-06 
#&gt; 460 2.85e-06 
#&gt; 461 2.72e-06 
#&gt; 462 2.6e-06 
#&gt; 463 2.48e-06 
#&gt; 464 2.37e-06 
#&gt; 465 2.26e-06 
#&gt; 466 2.15e-06 
#&gt; 467 2.06e-06 
#&gt; 468 1.96e-06 
#&gt; 469 1.87e-06 
#&gt; 470 1.79e-06 
#&gt; 471 1.71e-06 
#&gt; 472 1.63e-06 
#&gt; 473 1.55e-06 
#&gt; 474 1.48e-06 
#&gt; 475 1.42e-06 
#&gt; 476 1.35e-06 
#&gt; 477 1.29e-06 
#&gt; 478 1.23e-06 
#&gt; 479 1.17e-06 
#&gt; 480 1.12e-06 
#&gt; 481 1.07e-06 
#&gt; 482 1.02e-06 
#&gt; 483 9.74e-07 
#&gt; 484 9.3e-07 
#&gt; 485 8.88e-07 
#&gt; 486 8.47e-07 
#&gt; 487 8.09e-07 
#&gt; 488 7.72e-07 
#&gt; 489 7.37e-07 
#&gt; 490 7.03e-07 
#&gt; 491 6.71e-07 
#&gt; 492 6.41e-07 
#&gt; 493 6.12e-07 
#&gt; 494 5.84e-07 
#&gt; 495 5.57e-07 
#&gt; 496 5.32e-07 
#&gt; 497 5.08e-07 
#&gt; 498 4.85e-07 
#&gt; 499 4.63e-07 
#&gt; 500 4.42e-07 
#&gt; 4.668 sec elapsed</code></pre>
</div>
<div id="the-neural-network-written-in-pytorch" class="section level2" number="12.3">
<h2><span class="header-section-number">12.3</span> The neural network written in <code>PyTorch</code></h2>
<p>Here is the same example we have used above but written in PyTorch. Notice the following differences with the <code>numpy</code> code:</p>
<ul>
<li><p>we select the computation device which could be <code>cpu</code> or <code>gpu</code></p></li>
<li><p>when building or creating the tensors, we specify which device we want to use</p></li>
<li><p>the tensors have <code>torch</code> methods and properties. Example: <code>mm()</code>, <code>clamp()</code>, <code>sum()</code>, <code>clone()</code>, and <code>t()</code>,</p></li>
<li><p>also notice the use some <code>torch</code> functions: <code>device()</code>, <code>randn()</code></p></li>
</ul>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb513-1" aria-hidden="true" tabindex="-1"></a>reticulate<span class="sc">::</span><span class="fu">use_condaenv</span>(<span class="st">&quot;r-torch&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb514"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb514-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Code in file tensor/two_layer_net_tensor.py</span></span>
<span id="cb514-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb514-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb514-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb514-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-5" aria-hidden="true" tabindex="-1"></a>ms <span class="op">=</span> torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb514-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-6" aria-hidden="true" tabindex="-1"></a>tic <span class="op">=</span> time.process_time()</span>
<span id="cb514-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-7" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb514-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-8" aria-hidden="true" tabindex="-1"></a><span class="co"># device = torch.device(&#39;cuda&#39;)  # Uncomment this to run on GPU</span></span>
<span id="cb514-9"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb514-10"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-10" aria-hidden="true" tabindex="-1"></a><span class="co"># N is batch size; D_in is input dimension;</span></span>
<span id="cb514-11"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-11" aria-hidden="true" tabindex="-1"></a><span class="co"># H is hidden dimension; D_out is output dimension.</span></span>
<span id="cb514-12"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-12" aria-hidden="true" tabindex="-1"></a>N, D_in, H, D_out <span class="op">=</span> <span class="dv">64</span>, <span class="dv">1000</span>, <span class="dv">100</span>, <span class="dv">10</span></span>
<span id="cb514-13"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb514-14"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random input and output data</span></span>
<span id="cb514-15"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-15" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(N, D_in, device<span class="op">=</span>device)</span>
<span id="cb514-16"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-16" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.randn(N, D_out, device<span class="op">=</span>device)</span>
<span id="cb514-17"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb514-18"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly initialize weights</span></span>
<span id="cb514-19"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-19" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> torch.randn(D_in, H, device<span class="op">=</span>device)</span>
<span id="cb514-20"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-20" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> torch.randn(H, D_out, device<span class="op">=</span>device)</span>
<span id="cb514-21"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb514-22"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-22" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb514-23"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb514-24"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Forward pass: compute predicted y</span></span>
<span id="cb514-25"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-25" aria-hidden="true" tabindex="-1"></a>  h <span class="op">=</span> x.mm(w1)</span>
<span id="cb514-26"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-26" aria-hidden="true" tabindex="-1"></a>  h_relu <span class="op">=</span> h.clamp(<span class="bu">min</span><span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb514-27"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-27" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="op">=</span> h_relu.mm(w2)</span>
<span id="cb514-28"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb514-29"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor</span></span>
<span id="cb514-30"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-30" aria-hidden="true" tabindex="-1"></a>  <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></span>
<span id="cb514-31"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-31" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> (y_pred <span class="op">-</span> y).<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb514-32"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-32" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(t, loss.item())</span>
<span id="cb514-33"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb514-34"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span>
<span id="cb514-35"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-35" aria-hidden="true" tabindex="-1"></a>  grad_y_pred <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> (y_pred <span class="op">-</span> y)</span>
<span id="cb514-36"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-36" aria-hidden="true" tabindex="-1"></a>  grad_w2 <span class="op">=</span> h_relu.t().mm(grad_y_pred)</span>
<span id="cb514-37"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-37" aria-hidden="true" tabindex="-1"></a>  grad_h_relu <span class="op">=</span> grad_y_pred.mm(w2.t())</span>
<span id="cb514-38"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-38" aria-hidden="true" tabindex="-1"></a>  grad_h <span class="op">=</span> grad_h_relu.clone()</span>
<span id="cb514-39"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-39" aria-hidden="true" tabindex="-1"></a>  grad_h[h <span class="op">&lt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb514-40"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-40" aria-hidden="true" tabindex="-1"></a>  grad_w1 <span class="op">=</span> x.t().mm(grad_h)</span>
<span id="cb514-41"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb514-42"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-42" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update weights using gradient descent</span></span>
<span id="cb514-43"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-43" aria-hidden="true" tabindex="-1"></a>  w1 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w1</span>
<span id="cb514-44"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb514-44" aria-hidden="true" tabindex="-1"></a>  w2 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w2</span></code></pre></div>
<pre><code>#&gt; 0 28304951.678202875
#&gt; 1 24927328.6639303
#&gt; 2 28155855.77058386
#&gt; 3 34180357.719430685
#&gt; 4 37707746.887789354
#&gt; 5 33620991.028499454
#&gt; 6 22446553.70772245
#&gt; 7 11618280.121770754
#&gt; 8 5223431.478905056
#&gt; 9 2461507.5224659974
#&gt; 10 1379540.558178643
#&gt; 11 932839.5714197141
#&gt; 12 715554.0194977624
#&gt; 13 585999.3830106166
#&gt; 14 495224.1190599696
#&gt; 15 425139.8269296169
#&gt; 16 368213.26971499936
#&gt; 17 320861.36850887095
#&gt; 18 280855.4664459439
#&gt; 19 246786.8849917458
#&gt; 20 217605.04415180004
#&gt; 21 192469.10582625505
#&gt; 22 170743.82020579863
#&gt; 23 151851.94095434336
#&gt; 24 135381.09682470877
#&gt; 25 120989.87510382346
#&gt; 26 108377.08193979261
#&gt; 27 97286.46466266317
#&gt; 28 87516.74262182202
#&gt; 29 78871.40974253973
#&gt; 30 71194.55619418155
#&gt; 31 64368.69806881405
#&gt; 32 58290.88589673794
#&gt; 33 52862.53039132413
#&gt; 34 48006.48628965374
#&gt; 35 43648.17638706032
#&gt; 36 39733.12183144018
#&gt; 37 36212.83667739256
#&gt; 38 33041.3423372085
#&gt; 39 30180.249173055217
#&gt; 40 27596.031029894235
#&gt; 41 25257.71896952727
#&gt; 42 23140.477484791758
#&gt; 43 21219.215332345837
#&gt; 44 19477.346011970214
#&gt; 45 17898.942492450115
#&gt; 46 16461.186144096366
#&gt; 47 15150.468945112118
#&gt; 48 13954.32764792479
#&gt; 49 12862.049952206942
#&gt; 50 11863.846542866391
#&gt; 51 10950.190923462585
#&gt; 52 10112.94426786356
#&gt; 53 9346.057705583586
#&gt; 54 8643.227456215845
#&gt; 55 7998.013709383486
#&gt; 56 7404.314143395906
#&gt; 57 6858.340342363264
#&gt; 58 6355.823822117182
#&gt; 59 5893.272718725863
#&gt; 60 5467.590824799911
#&gt; 61 5075.314900617862
#&gt; 62 4713.257357968152
#&gt; 63 4378.928521878623
#&gt; 64 4070.0899099030976
#&gt; 65 3784.681538189922
#&gt; 66 3520.732192065661
#&gt; 67 3276.4374575280553
#&gt; 68 3050.1573582667443
#&gt; 69 2840.7435683049735
#&gt; 70 2646.8554702835545
#&gt; 71 2467.1301658974303
#&gt; 72 2300.493116013281
#&gt; 73 2145.7384163982897
#&gt; 74 2002.0645278499069
#&gt; 75 1868.62585232437
#&gt; 76 1744.5305134660546
#&gt; 77 1629.2088434191014
#&gt; 78 1521.9212637440608
#&gt; 79 1422.070202881318
#&gt; 80 1329.1314050876533
#&gt; 81 1242.57843915715
#&gt; 82 1161.9741527800966
#&gt; 83 1086.859274942149
#&gt; 84 1016.8590222123014
#&gt; 85 951.566250754007
#&gt; 86 890.6602544816294
#&gt; 87 833.8443278601629
#&gt; 88 780.8578032641242
#&gt; 89 731.3973374911332
#&gt; 90 685.1947512043134
#&gt; 91 642.044298154148
#&gt; 92 601.7328625418818
#&gt; 93 564.0571957786619
#&gt; 94 528.8261654125919
#&gt; 95 495.9025536938795
#&gt; 96 465.09740183895497
#&gt; 97 436.2913070884209
#&gt; 98 409.31979498319464
#&gt; 99 384.0735885326187
#&gt; 100 360.44638041822657
#&gt; 101 338.3392483462631
#&gt; 102 317.6202138550293
#&gt; 103 298.2113941729458
#&gt; 104 280.034373801912
#&gt; 105 263.0044766552596
#&gt; 106 247.03986887883838
#&gt; 107 232.07821803150009
#&gt; 108 218.048818247877
#&gt; 109 204.90061200354214
#&gt; 110 192.56394398444112
#&gt; 111 180.98923443492257
#&gt; 112 170.13203285049397
#&gt; 113 159.95050694576628
#&gt; 114 150.38972485782446
#&gt; 115 141.41734198411572
#&gt; 116 132.99611497367866
#&gt; 117 125.09156325963338
#&gt; 118 117.66642940606687
#&gt; 119 110.69533918173776
#&gt; 120 104.14845396396463
#&gt; 121 97.99893007959557
#&gt; 122 92.22011750510285
#&gt; 123 86.78965563408303
#&gt; 124 81.68832953028851
#&gt; 125 76.8936948513737
#&gt; 126 72.38701330673442
#&gt; 127 68.15098341186042
#&gt; 128 64.17155344502598
#&gt; 129 60.428038476921586
#&gt; 130 56.908227350924264
#&gt; 131 53.597359299554086
#&gt; 132 50.48291263509982
#&gt; 133 47.55356853186795
#&gt; 134 44.79743700983829
#&gt; 135 42.20495473902311
#&gt; 136 39.76511807450793
#&gt; 137 37.468859040581314
#&gt; 138 35.308453101613594
#&gt; 139 33.27467715145929
#&gt; 140 31.360220767968155
#&gt; 141 29.558058573195954
#&gt; 142 27.861685433714385
#&gt; 143 26.26412411616196
#&gt; 144 24.759623750310396
#&gt; 145 23.343125632673978
#&gt; 146 22.008981468973293
#&gt; 147 20.752466464076754
#&gt; 148 19.568797416671696
#&gt; 149 18.453977382984238
#&gt; 150 17.4033728523491
#&gt; 151 16.413591679613166
#&gt; 152 15.481311105283973
#&gt; 153 14.602503969149222
#&gt; 154 13.774233452363154
#&gt; 155 12.994002407101597
#&gt; 156 12.25856122789346
#&gt; 157 11.56544379208444
#&gt; 158 10.912019785853294
#&gt; 159 10.295986095133893
#&gt; 160 9.715282302954558
#&gt; 161 9.16779373332011
#&gt; 162 8.651771772979203
#&gt; 163 8.16494247053697
#&gt; 164 7.705943656616575
#&gt; 165 7.273075881962532
#&gt; 166 6.864819666904757
#&gt; 167 6.479820163401722
#&gt; 168 6.116708158449551
#&gt; 169 5.774100177816865
#&gt; 170 5.450995917449771
#&gt; 171 5.1461795385705855
#&gt; 172 4.858643025681388
#&gt; 173 4.587382709817182
#&gt; 174 4.3313872873690915
#&gt; 175 4.089901424925121
#&gt; 176 3.862103131723745
#&gt; 177 3.6470527511395345
#&gt; 178 3.444106712594519
#&gt; 179 3.252609836897004
#&gt; 180 3.07181676586878
#&gt; 181 2.901218541067528
#&gt; 182 2.7402092504329087
#&gt; 183 2.588219680628511
#&gt; 184 2.444769343935288
#&gt; 185 2.3093519385929073
#&gt; 186 2.1815040103726817
#&gt; 187 2.0608004075821134
#&gt; 188 1.9468628778712722
#&gt; 189 1.8395089185876619
#&gt; 190 1.7383087355430398
#&gt; 191 1.6427212247232674
#&gt; 192 1.5524241674870793
#&gt; 193 1.467167641479254
#&gt; 194 1.3866326949062429
#&gt; 195 1.3105816336367393
#&gt; 196 1.2387287987377498
#&gt; 197 1.1708405590526374
#&gt; 198 1.1067184254730398
#&gt; 199 1.0461214720168739
#&gt; 200 0.9888821187836597
#&gt; 201 0.9348112964047595
#&gt; 202 0.8837227088870757
#&gt; 203 0.8354674137424224
#&gt; 204 0.7898553794335115
#&gt; 205 0.7467719311431791
#&gt; 206 0.7060574954970198
#&gt; 207 0.6675658111377627
#&gt; 208 0.6311950860163134
#&gt; 209 0.5968226032996442
#&gt; 210 0.5643375968524703
#&gt; 211 0.533635574131998
#&gt; 212 0.5046100881587827
#&gt; 213 0.477185660092641
#&gt; 214 0.4512577894894808
#&gt; 215 0.42675382897272557
#&gt; 216 0.4035914490049111
#&gt; 217 0.3816938006385138
#&gt; 218 0.36099433662345515
#&gt; 219 0.34142901569774986
#&gt; 220 0.3229336644019144
#&gt; 221 0.30544089540020086
#&gt; 222 0.28890489048687534
#&gt; 223 0.2732695786637561
#&gt; 224 0.25848651426787905
#&gt; 225 0.2445098453883533
#&gt; 226 0.2312929773275953
#&gt; 227 0.21879670108271798
#&gt; 228 0.20697931842620834
#&gt; 229 0.195806713788599
#&gt; 230 0.18523834142720977
#&gt; 231 0.17524520436756333
#&gt; 232 0.1657943113708178
#&gt; 233 0.15685667535616374
#&gt; 234 0.14840827333326329
#&gt; 235 0.1404142723758336
#&gt; 236 0.13285399914584078
#&gt; 237 0.12570202269005357
#&gt; 238 0.1189386653565856
#&gt; 239 0.11254012548433291
#&gt; 240 0.1064897371596009
#&gt; 241 0.10076465672097966
#&gt; 242 0.09535057382492915
#&gt; 243 0.09022846810146344
#&gt; 244 0.0853831482366543
#&gt; 245 0.08079988857826755
#&gt; 246 0.07646497849045485
#&gt; 247 0.07236264442671389
#&gt; 248 0.06848434833109891
#&gt; 249 0.06481281290915701
#&gt; 250 0.061339266016385195
#&gt; 251 0.05805307662477996
#&gt; 252 0.0549438470297849
#&gt; 253 0.05200185854871002
#&gt; 254 0.04921917115136563
#&gt; 255 0.046585968334898406
#&gt; 256 0.04409498516073983
#&gt; 257 0.041737229429376375
#&gt; 258 0.039506460963111716
#&gt; 259 0.03739526392873394
#&gt; 260 0.03539767598323861
#&gt; 261 0.033507149403411894
#&gt; 262 0.03171914800559517
#&gt; 263 0.030026565963995385
#&gt; 264 0.028424539700389058
#&gt; 265 0.026908264596902605
#&gt; 266 0.025473451972246715
#&gt; 267 0.024115391447909144
#&gt; 268 0.022829917524529335
#&gt; 269 0.021613534126580944
#&gt; 270 0.020462113245879446
#&gt; 271 0.019372541709334515
#&gt; 272 0.018341010110627966
#&gt; 273 0.017364750002383487
#&gt; 274 0.016440801352077355
#&gt; 275 0.01556635157094091
#&gt; 276 0.014738758943593583
#&gt; 277 0.013955031983018732
#&gt; 278 0.0132131375273473
#&gt; 279 0.012510963462682914
#&gt; 280 0.011846186595046146
#&gt; 281 0.011216909862565686
#&gt; 282 0.010621170683656261
#&gt; 283 0.010057232252595813
#&gt; 284 0.009523402267302118
#&gt; 285 0.00901796344305956
#&gt; 286 0.008539560661121416
#&gt; 287 0.00808664139518218
#&gt; 288 0.007657798604422466
#&gt; 289 0.007251985398671241
#&gt; 290 0.006867552434240349
#&gt; 291 0.006503615024208403
#&gt; 292 0.0061589977274497335
#&gt; 293 0.005832728877824728
#&gt; 294 0.00552383330657353
#&gt; 295 0.005231316887344834
#&gt; 296 0.004954423112991046
#&gt; 297 0.004692217253589943
#&gt; 298 0.004443918325610545
#&gt; 299 0.004208844585195957
#&gt; 300 0.003986240553018342
#&gt; 301 0.003775486718927447
#&gt; 302 0.0035759236905801945
#&gt; 303 0.003386943755826484
#&gt; 304 0.0032079271944684923
#&gt; 305 0.0030384316988295917
#&gt; 306 0.0028779268787492195
#&gt; 307 0.002725934467665546
#&gt; 308 0.002581984589507745
#&gt; 309 0.0024456583936272873
#&gt; 310 0.0023165757211508816
#&gt; 311 0.0021943198339197862
#&gt; 312 0.0020785379932883805
#&gt; 313 0.0019688908467591876
#&gt; 314 0.001865039138166403
#&gt; 315 0.0017667028380198072
#&gt; 316 0.001673582502883412
#&gt; 317 0.001585367679173598
#&gt; 318 0.001501810539526399
#&gt; 319 0.001422673117716745
#&gt; 320 0.0013477143467884333
#&gt; 321 0.0012767229593053158
#&gt; 322 0.0012094776044852774
#&gt; 323 0.0011457905013111825
#&gt; 324 0.0010854694552161603
#&gt; 325 0.0010283296937639874
#&gt; 326 0.0009742096258547116
#&gt; 327 0.000922945854411098
#&gt; 328 0.0008743961838104937
#&gt; 329 0.000828422228956311
#&gt; 330 0.0007848532048364633
#&gt; 331 0.0007435814161682407
#&gt; 332 0.0007044863185208374
#&gt; 333 0.0006674517964915639
#&gt; 334 0.0006323708525695086
#&gt; 335 0.0005991396001984025
#&gt; 336 0.0005676591273796805
#&gt; 337 0.0005378410068842657
#&gt; 338 0.000509593206205131
#&gt; 339 0.00048283262452890964
#&gt; 340 0.0004574801372963568
#&gt; 341 0.00043346425597078986
#&gt; 342 0.0004107167685464537
#&gt; 343 0.00038916698112475027
#&gt; 344 0.00036874509105280215
#&gt; 345 0.0003493980023397482
#&gt; 346 0.00033106827511510654
#&gt; 347 0.00031370294820848413
#&gt; 348 0.00029725119213414107
#&gt; 349 0.00028166684242299266
#&gt; 350 0.0002669014349461206
#&gt; 351 0.0002529104551692411
#&gt; 352 0.0002396549909150608
#&gt; 353 0.00022709499915641242
#&gt; 354 0.0002151964743005534
#&gt; 355 0.0002039240941797398
#&gt; 356 0.00019324565890046152
#&gt; 357 0.0001831235294622124
#&gt; 358 0.00017353441806004343
#&gt; 359 0.0001644493172112498
#&gt; 360 0.00015584082439274125
#&gt; 361 0.00014768295498887574
#&gt; 362 0.00013995332436295415
#&gt; 363 0.00013262955726596432
#&gt; 364 0.00012569091305008096
#&gt; 365 0.00011911540230945581
#&gt; 366 0.00011288450617749258
#&gt; 367 0.00010698023609061709
#&gt; 368 0.00010138656406183746
#&gt; 369 9.608703941253804e-05
#&gt; 370 9.106416188787175e-05
#&gt; 371 8.630363500639326e-05
#&gt; 372 8.179250816284079e-05
#&gt; 373 7.75177155755295e-05
#&gt; 374 7.346701689449564e-05
#&gt; 375 6.962838417570068e-05
#&gt; 376 6.599115070747433e-05
#&gt; 377 6.254418117212432e-05
#&gt; 378 5.927752171205165e-05
#&gt; 379 5.6181730723921865e-05
#&gt; 380 5.3248296817560075e-05
#&gt; 381 5.0468440039186904e-05
#&gt; 382 4.783531311318069e-05
#&gt; 383 4.533824051135289e-05
#&gt; 384 4.2971925902375094e-05
#&gt; 385 4.072931820046649e-05
#&gt; 386 3.860419712035776e-05
#&gt; 387 3.658998409536165e-05
#&gt; 388 3.468116364031449e-05
#&gt; 389 3.287222611892173e-05
#&gt; 390 3.115784475157776e-05
#&gt; 391 2.9533143774768434e-05
#&gt; 392 2.7993239385118317e-05
#&gt; 393 2.6533713457293835e-05
#&gt; 394 2.515050370686264e-05
#&gt; 395 2.384005448398065e-05
#&gt; 396 2.259773576968608e-05
#&gt; 397 2.1420010467094908e-05
#&gt; 398 2.0303827964836834e-05
#&gt; 399 1.9245876019466693e-05
#&gt; 400 1.8243200207082624e-05
#&gt; 401 1.7292859010059725e-05
#&gt; 402 1.6392242334282593e-05
#&gt; 403 1.5538607874558062e-05
#&gt; 404 1.4729443663403283e-05
#&gt; 405 1.396245233615671e-05
#&gt; 406 1.3235478072004572e-05
#&gt; 407 1.2546412926492304e-05
#&gt; 408 1.1893604229102106e-05
#&gt; 409 1.1274605651207216e-05
#&gt; 410 1.0687845492609113e-05
#&gt; 411 1.0131680171617125e-05
#&gt; 412 9.604526090125228e-06
#&gt; 413 9.104866882652663e-06
#&gt; 414 8.631198697407043e-06
#&gt; 415 8.182218044143847e-06
#&gt; 416 7.75664115622133e-06
#&gt; 417 7.353270255677984e-06
#&gt; 418 6.970852594884578e-06
#&gt; 419 6.608387207051329e-06
#&gt; 420 6.26478439097928e-06
#&gt; 421 5.939231288672959e-06
#&gt; 422 5.630510532150695e-06
#&gt; 423 5.337868490845549e-06
#&gt; 424 5.060468769124625e-06
#&gt; 425 4.797479196554172e-06
#&gt; 426 4.548175419581931e-06
#&gt; 427 4.3118425691448e-06
#&gt; 428 4.0878168952303456e-06
#&gt; 429 3.875456427743628e-06
#&gt; 430 3.6741748596127134e-06
#&gt; 431 3.4833269248291047e-06
#&gt; 432 3.302413300508785e-06
#&gt; 433 3.1309131720164374e-06
#&gt; 434 2.9684202192256292e-06
#&gt; 435 2.8143063954514532e-06
#&gt; 436 2.6681942942812437e-06
#&gt; 437 2.529678830678848e-06
#&gt; 438 2.3983594748975304e-06
#&gt; 439 2.2738738446053443e-06
#&gt; 440 2.155852990893322e-06
#&gt; 441 2.0439734146668315e-06
#&gt; 442 1.9379149561299024e-06
#&gt; 443 1.8373813024806672e-06
#&gt; 444 1.7420525193107436e-06
#&gt; 445 1.651681837934868e-06
#&gt; 446 1.5660026986343254e-06
#&gt; 447 1.4848126887738326e-06
#&gt; 448 1.407805243604439e-06
#&gt; 449 1.3347904750171164e-06
#&gt; 450 1.265571380938484e-06
#&gt; 451 1.1999412748704718e-06
#&gt; 452 1.137722153780124e-06
#&gt; 453 1.0787354005070172e-06
#&gt; 454 1.0228096436038175e-06
#&gt; 455 9.697877295536134e-07
#&gt; 456 9.195232359617768e-07
#&gt; 457 8.718667277682245e-07
#&gt; 458 8.266807141735369e-07
#&gt; 459 7.838400421318883e-07
#&gt; 460 7.432422670610245e-07
#&gt; 461 7.047295293241073e-07
#&gt; 462 6.682172780720125e-07
#&gt; 463 6.335979168913036e-07
#&gt; 464 6.007739196907603e-07
#&gt; 465 5.696536281572727e-07
#&gt; 466 5.401483768129584e-07
#&gt; 467 5.121740258956327e-07
#&gt; 468 4.856503635693801e-07
#&gt; 469 4.604994834612916e-07
#&gt; 470 4.3665309177253875e-07
#&gt; 471 4.140445324903026e-07
#&gt; 472 3.92606762471423e-07
#&gt; 473 3.722903378943339e-07
#&gt; 474 3.530199059473835e-07
#&gt; 475 3.347453713024692e-07
#&gt; 476 3.174178528193289e-07
#&gt; 477 3.0098863903561565e-07
#&gt; 478 2.8541218481059226e-07
#&gt; 479 2.7064186642557194e-07
#&gt; 480 2.5663718610336863e-07
#&gt; 481 2.4335703458243967e-07
#&gt; 482 2.3076549087628935e-07
#&gt; 483 2.1882618344371512e-07
#&gt; 484 2.0750489949470836e-07
#&gt; 485 1.967709863796408e-07
#&gt; 486 1.8659788188413924e-07
#&gt; 487 1.7694719372113647e-07
#&gt; 488 1.6779487766772397e-07
#&gt; 489 1.591173463100117e-07
#&gt; 490 1.5088941982411513e-07
#&gt; 491 1.4308626189302695e-07
#&gt; 492 1.3568760330487115e-07
#&gt; 493 1.286714248189776e-07
#&gt; 494 1.2201888487984934e-07
#&gt; 495 1.1571047960904828e-07
#&gt; 496 1.0972832075560864e-07
#&gt; 497 1.0405612689057184e-07
#&gt; 498 9.867753044394926e-08
#&gt; 499 9.357999190790877e-08</code></pre>
<div class="sourceCode" id="cb516"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb516-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-1" aria-hidden="true" tabindex="-1"></a>toc <span class="op">=</span> time.process_time()</span>
<span id="cb516-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb516-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(toc <span class="op">-</span> tic, <span class="st">&quot;seconds&quot;</span>)</span></code></pre></div>
<pre><code>#&gt; 9.014092823999995 seconds</code></pre>
</div>
<div id="a-neural-network-written-in-rtorch" class="section level2" number="12.4">
<h2><span class="header-section-number">12.4</span> A neural network written in <code>rTorch</code></h2>
<p>The example shows the long and manual way of calculating the forward and backward passes but using <code>rTorch</code>. The objective is getting familiarized with the rTorch tensor operations.</p>
<p>The following example was converted from <strong>PyTorch</strong> to <strong>rTorch</strong> to show differences and similarities of both approaches. The original source can be found here: <a href="https://github.com/jcjohnson/pytorch-examples#pytorch-tensors" class="uri">Source</a>.</p>
<div id="load-the-libraries" class="section level3" number="12.4.1">
<h3><span class="header-section-number">12.4.1</span> Load the libraries</h3>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb518-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rTorch)</span>
<span id="cb518-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb518-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb518-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb518-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb518-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb518-4" aria-hidden="true" tabindex="-1"></a>device <span class="ot">=</span> torch<span class="sc">$</span><span class="fu">device</span>(<span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb518-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb518-5" aria-hidden="true" tabindex="-1"></a><span class="co"># device = torch.device(&#39;cuda&#39;)  # Uncomment this to run on GPU</span></span>
<span id="cb518-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb518-6" aria-hidden="true" tabindex="-1"></a><span class="fu">invisible</span>(torch<span class="sc">$</span><span class="fu">manual_seed</span>(<span class="dv">0</span>))</span></code></pre></div>
<ul>
<li><code>N</code> is batch size;</li>
<li><code>D_in</code> is input dimension;</li>
<li><code>H</code> is hidden dimension;</li>
<li><code>D_out</code> is output dimension.</li>
</ul>
</div>
<div id="dataset" class="section level3" number="12.4.2">
<h3><span class="header-section-number">12.4.2</span> Dataset</h3>
<p>We will create a random dataset for a <strong>two layer neural network</strong>.</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> 64L; D_in <span class="ot">&lt;-</span> 1000L; H <span class="ot">&lt;-</span> 100L; D_out <span class="ot">&lt;-</span> 10L</span>
<span id="cb519-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb519-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random Tensors to hold inputs and outputs</span></span>
<span id="cb519-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(N, D_in, <span class="at">device=</span>device)</span>
<span id="cb519-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(N, D_out, <span class="at">device=</span>device)</span>
<span id="cb519-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-6" aria-hidden="true" tabindex="-1"></a><span class="co"># dimensions of both tensors</span></span>
<span id="cb519-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-7" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(x)</span>
<span id="cb519-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb519-8" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(y)</span></code></pre></div>
<pre><code>#&gt; [1]   64 1000
#&gt; [1] 64 10</code></pre>
</div>
<div id="initialize-the-weights" class="section level3" number="12.4.3">
<h3><span class="header-section-number">12.4.3</span> Initialize the weights</h3>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb521-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb521-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly initialize weights</span></span>
<span id="cb521-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb521-2" aria-hidden="true" tabindex="-1"></a>w1 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(D_in, H, <span class="at">device=</span>device)   <span class="co"># layer 1</span></span>
<span id="cb521-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb521-3" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(H, D_out, <span class="at">device=</span>device)  <span class="co"># layer 2</span></span>
<span id="cb521-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb521-4" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(w1)</span>
<span id="cb521-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb521-5" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(w2)</span></code></pre></div>
<pre><code>#&gt; [1] 1000  100
#&gt; [1] 100  10</code></pre>
</div>
<div id="iterate-through-the-dataset" class="section level3" number="12.4.4">
<h3><span class="header-section-number">12.4.4</span> Iterate through the dataset</h3>
<p>Now, we are going to train our neural network on the <code>training</code> dataset. The equestion is: <em>“how many times do we have to expose the training data to the algorithm?”</em> By looking at the graph of the loss we may get an idea when we should stop.</p>
<div id="iterate-50-times" class="section level4" number="12.4.4.1">
<h4><span class="header-section-number">12.4.4.1</span> Iterate 50 times</h4>
<p>Let’s say that for the sake of time we select to run only 50 iterations of the loop doing the training.</p>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb523-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="ot">=</span> <span class="fl">1e-6</span></span>
<span id="cb523-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb523-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-3" aria-hidden="true" tabindex="-1"></a><span class="co"># loop</span></span>
<span id="cb523-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>) {</span>
<span id="cb523-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Forward pass: compute predicted y, y_pred</span></span>
<span id="cb523-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-6" aria-hidden="true" tabindex="-1"></a>  h <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">mm</span>(w1)              <span class="co"># matrix multiplication, x*w1</span></span>
<span id="cb523-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-7" aria-hidden="true" tabindex="-1"></a>  h_relu <span class="ot">&lt;-</span> h<span class="sc">$</span><span class="fu">clamp</span>(<span class="at">min=</span><span class="dv">0</span>)   <span class="co"># make elements greater than zero</span></span>
<span id="cb523-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-8" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">mm</span>(w2)    <span class="co"># matrix multiplication, h_relu*w2</span></span>
<span id="cb523-9"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb523-10"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor</span></span>
<span id="cb523-11"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></span>
<span id="cb523-12"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-12" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">&lt;-</span> (torch<span class="sc">$</span><span class="fu">sub</span>(y_pred, y))<span class="sc">$</span><span class="fu">pow</span>(<span class="dv">2</span>)<span class="sc">$</span><span class="fu">sum</span>()   <span class="co"># sum((y_pred-y)^2)</span></span>
<span id="cb523-13"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># cat(t, &quot;\t&quot;)</span></span>
<span id="cb523-14"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># cat(loss$item(), &quot;\n&quot;)</span></span>
<span id="cb523-15"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb523-16"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span>
<span id="cb523-17"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-17" aria-hidden="true" tabindex="-1"></a>  grad_y_pred <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">mul</span>(torch<span class="sc">$</span><span class="fu">scalar_tensor</span>(<span class="fl">2.0</span>), torch<span class="sc">$</span><span class="fu">sub</span>(y_pred, y))</span>
<span id="cb523-18"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-18" aria-hidden="true" tabindex="-1"></a>  grad_w2 <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_y_pred)        <span class="co"># compute gradient of w2</span></span>
<span id="cb523-19"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-19" aria-hidden="true" tabindex="-1"></a>  grad_h_relu <span class="ot">&lt;-</span> grad_y_pred<span class="sc">$</span><span class="fu">mm</span>(w2<span class="sc">$</span><span class="fu">t</span>())</span>
<span id="cb523-20"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-20" aria-hidden="true" tabindex="-1"></a>  grad_h <span class="ot">&lt;-</span> grad_h_relu<span class="sc">$</span><span class="fu">clone</span>()</span>
<span id="cb523-21"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-21" aria-hidden="true" tabindex="-1"></a>  mask <span class="ot">&lt;-</span> grad_h<span class="sc">$</span><span class="fu">lt</span>(<span class="dv">0</span>)                         <span class="co"># filter values lower than zero </span></span>
<span id="cb523-22"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-22" aria-hidden="true" tabindex="-1"></a>  torch<span class="sc">$</span><span class="fu">masked_select</span>(grad_h, mask)<span class="sc">$</span><span class="fu">fill_</span>(<span class="fl">0.0</span>) <span class="co"># make them equal to zero</span></span>
<span id="cb523-23"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-23" aria-hidden="true" tabindex="-1"></a>  grad_w1 <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_h)                  <span class="co"># compute gradient of w1</span></span>
<span id="cb523-24"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-24" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb523-25"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update weights using gradient descent</span></span>
<span id="cb523-26"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-26" aria-hidden="true" tabindex="-1"></a>  w1 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">sub</span>(w1, torch<span class="sc">$</span><span class="fu">mul</span>(learning_rate, grad_w1))</span>
<span id="cb523-27"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-27" aria-hidden="true" tabindex="-1"></a>  w2 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">sub</span>(w2, torch<span class="sc">$</span><span class="fu">mul</span>(learning_rate, grad_w2))</span>
<span id="cb523-28"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb523-29"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-29" aria-hidden="true" tabindex="-1"></a><span class="co"># y vs predicted y</span></span>
<span id="cb523-30"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-30" aria-hidden="true" tabindex="-1"></a>df_50 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> y<span class="sc">$</span><span class="fu">flatten</span>()<span class="sc">$</span><span class="fu">numpy</span>(), </span>
<span id="cb523-31"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-31" aria-hidden="true" tabindex="-1"></a>                    <span class="at">y_pred =</span> y_pred<span class="sc">$</span><span class="fu">flatten</span>()<span class="sc">$</span><span class="fu">numpy</span>(), <span class="at">iter =</span> <span class="dv">50</span>)</span>
<span id="cb523-32"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb523-33"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-33" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_50, <span class="fu">aes</span>(<span class="at">x =</span> y, <span class="at">y =</span> y_pred)) <span class="sc">+</span></span>
<span id="cb523-34"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb523-34" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="rtorch-minimal-book_files/figure-html/run-model-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We see a lot of dispersion between the predicted values, <span class="math inline">\(y_{pred}\)</span> and the real values, <span class="math inline">\(y\)</span>. We are far from our goal.</p>
<p>Let’s take a look at the dataframe:</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb524-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&#39;DT&#39;</span>)</span>
<span id="cb524-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb524-2" aria-hidden="true" tabindex="-1"></a><span class="fu">datatable</span>(df_50, <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">10</span>))</span></code></pre></div>
<div id="htmlwidget-1202f61defa40973a1ae" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1202f61defa40973a1ae">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640"],[0.83994026831249,-0.00261469246496478,1.17331508976272,0.113064550776486,0.0103083101232198,0.741725300575136,0.757130556967624,-1.19798639091765,0.977469466953348,0.958784411472249,0.0208831428895932,-1.64816158891565,-0.0500028243495128,0.733813527145886,1.58085339989783,1.37525327872787,-0.386739371622718,-0.360212335648884,0.757263768677633,-2.33097748052543,-0.280231384151112,-0.958825812111196,-0.0248397757648495,1.0914318283759,-0.458192050288552,1.88579895675686,1.8374918101401,-0.102590503582433,0.158376834397865,-1.69505820821086,0.0630930916536182,1.3418620933296,0.173417858163622,-0.972149178523607,-0.312413823093578,2.16446071258402,-1.08569798526669,-0.570029361135791,0.180937256764405,-1.26229551939497,-1.36950465873129,-1.15493391741737,1.40785825831085,0.781023621423221,0.0780186960888742,2.11273169799433,-0.382805598728771,-0.332249209115966,0.833881240069712,0.59587340242551,-1.10900749622674,0.768900089169295,-1.35001396009045,0.483618658476077,-0.286807646207213,0.141828938278373,-0.374918514638461,-0.231434565088581,-0.00992419102991136,-2.11812756250465,-0.523762933650836,-1.32458158157352,0.431681037219444,0.925057143386304,0.93777525245556,0.546488768273407,1.11596946687424,0.570940155822875,-1.05140854030057,-0.318810634218592,-0.289448897409276,-0.418406412091964,0.781141791150043,-0.114891413907268,-0.845416526254121,-0.91845936893655,1.20789854386747,0.741312355103571,0.634427251618938,2.20585574787314,0.356161490453256,0.267242739519324,0.484831528360182,-0.165738361704601,0.189930678043718,-0.93651989042175,-1.45410391289995,0.233684800337273,0.502157215971716,0.28920149072855,-1.22551276737874,-0.385846452553303,1.42231442741027,-1.7116822994054,0.155053435413657,1.75487195590539,-0.480695737266194,-0.744335584259619,-2.13245053322139,-0.364322343399947,-0.233973774563064,0.111681118770621,-0.88914370304214,-0.722085375742162,-2.32991274785241,-0.479068930295669,0.789931238294979,1.06380136135981,-0.0798134682221333,-0.374168001227348,0.122704935131071,-1.43690716269235,-0.853944502445023,0.456289787735743,-0.0142516296486563,0.816751922214445,0.0849881110641102,0.446428132382165,2.42338976222122,-0.246995607668513,0.276176450241468,0.930116509469416,-1.02544718711831,0.104887979909664,-0.0142328430202254,2.03937753638296,0.373422932290684,1.43684984339667,0.948776626114924,-1.31827952307809,0.292905851185605,0.147308597580878,-1.03373092754513,0.419607277988786,-0.261092633509276,1.6071495149865,-0.647906145700861,0.657156199635267,-0.57098265859795,-0.818600322951857,1.31069089175351,1.81375842080092,-0.943433957369921,0.857269379202826,-0.263283789177259,1.31296346802011,-0.955590491841724,-1.56443546642085,-2.17226562414852,1.45591099037169,1.03814194129823,-0.197772383407226,-0.650297223745001,-1.33005360476313,0.921942504799078,0.752209085763665,-0.0361295533712819,2.63443405115608,0.287654631188146,0.655186314323695,0.400648171873719,-0.307311020383442,-0.0880147158656964,-1.41821667150147,-0.313515957169452,1.02977246630598,1.63557919370632,0.471406593501004,0.996474278664091,0.682501811766753,-0.971734007624617,-0.654540506542262,1.99500285811451,-1.64399263553111,-0.663557865235524,-1.68572596581282,-0.00487618116822535,0.458801236543731,0.5824022309138,-1.36287855122539,-1.27186820648675,0.714902868782501,-1.0165666698022,-0.0235973711331876,-0.0710369673541219,0.142253138818817,-0.675694865931503,-0.256829936236055,-0.101224481444322,-2.15395839517849,-0.964817551233336,-0.58900570028774,-1.77786400328839,1.14147095872602,-0.32772078240833,-1.35670287649227,0.415691882771873,-0.925932387905733,-1.61418322141891,-0.0867796005444939,0.601846649732015,1.12863379512717,0.184978687339329,0.156848912253547,-0.484058595314498,1.12522934475228,0.243459583230952,-0.634978878232694,-1.25090253023824,-1.43563909830578,1.13586385748573,0.215411392403429,1.86239774513426,1.11499996333691,-0.674626192034018,-0.434131329001274,1.03699553226858,-0.0366741372856878,-0.402271366973104,2.10246137335328,0.459018936796291,-0.25503953202401,-0.672610058718924,1.09980827026172,-0.410925835278326,-0.896790320270794,-1.03109344053961,-0.262748015975908,-0.333069074621615,0.977294210489194,0.678715733607743,1.25873526914719,0.42400531072313,1.79340828861104,0.294284737340495,-1.51557096175814,-0.566556739572426,0.0695601296483909,1.20367672097015,-0.549833339055226,0.120064475138402,0.307536795611604,1.21556005041361,0.12602924786391,-1.47214013475447,-0.318687822897892,0.279546357353257,2.70523892501022,1.68587064051207,1.1239941108417,0.424088292374247,0.104431077381569,-1.18021245837732,-0.916706926774142,-1.56221497392067,-0.851852947077733,-1.11863086095433,0.148312190622533,0.502631432428189,1.58774800017961,-1.10166271639973,-0.777449867610656,0.942870568479463,-1.9339574431339,0.00698050744702491,0.673684269774716,-1.39732869525216,0.283501034389694,0.874755970337202,-0.417645129576752,-1.36874292450096,-0.0747812970509281,0.0105724695173432,-0.0456309112554566,-1.47339841259021,1.33700556433092,1.94209421947211,0.284457070328279,-1.98482664413377,-0.50080737939018,-0.274676881900125,0.710938213804541,2.25851536328219,0.618237686663071,0.230927765626917,1.10122470908834,-0.572603422146998,-0.53302471140162,-0.717679874784871,-0.877038791682033,-0.748485888179382,0.184771684002625,-0.22231743531996,-1.00929695552901,-1.37047063999704,-1.07025776684948,-0.0066305826821924,0.00463777722911298,0.200669539977365,0.809383668396162,0.287278665935646,-0.303985246895298,0.139672450093511,0.127722862850529,-0.95565825905712,-0.799909006767028,-0.202940490973971,-2.41551300196281,0.3750531988884,2.0938362702894,0.605430623566889,-0.816263936823711,1.4935718662945,2.15379218446882,1.28063423391778,-1.16859150067533,-0.495931078057793,-1.12572347427127,-0.0289018780637711,-0.491217346285856,0.582465462265955,-1.73320910486715,-0.250616430567373,1.37773646638411,0.505183750595998,1.03329001588251,-0.148129803407604,-1.02024054280369,-1.16747702799092,-0.698262649012394,-0.29088420462642,-0.0873481684419898,2.28367061778488,0.799274120229441,0.496859660507907,-1.16771232655097,0.0560084744790094,0.0697664131694416,-0.63172486335937,-1.61579250963407,0.822541288820032,-0.585096534350738,-0.316981493648855,-0.445729843878749,-2.50838624530302,-1.39899463359675,-1.31102892051884,0.750402144358772,0.703404121659897,-0.25083983221813,-0.25095095379119,-0.872342684631607,0.628192536989848,-0.936217675649455,-0.550371466231215,-0.26970983693769,-0.688604841006888,-0.390605966140334,1.06988684039523,0.498849800595456,0.333419121051892,0.910556713584374,-1.27892253213922,-1.84182185333004,-0.512035156980371,-0.364119099300724,0.459286557995962,-0.0437559190006266,-0.453340737020902,2.02055656551793,1.47116567306761,-0.0115038122635589,0.131722917926454,-2.28429446135232,0.669356739346067,-1.14593193034066,-1.91291968267857,-1.67212741707599,0.54073309384294,-0.256724509418031,-1.89134380794797,0.161915204911539,-0.458809181262073,-0.194078077908355,0.296793193074685,-0.0250807732722297,-1.05794861180844,0.24484382118802,-0.648883034347149,0.618876699768757,1.44663045545316,1.76518871577172,-1.92334252201028,0.202212741132606,-0.342486128124399,-0.119772048017177,0.266562321266478,0.722029306985646,0.403631540478367,-1.9875531289521,-0.781171921052648,0.987488573315664,0.308690446320804,0.106148749253679,0.185085607568157,-0.390160876288962,1.25429513802065,-1.35013386465017,-0.342974766571647,-0.232894537629491,0.980332519980097,-2.11322741735,0.23024995771627,-1.45705269393795,1.38625292848739,1.6457152752694,0.886358263746316,0.296335125360452,-0.442944664529715,0.274627958703436,1.37404495680148,-1.61754075111507,-1.01356309656133,0.34151499813101,0.60561927302074,0.466358958410998,1.18788000370773,-0.998222009747409,0.472411048704897,0.802363134564957,0.778863518671328,1.33312992214265,-0.11483784787192,-1.24014458172761,1.68097845650274,-0.156295579276488,-0.36167688476951,1.12990338986994,0.544741774675208,-0.739519141431302,0.348344512782951,-0.10457968657704,-1.54862790298088,0.991859873430099,-0.712951101315977,-0.688713554688238,-0.019645395217261,-0.4276668324237,0.485554217337533,-1.22769785113341,-0.0839624525226322,0.929646496319533,-1.04399669558851,1.5862211735349,-0.762919277781499,-0.0482214954780706,0.482863741201961,0.272647805689435,-0.529870425645623,-2.32824908375099,0.0361073596021704,-0.451810115323141,-0.282254053923175,1.54911580277808,0.253374436546217,2.1590870111341,1.00793354498743,1.2238963859792,-0.262656713944417,0.371188568837448,1.84975076615656,-0.32057748245843,-0.484757190293574,0.119115296354525,0.311425735299618,2.86340591290978,0.561681683625529,-1.36773128004884,-0.521412125803238,1.8438836271771,0.15291957051073,-1.32938108961612,1.12345625118363,-0.0988379062841592,-0.781398448382246,1.13052436749178,0.767681931298354,0.525356584083132,0.582232899579348,0.333066984200484,-0.806258638926797,0.556552894435339,-1.30665318755998,0.785186326876701,-0.394106214647196,0.550730858543516,0.774230360594257,1.25782444024924,-1.59645579014647,0.726215347670778,-0.63694860577194,1.27493610125473,-0.0352439186929309,2.89168261447142,1.37501515427315,-0.0491456451207457,-0.0546139127595145,0.260510410587632,0.981880273079814,0.14277683851885,-0.254490023068397,-1.08396906594294,0.271948629915079,0.377547175371029,0.981224651122629,1.80754059091588,-1.1073823023655,0.482139414365283,1.8188538137154,-0.58749687491936,1.1093609884837,-0.297608176884006,2.57784620418014,2.52204599693865,-0.384840198277502,-0.855562334499144,0.938512269689899,-0.468196363866475,0.260008292451711,-0.462982716485726,-0.886275855262118,1.12411879590505,-0.909935517062699,-0.751334836930073,0.514366756631972,0.195860128347231,1.55471787782953,-0.253467892483979,-0.944270533093175,0.0655037556538774,0.669046506691992,-0.329793629158233,1.30135750702321,1.40114405750616,0.796269800703752,-0.10853585230126,1.29063556898416,-0.693149828347284,1.15340154198528,-1.67650319533252,-1.49429593747768,-1.273469908539,1.31161289723836,-1.31181443286064,-0.950226359402931,0.144663430311268,-1.37563897316271,-0.229895425421889,0.419692024617746,0.516239563506011,0.740669495728343,-0.361952001881317,-1.60995844987436,0.327757025857471,1.6432950871785,1.20535256388868,-0.353510280844418,1.21473389654699,-1.53240226188522,1.92233286176372,-1.09253954985412,-1.59184155325035,-0.64646780319668,-0.426883748541837,0.209569862271875,0.0415756254512372,1.33238165750511,-0.681027400604152,0.620571329940411,-0.900045782913094,0.309651318876625,0.118175407084332,0.227812270228395,-1.18930438569969,0.0837636991730383,0.588009163095429,-1.5092271780105,0.995898635439402,-0.00639701815793872,-0.3502219106773,1.4288096764155,-0.372525850460849,-1.86657115947184,1.73533502830238,1.4419101995773,-0.0083570684921407,1.3092622868551,0.000392255402139403,-1.45683485419048,-0.844996213277658,-0.60477119564568,-0.0517937465881769,0.821273104631407,1.50318458611856,0.26033919453097,0.521628843921831,-0.525259906808531,-1.07777898882352,-0.340595705086819,-0.0688224810312909,0.024696280077251,1.76093398424226,0.175448866859079,0.138406693673293,-0.206063763800319,-0.435211393481228,0.529893499013784,0.967447102676697,-2.43106301578848,-0.0424933305199116,-1.99868317055989,-2.47557651859656,-0.965041073936391,-0.435050040400478,-0.507170912986636,-1.37604070291559,0.641342899780833,0.15067215935219,0.647532140383024,-1.21460078326391,1.46931171546606,-1.78034791140857,0.39149639637274,0.953942227218803,1.27980367071278,0.263325295304111,-1.03877191179501,-0.962563912956559,0.895371173457089,0.905913690706474],[-10.5647346894391,2.14915615578889,-8.13878442490376,-4.29308261348074,18.7224487868666,-0.849544993526666,2.80821304968626,-5.75869401904617,-15.5218068640171,3.03253908015019,0.12208291407666,-0.68287849654192,1.4766268797082,2.02538209822381,-1.42475662127582,-3.64222107442104,-0.047960978290547,3.82343746416712,0.331072344632535,-7.41052874763578,4.5005791224336,0.133744550210691,-7.29968383960036,9.26743294691351,6.2409251121433,12.2589044561666,7.74282427061587,-3.967934636768,5.19211860796669,-3.96781544642297,-7.97965002478959,3.36402792540614,0.884761925217034,-5.40058636901513,-0.0479572253917486,-11.8566488392361,-4.51544671436251,6.70621346037393,-7.31497735022504,-4.39119607543386,-3.18222969599255,-3.40679178148802,-0.0294178323844925,4.49708976822953,6.33326553759501,0.254081703518681,-1.32148076120266,-1.20271979838297,-0.420883919363684,-3.76567644525085,3.55745621404253,4.79904769556007,0.983768487876333,11.0026902089052,-0.390108288909971,4.81433416707612,3.22117255366387,2.76960676328508,6.84981910148923,-0.388398738546234,-1.97372293654664,0.865936092025367,-1.87357898178474,-3.55140389294748,-4.84289985786933,-3.44725694199547,-2.13884917836362,4.42555744961002,-5.07797657137638,0.985188112786685,-3.52038121367873,-3.10394789006473,-2.7779821187106,-2.48415213217637,3.85623290247889,-4.62318696085969,4.59707710105135,0.343802613160041,-6.07772649610589,-0.697945527634079,2.81813612035609,-0.175952372494441,6.60179181865746,0.549551618986389,-3.94670039989037,-2.48891427937582,-1.36392808759367,1.87370351572561,4.13748091296566,-0.588058415574017,3.14754737951862,0.35502296551417,2.56658801513895,-0.133226063851396,-2.63413903493144,2.60140491856328,-0.0166972456695258,0.190204239561474,1.28258135744843,-1.8074957101205,-0.237605783528755,-1.48399400310477,-7.74427376379445,4.22965151577501,-6.79185151630957,-0.888192281547035,2.753882145764,3.20565688576148,1.82611606646579,-8.87640523350396,0.91481185203073,0.6155040511229,-3.21354354196474,1.21178678107393,2.24570921065806,5.47434741525278,0.27440342396131,-2.87590404098157,6.52334607198643,1.97668092081637,-4.32846075182524,1.82717418196658,-6.75576525721611,-1.22151168179276,4.83285464724798,5.57784285991963,2.62419181930306,-1.12196838121075,-3.77886753275595,-1.78234690001163,0.409638174199989,1.62896278228027,5.92507717373834,-4.52698431976117,-5.68188942123068,-1.13551998338509,-0.931547945076613,2.54139646813234,-5.9581431420544,2.74887086907464,-2.43890337283578,4.13479808509131,-0.63495856137127,-3.49197690853603,-5.55880328058861,-0.902115018759382,-4.17678880498853,0.549504035957035,-3.66754812149694,6.56959791964721,1.57769996861422,-2.4500627274156,-1.78110033990439,-3.079433631624,0.57844363527751,3.60043936925132,2.51052145214189,-0.837160496162372,-0.0811165176799538,-0.754515342436947,1.58830709710453,-0.504811152644248,-0.973361540982708,1.3334121230699,-0.569854206579635,3.96363740878081,-0.981422860074947,-0.760559514111428,4.27793204685202,-0.983127710397058,-1.00104787799589,-2.37589082267628,3.10981769825676,-3.65200035173794,-0.386185229199896,-5.37817164054658,-1.99042917871846,0.251143427522621,0.903769470383523,-0.885559456761411,0.525884604749218,-4.97205336888198,1.74066926801572,-0.601951957502275,6.80597303180353,1.62090634642433,0.772922002595395,-3.02586723532096,3.01522759616796,-2.63034673340512,-1.01465711612134,-1.62037261798791,5.41952599917802,5.20558200877229,-3.49743062967812,-8.27725109277561,-0.555283399521476,2.77718234285985,-0.329297476234803,-1.0558479122443,-2.46385284945106,3.93042450816311,-2.01211303356736,0.659077491264343,8.94285129267823,7.94719742351843,0.120119002324524,-7.59400275355767,-0.918623068573721,7.19247720295046,-3.33220200896191,-1.18517306594299,-10.9575240416155,3.42572208373302,2.21842112728362,-2.6485985988461,2.99611186275176,1.24150732708727,-5.60310772913009,-4.02950620482692,-1.68263789331655,-3.24994254592948,7.00043869580037,-7.3545494199033,-1.15399524804537,-4.85753020700979,-13.6549646631359,0.414775154899498,-1.04969492887514,21.7153171933748,-4.88290809624921,0.0759665127996407,5.80174844139903,-4.54364028006875,-6.64873020409227,-13.2067813359278,-1.21176178820888,2.40210018268537,-2.86992822833962,-8.9759894914074,-0.889525326468535,0.164773275392599,3.84657764376361,-4.54672533726853,-5.79056915818017,3.10085622862954,-3.39866743910134,1.28349436046609,2.09424105712005,4.02623340927815,8.10159951146783,2.28107293057092,-0.985841735158553,3.45510314053263,-6.38717501488363,3.49004534297861,-3.28367375627252,-3.22035982563885,7.41923952015786,-4.80052186200059,-0.877032891175887,-2.32033020025338,0.53302666629893,-5.22162160062944,-2.44067366671034,0.38403932688059,-1.10360136532661,-1.57017690290841,4.07453101727814,1.8426257661984,-1.15855594100577,4.48023054586635,18.1977139381726,-1.00185720652128,-12.6945310125566,2.07707114881292,-0.875595330005161,3.15164887454041,2.88660848009027,8.19387062995758,-4.37297923315375,2.82175835387859,1.13078399602522,-11.4375273536282,6.70878489214178,-0.483111294345787,0.738043003873584,-3.06616101303149,-14.7326721596789,-1.56112793472295,0.456289018188746,2.53354525436792,0.235940355583434,-1.02794545821823,-0.0855839650580092,7.22388363475988,-1.59061617575594,-2.9591610516548,4.38960876128705,10.2421798603605,0.622702942101773,-0.784336754841981,-3.77915787593732,0.617677078453413,4.63279154356153,8.60025835161503,1.11249295859091,-6.53231082616395,-0.151956442962547,7.77389656639393,0.867000720812691,-1.22335441627354,0.257466546413102,1.27319286462779,-0.888491950654313,0.504213863017425,-2.45298272474359,-0.159555002745176,0.817615706986203,2.03734948322443,0.320679832856516,0.482221586793965,7.5127471293631,8.16851627730247,5.54871855718118,-1.59853231039558,2.55837367203849,0.224364552929961,-0.879919748317504,-5.81594461603372,1.64399861772345,-3.88086499314129,0.978070124623173,4.69301546980807,4.84739779040004,-0.762798977048316,2.56885317255688,-2.89103719854509,0.818448396301768,-6.02822660816856,-0.787592782402409,-3.57644326961248,-1.76792716583964,-7.16400876418424,-1.56950182556553,0.327141020873658,-0.84539746905767,-0.32206867063433,-2.85305739162357,2.24055975074777,5.7850382820064,1.3393987193778,-1.04017531000198,8.58107880936385,-1.34299166081883,0.881461153519837,-3.17723213900755,3.16711881563156,10.2188789968476,-3.93632258826466,2.94906323245651,-1.03459880624032,4.21688678202809,-3.93875942437188,-3.10375756443252,-1.03470696147255,1.13176758992529,-1.25301544253051,0.590338442668589,2.32844757434319,2.38638033653526,-3.60184951871657,-3.01346559475034,-0.787742542126173,5.62476908877575,2.54072674316066,-0.741754649005731,-5.41974101112216,2.19803532039817,-0.318933079403224,-6.58448188587121,8.13901240188159,-1.27371300704175,-4.15045139533825,9.16260315678046,1.17535286432776,0.79469938911833,1.13876656192498,-11.7669550241595,9.07034805021107,0.767704972406142,2.03723958696946,-12.192450146397,3.39097065207948,8.71521283039187,4.01848704599337,1.48112115443198,-1.17783104031174,-3.11132426039515,-2.41808527213212,-4.58339910452184,-2.53626153614406,2.1812018920532,-5.43038098670469,-1.42050630400086,-9.13309833021985,0.874916603116878,-0.110764133934198,-7.28822708621123,-7.28563050370691,-1.3665832148622,-4.49985611509417,-4.47924660149263,-1.20056385295783,-4.05667098788989,-2.81449841268567,5.27565340249851,-0.721796814797969,-1.95446111571476,-15.6985294926974,1.11604384441139,0.682012790799511,3.58890947841135,-4.63705615828202,7.93442585399031,3.71588957709059,4.12855963665024,-4.90937566598999,-3.01263353230264,10.1801201364223,2.23771097623468,-2.2039905933657,-3.33282081758138,-0.479262581536489,8.04524639241568,3.19648118178899,-3.84424902966597,-2.21623689292125,2.545523868853,3.80296980478848,1.3952629838725,-1.14771628551264,-7.34828624672557,2.88946313064308,-4.00488697057194,1.2366881502442,0.217704739968335,-1.07169437871004,4.60424598036826,-5.92693280290248,-4.79531554774781,1.75664720426153,-9.94305246379414,1.93673218801168,-0.586373480978645,-1.13857815973359,-1.4910193195582,1.33690631538979,-4.09541426151177,-5.01026898060979,-5.436617490567,-1.03969019951818,9.86320701484055,-2.94119658214113,-2.99404967242439,2.5443762320913,-0.791897734033356,1.3189020692171,-2.12680936533824,11.2047052016522,3.64689080222531,-4.66292696129561,4.09181565277487,-6.61423071664974,-0.921603876063838,2.67451883351139,4.16361608385875,-4.268464020712,-1.49242454501386,5.33375343560491,4.98741640235837,-1.73711842077045,6.41963324336511,0.986813333474093,-5.15107121501746,4.44930761499271,-0.134971968598006,2.38021565324489,8.08187169367886,4.65424262141468,0.582824082555302,3.79599344780273,1.39766888203425,3.13313885753088,-7.34430870016085,-6.16037552674785,-2.36531015965215,7.46601288580591,-0.724742625069933,-3.95130507919651,-2.08592361873705,-2.26794401350631,-1.43116135019083,2.81342857309382,5.49378243133178,3.6466595919762,-2.74441427723778,-3.0572297207773,-3.42344479352917,2.69937472237561,1.90712357999917,0.335149041838783,3.30124907575719,0.963576868746223,-1.53054810596663,1.88641730792105,2.71583312984377,1.60552037532126,0.921300149179249,-3.06383379687746,3.8889748778448,2.49987564827567,-2.47392055599193,4.21777776945792,-4.69615220572349,3.79782019074152,1.42970766696369,-1.75523772809222,3.82890101642408,-2.10939939874931,-1.48852025720048,-0.754166735891449,5.59584025892694,-7.54812733029478,-7.98203917285928,-7.70996102494289,-1.75391446732367,2.85261662824871,0.69379515359474,-2.86657477684291,0.00528322474152432,0.540281353378511,4.49560390039219,3.36356376015985,-0.415645888370989,1.2437843144465,-2.61434390679421,-0.810739008343603,1.9934376784368,2.56789592693404,0.648464740497744,-0.609329204802297,-0.584699288813891,3.2796089286661,-0.405557211149532,-0.0367839747936625,-3.84015657089563,2.97608831345079,3.8418891173751,3.72918618025774,1.3714999873194,-0.814626666007971,-2.86358383759927,2.46887365644711,3.48832610459392,4.11074109702169,1.5406423519159,0.242194508600861,-3.8478018473792,0.899217294263752,2.54422998278002,-4.81209511864625,-1.81430200872227,4.70462108222735,0.147240294028649,3.79132183901974,3.04769990554752,-1.39992715282957,7.39250748383584,0.146438988699424,0.194322660054569,0.513096720145793,1.62233359665004,-0.60577795132532,0.891003093554286,2.3323955108124,-0.484728627624977,-1.08497701252794,0.648291996219077,1.76121589728909,-0.636029319320886,-5.21919103464105,-7.6720844461757,2.9128743825879,9.61518276707234,-2.32743536038678,1.8843326217148,-1.25478097789913,-5.50258513815276,-6.10591337304612,1.0556004128229,2.91855497800785,-5.95523215016064,8.61172991130766,-0.792804641443631,2.13504168705973,-0.283187226620999,1.04758022729368,4.06266529502032,-4.84142830710598,6.93054770421293,3.27858089307208,8.59646781758224,-3.51902570767892,-8.15109988115042,-6.21900715881115,4.77336080053165,4.04152801708861,0.218553463104532,-0.808888814760831,3.62707111715732,7.07899453740497,1.46273016425247,9.94527785752351,-11.3970722666505,2.91701155593933,0.326476875594595,6.77990222601838,14.3634908511185,2.15265044103146,-1.46370632292102,-1.54965456505625,-3.06496741913012,-1.8593111589005,2.48229955728184,-4.55548811739358,-1.04130411945134,2.94409719818516,-4.06281046113379,-2.10779797497356],[50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>y<\/th>\n      <th>y_pred<\/th>\n      <th>iter<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="a-function-to-train-the-neural-network" class="section level4" number="12.4.4.2">
<h4><span class="header-section-number">12.4.4.2</span> A function to train the neural network</h4>
<p>Now, we convert the script above to a function, so we could reuse it several times. We want to study the effect of the iteration on the performance of the algorithm.</p>
<p>This time we create a function <code>train</code> to input the number of iterations that we want to run:</p>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb525-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="cf">function</span>(iterations) {</span>
<span id="cb525-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Randomly initialize weights</span></span>
<span id="cb525-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-3" aria-hidden="true" tabindex="-1"></a>    w1 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(D_in, H, <span class="at">device=</span>device)   <span class="co"># layer 1</span></span>
<span id="cb525-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-4" aria-hidden="true" tabindex="-1"></a>    w2 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(H, D_out, <span class="at">device=</span>device)  <span class="co"># layer 2</span></span>
<span id="cb525-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb525-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-6" aria-hidden="true" tabindex="-1"></a>    learning_rate <span class="ot">=</span> <span class="fl">1e-6</span></span>
<span id="cb525-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loop</span></span>
<span id="cb525-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iterations) {</span>
<span id="cb525-9"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-9" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Forward pass: compute predicted y</span></span>
<span id="cb525-10"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-10" aria-hidden="true" tabindex="-1"></a>      h <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">mm</span>(w1)</span>
<span id="cb525-11"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-11" aria-hidden="true" tabindex="-1"></a>      h_relu <span class="ot">&lt;-</span> h<span class="sc">$</span><span class="fu">clamp</span>(<span class="at">min=</span><span class="dv">0</span>)</span>
<span id="cb525-12"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-12" aria-hidden="true" tabindex="-1"></a>      y_pred <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">mm</span>(w2)</span>
<span id="cb525-13"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb525-14"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-14" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Compute and print loss; loss is a scalar stored in a PyTorch Tensor</span></span>
<span id="cb525-15"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-15" aria-hidden="true" tabindex="-1"></a>      <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></span>
<span id="cb525-16"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-16" aria-hidden="true" tabindex="-1"></a>      loss <span class="ot">&lt;-</span> (torch<span class="sc">$</span><span class="fu">sub</span>(y_pred, y))<span class="sc">$</span><span class="fu">pow</span>(<span class="dv">2</span>)<span class="sc">$</span><span class="fu">sum</span>()</span>
<span id="cb525-17"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-17" aria-hidden="true" tabindex="-1"></a>      <span class="co"># cat(t, &quot;\t&quot;); cat(loss$item(), &quot;\n&quot;)</span></span>
<span id="cb525-18"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb525-19"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-19" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span>
<span id="cb525-20"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-20" aria-hidden="true" tabindex="-1"></a>      grad_y_pred <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">mul</span>(torch<span class="sc">$</span><span class="fu">scalar_tensor</span>(<span class="fl">2.0</span>), torch<span class="sc">$</span><span class="fu">sub</span>(y_pred, y))</span>
<span id="cb525-21"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-21" aria-hidden="true" tabindex="-1"></a>      grad_w2 <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_y_pred)</span>
<span id="cb525-22"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-22" aria-hidden="true" tabindex="-1"></a>      grad_h_relu <span class="ot">&lt;-</span> grad_y_pred<span class="sc">$</span><span class="fu">mm</span>(w2<span class="sc">$</span><span class="fu">t</span>())</span>
<span id="cb525-23"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-23" aria-hidden="true" tabindex="-1"></a>      grad_h <span class="ot">&lt;-</span> grad_h_relu<span class="sc">$</span><span class="fu">clone</span>()</span>
<span id="cb525-24"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-24" aria-hidden="true" tabindex="-1"></a>      mask <span class="ot">&lt;-</span> grad_h<span class="sc">$</span><span class="fu">lt</span>(<span class="dv">0</span>)</span>
<span id="cb525-25"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-25" aria-hidden="true" tabindex="-1"></a>      torch<span class="sc">$</span><span class="fu">masked_select</span>(grad_h, mask)<span class="sc">$</span><span class="fu">fill_</span>(<span class="fl">0.0</span>)</span>
<span id="cb525-26"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-26" aria-hidden="true" tabindex="-1"></a>      grad_w1 <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_h)</span>
<span id="cb525-27"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-27" aria-hidden="true" tabindex="-1"></a>       </span>
<span id="cb525-28"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-28" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Update weights using gradient descent</span></span>
<span id="cb525-29"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-29" aria-hidden="true" tabindex="-1"></a>      w1 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">sub</span>(w1, torch<span class="sc">$</span><span class="fu">mul</span>(learning_rate, grad_w1))</span>
<span id="cb525-30"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-30" aria-hidden="true" tabindex="-1"></a>      w2 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">sub</span>(w2, torch<span class="sc">$</span><span class="fu">mul</span>(learning_rate, grad_w2))</span>
<span id="cb525-31"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-31" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb525-32"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-32" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data.frame</span>(<span class="at">y =</span> y<span class="sc">$</span><span class="fu">flatten</span>()<span class="sc">$</span><span class="fu">numpy</span>(), </span>
<span id="cb525-33"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-33" aria-hidden="true" tabindex="-1"></a>                        <span class="at">y_pred =</span> y_pred<span class="sc">$</span><span class="fu">flatten</span>()<span class="sc">$</span><span class="fu">numpy</span>(), <span class="at">iter =</span> iterations)</span>
<span id="cb525-34"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb525-34" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="run-it-at-100-iterations" class="section level4" number="12.4.4.3">
<h4><span class="header-section-number">12.4.4.3</span> Run it at 100 iterations</h4>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb526-1" aria-hidden="true" tabindex="-1"></a><span class="co"># retrieve the results and store them in a dataframe</span></span>
<span id="cb526-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb526-2" aria-hidden="true" tabindex="-1"></a>df_100 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">iterations =</span> <span class="dv">100</span>)</span>
<span id="cb526-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb526-3" aria-hidden="true" tabindex="-1"></a><span class="fu">datatable</span>(df_100, <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">10</span>))</span>
<span id="cb526-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb526-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb526-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb526-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_100, <span class="fu">aes</span>(<span class="at">x =</span> y_pred, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb526-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb526-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>()</span></code></pre></div>
<p><div id="htmlwidget-a5e68a2cd2425dc65a1f" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-a5e68a2cd2425dc65a1f">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640"],[0.83994026831249,-0.00261469246496478,1.17331508976272,0.113064550776486,0.0103083101232198,0.741725300575136,0.757130556967624,-1.19798639091765,0.977469466953348,0.958784411472249,0.0208831428895932,-1.64816158891565,-0.0500028243495128,0.733813527145886,1.58085339989783,1.37525327872787,-0.386739371622718,-0.360212335648884,0.757263768677633,-2.33097748052543,-0.280231384151112,-0.958825812111196,-0.0248397757648495,1.0914318283759,-0.458192050288552,1.88579895675686,1.8374918101401,-0.102590503582433,0.158376834397865,-1.69505820821086,0.0630930916536182,1.3418620933296,0.173417858163622,-0.972149178523607,-0.312413823093578,2.16446071258402,-1.08569798526669,-0.570029361135791,0.180937256764405,-1.26229551939497,-1.36950465873129,-1.15493391741737,1.40785825831085,0.781023621423221,0.0780186960888742,2.11273169799433,-0.382805598728771,-0.332249209115966,0.833881240069712,0.59587340242551,-1.10900749622674,0.768900089169295,-1.35001396009045,0.483618658476077,-0.286807646207213,0.141828938278373,-0.374918514638461,-0.231434565088581,-0.00992419102991136,-2.11812756250465,-0.523762933650836,-1.32458158157352,0.431681037219444,0.925057143386304,0.93777525245556,0.546488768273407,1.11596946687424,0.570940155822875,-1.05140854030057,-0.318810634218592,-0.289448897409276,-0.418406412091964,0.781141791150043,-0.114891413907268,-0.845416526254121,-0.91845936893655,1.20789854386747,0.741312355103571,0.634427251618938,2.20585574787314,0.356161490453256,0.267242739519324,0.484831528360182,-0.165738361704601,0.189930678043718,-0.93651989042175,-1.45410391289995,0.233684800337273,0.502157215971716,0.28920149072855,-1.22551276737874,-0.385846452553303,1.42231442741027,-1.7116822994054,0.155053435413657,1.75487195590539,-0.480695737266194,-0.744335584259619,-2.13245053322139,-0.364322343399947,-0.233973774563064,0.111681118770621,-0.88914370304214,-0.722085375742162,-2.32991274785241,-0.479068930295669,0.789931238294979,1.06380136135981,-0.0798134682221333,-0.374168001227348,0.122704935131071,-1.43690716269235,-0.853944502445023,0.456289787735743,-0.0142516296486563,0.816751922214445,0.0849881110641102,0.446428132382165,2.42338976222122,-0.246995607668513,0.276176450241468,0.930116509469416,-1.02544718711831,0.104887979909664,-0.0142328430202254,2.03937753638296,0.373422932290684,1.43684984339667,0.948776626114924,-1.31827952307809,0.292905851185605,0.147308597580878,-1.03373092754513,0.419607277988786,-0.261092633509276,1.6071495149865,-0.647906145700861,0.657156199635267,-0.57098265859795,-0.818600322951857,1.31069089175351,1.81375842080092,-0.943433957369921,0.857269379202826,-0.263283789177259,1.31296346802011,-0.955590491841724,-1.56443546642085,-2.17226562414852,1.45591099037169,1.03814194129823,-0.197772383407226,-0.650297223745001,-1.33005360476313,0.921942504799078,0.752209085763665,-0.0361295533712819,2.63443405115608,0.287654631188146,0.655186314323695,0.400648171873719,-0.307311020383442,-0.0880147158656964,-1.41821667150147,-0.313515957169452,1.02977246630598,1.63557919370632,0.471406593501004,0.996474278664091,0.682501811766753,-0.971734007624617,-0.654540506542262,1.99500285811451,-1.64399263553111,-0.663557865235524,-1.68572596581282,-0.00487618116822535,0.458801236543731,0.5824022309138,-1.36287855122539,-1.27186820648675,0.714902868782501,-1.0165666698022,-0.0235973711331876,-0.0710369673541219,0.142253138818817,-0.675694865931503,-0.256829936236055,-0.101224481444322,-2.15395839517849,-0.964817551233336,-0.58900570028774,-1.77786400328839,1.14147095872602,-0.32772078240833,-1.35670287649227,0.415691882771873,-0.925932387905733,-1.61418322141891,-0.0867796005444939,0.601846649732015,1.12863379512717,0.184978687339329,0.156848912253547,-0.484058595314498,1.12522934475228,0.243459583230952,-0.634978878232694,-1.25090253023824,-1.43563909830578,1.13586385748573,0.215411392403429,1.86239774513426,1.11499996333691,-0.674626192034018,-0.434131329001274,1.03699553226858,-0.0366741372856878,-0.402271366973104,2.10246137335328,0.459018936796291,-0.25503953202401,-0.672610058718924,1.09980827026172,-0.410925835278326,-0.896790320270794,-1.03109344053961,-0.262748015975908,-0.333069074621615,0.977294210489194,0.678715733607743,1.25873526914719,0.42400531072313,1.79340828861104,0.294284737340495,-1.51557096175814,-0.566556739572426,0.0695601296483909,1.20367672097015,-0.549833339055226,0.120064475138402,0.307536795611604,1.21556005041361,0.12602924786391,-1.47214013475447,-0.318687822897892,0.279546357353257,2.70523892501022,1.68587064051207,1.1239941108417,0.424088292374247,0.104431077381569,-1.18021245837732,-0.916706926774142,-1.56221497392067,-0.851852947077733,-1.11863086095433,0.148312190622533,0.502631432428189,1.58774800017961,-1.10166271639973,-0.777449867610656,0.942870568479463,-1.9339574431339,0.00698050744702491,0.673684269774716,-1.39732869525216,0.283501034389694,0.874755970337202,-0.417645129576752,-1.36874292450096,-0.0747812970509281,0.0105724695173432,-0.0456309112554566,-1.47339841259021,1.33700556433092,1.94209421947211,0.284457070328279,-1.98482664413377,-0.50080737939018,-0.274676881900125,0.710938213804541,2.25851536328219,0.618237686663071,0.230927765626917,1.10122470908834,-0.572603422146998,-0.53302471140162,-0.717679874784871,-0.877038791682033,-0.748485888179382,0.184771684002625,-0.22231743531996,-1.00929695552901,-1.37047063999704,-1.07025776684948,-0.0066305826821924,0.00463777722911298,0.200669539977365,0.809383668396162,0.287278665935646,-0.303985246895298,0.139672450093511,0.127722862850529,-0.95565825905712,-0.799909006767028,-0.202940490973971,-2.41551300196281,0.3750531988884,2.0938362702894,0.605430623566889,-0.816263936823711,1.4935718662945,2.15379218446882,1.28063423391778,-1.16859150067533,-0.495931078057793,-1.12572347427127,-0.0289018780637711,-0.491217346285856,0.582465462265955,-1.73320910486715,-0.250616430567373,1.37773646638411,0.505183750595998,1.03329001588251,-0.148129803407604,-1.02024054280369,-1.16747702799092,-0.698262649012394,-0.29088420462642,-0.0873481684419898,2.28367061778488,0.799274120229441,0.496859660507907,-1.16771232655097,0.0560084744790094,0.0697664131694416,-0.63172486335937,-1.61579250963407,0.822541288820032,-0.585096534350738,-0.316981493648855,-0.445729843878749,-2.50838624530302,-1.39899463359675,-1.31102892051884,0.750402144358772,0.703404121659897,-0.25083983221813,-0.25095095379119,-0.872342684631607,0.628192536989848,-0.936217675649455,-0.550371466231215,-0.26970983693769,-0.688604841006888,-0.390605966140334,1.06988684039523,0.498849800595456,0.333419121051892,0.910556713584374,-1.27892253213922,-1.84182185333004,-0.512035156980371,-0.364119099300724,0.459286557995962,-0.0437559190006266,-0.453340737020902,2.02055656551793,1.47116567306761,-0.0115038122635589,0.131722917926454,-2.28429446135232,0.669356739346067,-1.14593193034066,-1.91291968267857,-1.67212741707599,0.54073309384294,-0.256724509418031,-1.89134380794797,0.161915204911539,-0.458809181262073,-0.194078077908355,0.296793193074685,-0.0250807732722297,-1.05794861180844,0.24484382118802,-0.648883034347149,0.618876699768757,1.44663045545316,1.76518871577172,-1.92334252201028,0.202212741132606,-0.342486128124399,-0.119772048017177,0.266562321266478,0.722029306985646,0.403631540478367,-1.9875531289521,-0.781171921052648,0.987488573315664,0.308690446320804,0.106148749253679,0.185085607568157,-0.390160876288962,1.25429513802065,-1.35013386465017,-0.342974766571647,-0.232894537629491,0.980332519980097,-2.11322741735,0.23024995771627,-1.45705269393795,1.38625292848739,1.6457152752694,0.886358263746316,0.296335125360452,-0.442944664529715,0.274627958703436,1.37404495680148,-1.61754075111507,-1.01356309656133,0.34151499813101,0.60561927302074,0.466358958410998,1.18788000370773,-0.998222009747409,0.472411048704897,0.802363134564957,0.778863518671328,1.33312992214265,-0.11483784787192,-1.24014458172761,1.68097845650274,-0.156295579276488,-0.36167688476951,1.12990338986994,0.544741774675208,-0.739519141431302,0.348344512782951,-0.10457968657704,-1.54862790298088,0.991859873430099,-0.712951101315977,-0.688713554688238,-0.019645395217261,-0.4276668324237,0.485554217337533,-1.22769785113341,-0.0839624525226322,0.929646496319533,-1.04399669558851,1.5862211735349,-0.762919277781499,-0.0482214954780706,0.482863741201961,0.272647805689435,-0.529870425645623,-2.32824908375099,0.0361073596021704,-0.451810115323141,-0.282254053923175,1.54911580277808,0.253374436546217,2.1590870111341,1.00793354498743,1.2238963859792,-0.262656713944417,0.371188568837448,1.84975076615656,-0.32057748245843,-0.484757190293574,0.119115296354525,0.311425735299618,2.86340591290978,0.561681683625529,-1.36773128004884,-0.521412125803238,1.8438836271771,0.15291957051073,-1.32938108961612,1.12345625118363,-0.0988379062841592,-0.781398448382246,1.13052436749178,0.767681931298354,0.525356584083132,0.582232899579348,0.333066984200484,-0.806258638926797,0.556552894435339,-1.30665318755998,0.785186326876701,-0.394106214647196,0.550730858543516,0.774230360594257,1.25782444024924,-1.59645579014647,0.726215347670778,-0.63694860577194,1.27493610125473,-0.0352439186929309,2.89168261447142,1.37501515427315,-0.0491456451207457,-0.0546139127595145,0.260510410587632,0.981880273079814,0.14277683851885,-0.254490023068397,-1.08396906594294,0.271948629915079,0.377547175371029,0.981224651122629,1.80754059091588,-1.1073823023655,0.482139414365283,1.8188538137154,-0.58749687491936,1.1093609884837,-0.297608176884006,2.57784620418014,2.52204599693865,-0.384840198277502,-0.855562334499144,0.938512269689899,-0.468196363866475,0.260008292451711,-0.462982716485726,-0.886275855262118,1.12411879590505,-0.909935517062699,-0.751334836930073,0.514366756631972,0.195860128347231,1.55471787782953,-0.253467892483979,-0.944270533093175,0.0655037556538774,0.669046506691992,-0.329793629158233,1.30135750702321,1.40114405750616,0.796269800703752,-0.10853585230126,1.29063556898416,-0.693149828347284,1.15340154198528,-1.67650319533252,-1.49429593747768,-1.273469908539,1.31161289723836,-1.31181443286064,-0.950226359402931,0.144663430311268,-1.37563897316271,-0.229895425421889,0.419692024617746,0.516239563506011,0.740669495728343,-0.361952001881317,-1.60995844987436,0.327757025857471,1.6432950871785,1.20535256388868,-0.353510280844418,1.21473389654699,-1.53240226188522,1.92233286176372,-1.09253954985412,-1.59184155325035,-0.64646780319668,-0.426883748541837,0.209569862271875,0.0415756254512372,1.33238165750511,-0.681027400604152,0.620571329940411,-0.900045782913094,0.309651318876625,0.118175407084332,0.227812270228395,-1.18930438569969,0.0837636991730383,0.588009163095429,-1.5092271780105,0.995898635439402,-0.00639701815793872,-0.3502219106773,1.4288096764155,-0.372525850460849,-1.86657115947184,1.73533502830238,1.4419101995773,-0.0083570684921407,1.3092622868551,0.000392255402139403,-1.45683485419048,-0.844996213277658,-0.60477119564568,-0.0517937465881769,0.821273104631407,1.50318458611856,0.26033919453097,0.521628843921831,-0.525259906808531,-1.07777898882352,-0.340595705086819,-0.0688224810312909,0.024696280077251,1.76093398424226,0.175448866859079,0.138406693673293,-0.206063763800319,-0.435211393481228,0.529893499013784,0.967447102676697,-2.43106301578848,-0.0424933305199116,-1.99868317055989,-2.47557651859656,-0.965041073936391,-0.435050040400478,-0.507170912986636,-1.37604070291559,0.641342899780833,0.15067215935219,0.647532140383024,-1.21460078326391,1.46931171546606,-1.78034791140857,0.39149639637274,0.953942227218803,1.27980367071278,0.263325295304111,-1.03877191179501,-0.962563912956559,0.895371173457089,0.905913690706474],[1.45455392903027,0.12797988614814,2.11576802619241,0.721590332592219,0.718727176390161,0.803945842125628,0.597823461346522,-0.906152421533704,0.881329706879939,1.01558387658974,0.50141567271585,-1.77181442248485,-2.17330423610894,0.531164425633954,1.32996798083001,1.12126640454507,-0.817779552351656,-0.475105646588482,0.483436672821705,-2.56896265138982,0.317092358915196,-2.73138669617811,1.37759381450212,1.25689413289042,-0.353259778222334,0.863237379295018,2.0076989533811,-1.36479900824125,0.885083779192689,-1.45357689323875,-0.054965818324213,0.651636871017602,0.492698843424188,-0.478886130712471,0.181665004812252,1.92884553005478,-1.30723002058802,-1.18049689058773,0.289360563463123,-0.833819138141955,-0.749931483644517,-1.54194698426978,1.24376330960567,1.0489399763066,0.860154184925422,1.73411218509724,-0.611696074239276,-0.346032671845947,0.850541980347568,0.96865433825316,-0.652146087946362,-0.79525338503359,0.064527006301951,2.07323829593545,0.800007614995842,-0.641451027758441,0.0307467462618156,-1.73207068466645,0.913499820117744,-1.75470463366409,-1.00171898465549,0.488653821720707,0.341772257061322,-0.0191690432348937,0.168221574568596,1.93677353424089,1.52478452124729,1.70711699931757,-1.21379895471538,-0.579687709547013,1.08176331486209,-2.0484954117167,2.31484013187917,0.583789305251481,0.212293255523622,-1.46726595789446,2.14968307907627,-0.429631173327884,1.01015924271914,2.81228395233803,-1.67041252198137,2.02265997192767,3.14453514348799,0.272801658808231,1.11925472235877,-1.1809214158851,-2.24649601764902,0.211977866013172,-0.114010406794155,-0.373567670849202,-1.42324822194105,0.903949132050102,-0.107839925428909,-3.51575943775946,-1.34177456131273,2.53613269956277,0.0830842520009174,0.423020598577507,-2.91457644972354,-1.07695804093016,-0.408504292314587,0.275481290724981,-1.03467107620129,-0.780661908801691,-2.58845821883299,-0.497057679416884,0.679043470307581,1.00939587674768,-0.233394365500754,-0.535670660164211,0.799305122095876,-1.6452793401825,-0.181881907294338,0.876129109537612,0.601309604984976,0.173340568439798,-0.343956933997942,0.543563152474956,2.52803452499357,-0.118459711932324,2.32243515374651,-0.0293466551201454,-4.49055909475209,-1.84227509395357,-1.0019072677499,1.56068978336838,0.747921442158856,1.69971029037942,0.772172301978836,-1.41331907870855,0.533558691832109,0.342638451384691,-1.88817354239879,0.201734093551886,-1.29866349567305,1.98826124584684,-0.389845075695877,0.681704581822673,-0.635053550371995,-1.14187024975347,-0.0295016596040654,2.89691548422349,0.909776629752306,1.44510775292414,-0.194461974747602,1.63069755406737,-1.33918535897071,-1.39061859749572,-2.13192118086289,1.13678589532441,0.652947681553834,-0.585548446252683,0.161569261096538,-0.86186667596288,1.21048323698702,0.674846879718994,-0.0428775308316389,2.09717722239237,0.424909359777882,0.968902144687329,0.720648389503125,0.338956573481511,-3.79896497297371,-2.89481154898868,-1.83730957559214,0.767066388013232,1.12529337195701,1.90865934638168,-0.00934110260504184,0.0241562430869422,-2.50421354345063,0.241307626630338,4.8145661016289,-0.536173434119028,-0.34823740167003,-0.364574779887146,-0.248153578822541,0.219908049768737,1.05008298690295,-2.18279433458071,-1.16107569537403,-0.349225490518172,-1.09601040931305,0.413091574945792,0.314851237634135,-0.595926499161411,-0.974984776581215,-0.723730784957866,0.0947309321319488,-1.86336720469738,-1.98556616046284,1.76749958910263,-1.66106034852562,0.88774962117723,-1.73687535461983,0.78553245614627,1.13801632985198,0.415236190863387,-1.51036257259181,-2.10222190817497,1.19545563290235,0.884275256003735,0.261501403287497,0.380178597713983,0.372055616829014,0.685339080094025,0.0154980926015842,-0.4730130495356,-1.15844711794675,-0.731533158823604,0.0982469612561379,0.538262146139794,4.10958694104804,1.53491312612802,-0.140240593356193,-0.181076455899157,0.393692328701354,-0.258512537968757,-0.278839196975222,1.96728059785325,1.26718946002212,-1.18932931116709,-1.41506446773377,1.56564741934369,-0.191624299598216,-0.887110658167666,-1.22602745291921,-0.282281827229364,-0.164211825870995,1.7498580768097,-0.115580533187076,1.49362246759966,1.77858941393825,2.893739077284,0.830891209413581,-1.50504382066175,-1.01150158127992,0.300426257442835,1.54446061356606,-0.214719834979775,-4.09948263758041,3.26016855273212,3.20708042457382,1.98475596005341,-1.67419300199418,0.907736080076942,-1.55698251828933,4.28431201979003,1.3801146305027,0.717444239964258,2.87707017259639,-1.62999689002427,-3.14909123556605,-0.87699129678682,0.584516240230801,-3.29004776983301,-0.693737679417396,-2.09837321699321,0.83498590533027,2.73881473941956,-0.763514133444936,-0.956473611468942,0.227499512869325,-2.40064381397692,-0.3443266833515,0.620190330234696,-0.915252864804323,0.129227172400089,0.815139684592003,-0.661560912298214,-0.63858500635955,-0.645483846394341,-1.66742917561221,-0.866855661558458,-1.89701242725756,0.900688814417872,2.27471261649346,-0.00214390383310887,-2.35315324485581,-0.433600728272183,-1.19405178237168,2.29626674253041,4.43487216639768,0.681710701915403,-0.300485831740403,2.19198870250149,-0.00220785701127468,-0.163547702042475,-0.650277521706906,-1.45041401902209,1.21908260933439,-0.223586434197832,-3.44183843448319,-3.7262418350096,-2.42977014635481,-1.19857353592337,0.527858325814708,0.0905821559381097,-0.415973609380482,0.375218540334056,1.52391430619354,-1.78336572638265,-1.45481773949947,0.206169053099616,-0.386800100736468,-2.0972368696761,-0.14284300946451,-2.96566427471436,0.79427546062199,2.59625203241023,0.482252436633166,-0.784720647164995,1.57038775879246,2.17170726395749,1.17291993020313,-0.999568131866035,-0.129302721084848,-1.16342328775627,0.0243019729863776,-0.356863839457359,0.138204614080564,-2.62181799626037,1.64013909572143,3.07155874612327,1.67377416699435,0.492394737183615,-0.745320929216272,-1.75137110253138,-0.52911140194209,0.126975656902036,-1.49486860049751,1.10372145153649,4.63069446895067,1.26390429598712,0.56156058424592,-0.449559254059586,-0.255761499677121,0.40896368645307,-0.622867006537271,-1.90262025556079,1.00713858724759,-0.604883686281946,-1.83973191348294,-0.853855309695861,-2.86573285671042,-1.82240757965388,-1.24797095603543,1.1512724419703,0.432004132079212,0.130236362987493,0.319992469896359,-0.775551080535539,-1.56979998341232,-2.59794594646739,-1.47923054092287,0.131541236467926,-0.0417306289959143,0.166039690906494,0.675080522806063,0.352458861799967,-0.628918675127883,1.35826785972959,0.409451590302957,-0.549846718180778,-0.212768151768054,-0.138415522213455,0.394440584490884,0.110802190981339,-0.0292900077336293,2.31933419150071,2.31614280334719,-1.05347670593294,0.143648912722378,-1.27836808988432,1.79000580579184,-2.33139543754849,-2.6679882597543,-1.5805681822613,0.553766185402471,0.992420070337207,-2.87590010010744,1.3122469393592,1.06636122126482,0.240472130173282,0.603525789113669,0.630511249917536,-0.947996001186143,0.474034921568299,-0.371864801261553,0.177580435769475,1.57538126714231,2.33290987889675,-2.20001580420098,-0.408190793581904,-0.618930368177423,-0.159840744357735,0.0417192575833099,1.45203748443876,0.0024107812531533,-2.05292213844459,0.320641156128794,-0.350133576403522,-0.949457969300993,-0.237461614905247,0.283470835577784,-1.36849060405784,1.57486432513346,-2.38700299403074,-0.689571185212417,0.2069917453243,0.496021588232181,-2.27133889982981,0.240172936158149,-1.74374183787739,1.191567501037,1.52894377916155,0.722441743400314,0.380572349630757,-0.397124998263771,0.446177543910551,2.30721755084266,-2.6073036002372,-0.274966720829298,0.576495626031113,1.73883465600719,-0.281731180738404,1.26966273936002,-1.81465370996851,1.03504240186829,1.55808908828967,-0.314541666801345,2.53654633667741,-1.19360888543313,-1.95575776327441,0.380293921771475,0.883024267143001,-0.333975690256931,1.84000568194361,-0.143271545103193,-1.22147602952042,1.28440120358816,-1.75540337357656,-2.68176433579537,0.729280864852566,-0.906549175756879,-1.34329143605228,0.123089731334208,-1.54280026706535,0.582232567049608,-1.31218691044883,-0.420630093317769,1.27283892499423,-1.30740926390193,1.15780414938549,-0.969906283286529,-0.114758343783201,0.249644012583886,0.47866606718852,-0.759522837390978,-2.41137942969571,-0.0745249021320156,-0.742815077254259,-0.614445891129364,0.997923179020242,0.100083644735658,1.97599343577645,0.942661552609941,1.01379485170839,-0.498172929921028,0.849627483684362,3.61512216837094,-1.9779065843687,-2.23063491164493,-0.621891715604255,-0.0822411984597954,2.35350505966306,0.824667213801033,-1.87483323087202,-0.487203980108428,2.38577502475488,-0.523199638894077,-0.726507414404603,2.45298614370977,0.860396900571404,-0.110055407855852,1.28588225032212,0.34105720284624,0.605377593150335,0.721367313961674,0.54779439774661,-1.01421454710783,2.50927772890169,-0.0526863973410912,0.00405369128727173,-0.0718443107416977,1.07237846841923,1.14129713910932,2.31009413407531,-2.09400137470918,0.456663798419704,-2.05975288684791,1.87581503623655,2.59853847379167,4.60136078507921,2.21698131858044,0.72611520976325,-0.599810130945391,0.35431150178438,1.87948293047603,0.23836968220494,-0.713775183119392,-0.604800438491786,1.41015571001828,0.585320930719898,0.871318647182824,2.20488299671373,-0.905842377790062,0.368898385076719,1.95550307555843,-0.852661457313609,2.16609494241623,-2.40814694374319,1.66722484427432,3.95022102628247,-0.224828330587115,-1.45054315335316,0.673972954807567,-1.15694441492167,1.21741454828681,0.539789278198338,-1.66442938337561,1.62032494568482,0.312940739364698,-0.284046938397534,0.716033039646398,0.24020977758773,1.47233769756668,-0.194790937304189,-0.861548194980068,-0.280305765621907,-0.400783911151905,-0.0549159865720696,2.24952703129322,2.26871913956658,0.935238743598322,0.221751661563001,0.836068369093221,-0.968649584523536,1.35489158742572,-1.9340864388034,-1.46613404227312,-0.0520950140496197,2.54842161913707,-0.821571654630096,-0.969645038717308,1.45741046120891,-1.02059475018,0.369639267299899,0.320503524070299,-0.0219988188576394,-0.966019536349998,0.442317259580218,0.205616202338405,1.23819690775901,1.86837067274742,2.09506832443394,-0.801652955390921,1.40664441930205,-1.45812241776302,1.74825092341351,0.0483990439653681,-3.57321874602451,-2.24003034446224,0.0898891125258849,0.551344440954126,-0.320407001905911,1.50560335886083,-1.33189253414684,0.972670768806129,-0.21288615821371,-0.523813652756469,0.22046971572109,1.12597543686784,-0.329113110118786,0.00299606810824787,1.05097137088762,-1.52434677327582,0.962409482895295,0.364286359190405,-0.398763687573374,1.22172508835382,-1.10005583179787,-1.12944949344244,2.44620890006626,1.76391759810651,-0.266254565831573,1.09376037814424,-0.565286286921964,-1.04893457766946,-0.898431187485722,0.0962036420034482,-0.283099620545382,-0.0122664644881138,0.930619262132318,-0.166737790575625,0.352367380068031,-0.187621917616961,-0.751548050496365,-0.394702548395364,-0.384307588424715,1.88359886684136,1.11680786047882,-2.33830334608041,-1.32145777297885,-0.868277734201207,-0.958066533569083,1.57101119098721,0.748832192667491,-3.12325778802293,0.1606898088406,-1.86207835076338,-2.75837542867779,-2.8167053284867,-1.75357085148441,-1.51555079796267,-1.60236484047338,0.834266455442922,0.138222031389198,0.369755084286667,-1.54496025448783,4.6831745032001,-2.38573023625455,-7.44924962851401,-3.70339096894283,-0.192019527368001,-0.689648536676621,-0.166685459988505,0.384542797501801,-0.804133539516759,0.165326348978837],[100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>y<\/th>\n      <th>y_pred<\/th>\n      <th>iter<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script><img src="rtorch-minimal-book_files/figure-html/dt-100-iters-2.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="iterations" class="section level4" number="12.4.4.4">
<h4><span class="header-section-number">12.4.4.4</span> 250 iterations</h4>
<p>Still there are differences between the value and the prediction. Let’s try with more iterations, like <strong>250</strong>:</p>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb527-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb527-1" aria-hidden="true" tabindex="-1"></a>df_250 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">iterations =</span> <span class="dv">200</span>)</span>
<span id="cb527-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb527-2" aria-hidden="true" tabindex="-1"></a><span class="fu">datatable</span>(df_250, <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">25</span>))</span>
<span id="cb527-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb527-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb527-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb527-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_250, <span class="fu">aes</span>(<span class="at">x =</span> y_pred, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb527-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb527-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>()</span></code></pre></div>
<p><div id="htmlwidget-e1231a09600ae0410e72" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-e1231a09600ae0410e72">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640"],[0.83994026831249,-0.00261469246496478,1.17331508976272,0.113064550776486,0.0103083101232198,0.741725300575136,0.757130556967624,-1.19798639091765,0.977469466953348,0.958784411472249,0.0208831428895932,-1.64816158891565,-0.0500028243495128,0.733813527145886,1.58085339989783,1.37525327872787,-0.386739371622718,-0.360212335648884,0.757263768677633,-2.33097748052543,-0.280231384151112,-0.958825812111196,-0.0248397757648495,1.0914318283759,-0.458192050288552,1.88579895675686,1.8374918101401,-0.102590503582433,0.158376834397865,-1.69505820821086,0.0630930916536182,1.3418620933296,0.173417858163622,-0.972149178523607,-0.312413823093578,2.16446071258402,-1.08569798526669,-0.570029361135791,0.180937256764405,-1.26229551939497,-1.36950465873129,-1.15493391741737,1.40785825831085,0.781023621423221,0.0780186960888742,2.11273169799433,-0.382805598728771,-0.332249209115966,0.833881240069712,0.59587340242551,-1.10900749622674,0.768900089169295,-1.35001396009045,0.483618658476077,-0.286807646207213,0.141828938278373,-0.374918514638461,-0.231434565088581,-0.00992419102991136,-2.11812756250465,-0.523762933650836,-1.32458158157352,0.431681037219444,0.925057143386304,0.93777525245556,0.546488768273407,1.11596946687424,0.570940155822875,-1.05140854030057,-0.318810634218592,-0.289448897409276,-0.418406412091964,0.781141791150043,-0.114891413907268,-0.845416526254121,-0.91845936893655,1.20789854386747,0.741312355103571,0.634427251618938,2.20585574787314,0.356161490453256,0.267242739519324,0.484831528360182,-0.165738361704601,0.189930678043718,-0.93651989042175,-1.45410391289995,0.233684800337273,0.502157215971716,0.28920149072855,-1.22551276737874,-0.385846452553303,1.42231442741027,-1.7116822994054,0.155053435413657,1.75487195590539,-0.480695737266194,-0.744335584259619,-2.13245053322139,-0.364322343399947,-0.233973774563064,0.111681118770621,-0.88914370304214,-0.722085375742162,-2.32991274785241,-0.479068930295669,0.789931238294979,1.06380136135981,-0.0798134682221333,-0.374168001227348,0.122704935131071,-1.43690716269235,-0.853944502445023,0.456289787735743,-0.0142516296486563,0.816751922214445,0.0849881110641102,0.446428132382165,2.42338976222122,-0.246995607668513,0.276176450241468,0.930116509469416,-1.02544718711831,0.104887979909664,-0.0142328430202254,2.03937753638296,0.373422932290684,1.43684984339667,0.948776626114924,-1.31827952307809,0.292905851185605,0.147308597580878,-1.03373092754513,0.419607277988786,-0.261092633509276,1.6071495149865,-0.647906145700861,0.657156199635267,-0.57098265859795,-0.818600322951857,1.31069089175351,1.81375842080092,-0.943433957369921,0.857269379202826,-0.263283789177259,1.31296346802011,-0.955590491841724,-1.56443546642085,-2.17226562414852,1.45591099037169,1.03814194129823,-0.197772383407226,-0.650297223745001,-1.33005360476313,0.921942504799078,0.752209085763665,-0.0361295533712819,2.63443405115608,0.287654631188146,0.655186314323695,0.400648171873719,-0.307311020383442,-0.0880147158656964,-1.41821667150147,-0.313515957169452,1.02977246630598,1.63557919370632,0.471406593501004,0.996474278664091,0.682501811766753,-0.971734007624617,-0.654540506542262,1.99500285811451,-1.64399263553111,-0.663557865235524,-1.68572596581282,-0.00487618116822535,0.458801236543731,0.5824022309138,-1.36287855122539,-1.27186820648675,0.714902868782501,-1.0165666698022,-0.0235973711331876,-0.0710369673541219,0.142253138818817,-0.675694865931503,-0.256829936236055,-0.101224481444322,-2.15395839517849,-0.964817551233336,-0.58900570028774,-1.77786400328839,1.14147095872602,-0.32772078240833,-1.35670287649227,0.415691882771873,-0.925932387905733,-1.61418322141891,-0.0867796005444939,0.601846649732015,1.12863379512717,0.184978687339329,0.156848912253547,-0.484058595314498,1.12522934475228,0.243459583230952,-0.634978878232694,-1.25090253023824,-1.43563909830578,1.13586385748573,0.215411392403429,1.86239774513426,1.11499996333691,-0.674626192034018,-0.434131329001274,1.03699553226858,-0.0366741372856878,-0.402271366973104,2.10246137335328,0.459018936796291,-0.25503953202401,-0.672610058718924,1.09980827026172,-0.410925835278326,-0.896790320270794,-1.03109344053961,-0.262748015975908,-0.333069074621615,0.977294210489194,0.678715733607743,1.25873526914719,0.42400531072313,1.79340828861104,0.294284737340495,-1.51557096175814,-0.566556739572426,0.0695601296483909,1.20367672097015,-0.549833339055226,0.120064475138402,0.307536795611604,1.21556005041361,0.12602924786391,-1.47214013475447,-0.318687822897892,0.279546357353257,2.70523892501022,1.68587064051207,1.1239941108417,0.424088292374247,0.104431077381569,-1.18021245837732,-0.916706926774142,-1.56221497392067,-0.851852947077733,-1.11863086095433,0.148312190622533,0.502631432428189,1.58774800017961,-1.10166271639973,-0.777449867610656,0.942870568479463,-1.9339574431339,0.00698050744702491,0.673684269774716,-1.39732869525216,0.283501034389694,0.874755970337202,-0.417645129576752,-1.36874292450096,-0.0747812970509281,0.0105724695173432,-0.0456309112554566,-1.47339841259021,1.33700556433092,1.94209421947211,0.284457070328279,-1.98482664413377,-0.50080737939018,-0.274676881900125,0.710938213804541,2.25851536328219,0.618237686663071,0.230927765626917,1.10122470908834,-0.572603422146998,-0.53302471140162,-0.717679874784871,-0.877038791682033,-0.748485888179382,0.184771684002625,-0.22231743531996,-1.00929695552901,-1.37047063999704,-1.07025776684948,-0.0066305826821924,0.00463777722911298,0.200669539977365,0.809383668396162,0.287278665935646,-0.303985246895298,0.139672450093511,0.127722862850529,-0.95565825905712,-0.799909006767028,-0.202940490973971,-2.41551300196281,0.3750531988884,2.0938362702894,0.605430623566889,-0.816263936823711,1.4935718662945,2.15379218446882,1.28063423391778,-1.16859150067533,-0.495931078057793,-1.12572347427127,-0.0289018780637711,-0.491217346285856,0.582465462265955,-1.73320910486715,-0.250616430567373,1.37773646638411,0.505183750595998,1.03329001588251,-0.148129803407604,-1.02024054280369,-1.16747702799092,-0.698262649012394,-0.29088420462642,-0.0873481684419898,2.28367061778488,0.799274120229441,0.496859660507907,-1.16771232655097,0.0560084744790094,0.0697664131694416,-0.63172486335937,-1.61579250963407,0.822541288820032,-0.585096534350738,-0.316981493648855,-0.445729843878749,-2.50838624530302,-1.39899463359675,-1.31102892051884,0.750402144358772,0.703404121659897,-0.25083983221813,-0.25095095379119,-0.872342684631607,0.628192536989848,-0.936217675649455,-0.550371466231215,-0.26970983693769,-0.688604841006888,-0.390605966140334,1.06988684039523,0.498849800595456,0.333419121051892,0.910556713584374,-1.27892253213922,-1.84182185333004,-0.512035156980371,-0.364119099300724,0.459286557995962,-0.0437559190006266,-0.453340737020902,2.02055656551793,1.47116567306761,-0.0115038122635589,0.131722917926454,-2.28429446135232,0.669356739346067,-1.14593193034066,-1.91291968267857,-1.67212741707599,0.54073309384294,-0.256724509418031,-1.89134380794797,0.161915204911539,-0.458809181262073,-0.194078077908355,0.296793193074685,-0.0250807732722297,-1.05794861180844,0.24484382118802,-0.648883034347149,0.618876699768757,1.44663045545316,1.76518871577172,-1.92334252201028,0.202212741132606,-0.342486128124399,-0.119772048017177,0.266562321266478,0.722029306985646,0.403631540478367,-1.9875531289521,-0.781171921052648,0.987488573315664,0.308690446320804,0.106148749253679,0.185085607568157,-0.390160876288962,1.25429513802065,-1.35013386465017,-0.342974766571647,-0.232894537629491,0.980332519980097,-2.11322741735,0.23024995771627,-1.45705269393795,1.38625292848739,1.6457152752694,0.886358263746316,0.296335125360452,-0.442944664529715,0.274627958703436,1.37404495680148,-1.61754075111507,-1.01356309656133,0.34151499813101,0.60561927302074,0.466358958410998,1.18788000370773,-0.998222009747409,0.472411048704897,0.802363134564957,0.778863518671328,1.33312992214265,-0.11483784787192,-1.24014458172761,1.68097845650274,-0.156295579276488,-0.36167688476951,1.12990338986994,0.544741774675208,-0.739519141431302,0.348344512782951,-0.10457968657704,-1.54862790298088,0.991859873430099,-0.712951101315977,-0.688713554688238,-0.019645395217261,-0.4276668324237,0.485554217337533,-1.22769785113341,-0.0839624525226322,0.929646496319533,-1.04399669558851,1.5862211735349,-0.762919277781499,-0.0482214954780706,0.482863741201961,0.272647805689435,-0.529870425645623,-2.32824908375099,0.0361073596021704,-0.451810115323141,-0.282254053923175,1.54911580277808,0.253374436546217,2.1590870111341,1.00793354498743,1.2238963859792,-0.262656713944417,0.371188568837448,1.84975076615656,-0.32057748245843,-0.484757190293574,0.119115296354525,0.311425735299618,2.86340591290978,0.561681683625529,-1.36773128004884,-0.521412125803238,1.8438836271771,0.15291957051073,-1.32938108961612,1.12345625118363,-0.0988379062841592,-0.781398448382246,1.13052436749178,0.767681931298354,0.525356584083132,0.582232899579348,0.333066984200484,-0.806258638926797,0.556552894435339,-1.30665318755998,0.785186326876701,-0.394106214647196,0.550730858543516,0.774230360594257,1.25782444024924,-1.59645579014647,0.726215347670778,-0.63694860577194,1.27493610125473,-0.0352439186929309,2.89168261447142,1.37501515427315,-0.0491456451207457,-0.0546139127595145,0.260510410587632,0.981880273079814,0.14277683851885,-0.254490023068397,-1.08396906594294,0.271948629915079,0.377547175371029,0.981224651122629,1.80754059091588,-1.1073823023655,0.482139414365283,1.8188538137154,-0.58749687491936,1.1093609884837,-0.297608176884006,2.57784620418014,2.52204599693865,-0.384840198277502,-0.855562334499144,0.938512269689899,-0.468196363866475,0.260008292451711,-0.462982716485726,-0.886275855262118,1.12411879590505,-0.909935517062699,-0.751334836930073,0.514366756631972,0.195860128347231,1.55471787782953,-0.253467892483979,-0.944270533093175,0.0655037556538774,0.669046506691992,-0.329793629158233,1.30135750702321,1.40114405750616,0.796269800703752,-0.10853585230126,1.29063556898416,-0.693149828347284,1.15340154198528,-1.67650319533252,-1.49429593747768,-1.273469908539,1.31161289723836,-1.31181443286064,-0.950226359402931,0.144663430311268,-1.37563897316271,-0.229895425421889,0.419692024617746,0.516239563506011,0.740669495728343,-0.361952001881317,-1.60995844987436,0.327757025857471,1.6432950871785,1.20535256388868,-0.353510280844418,1.21473389654699,-1.53240226188522,1.92233286176372,-1.09253954985412,-1.59184155325035,-0.64646780319668,-0.426883748541837,0.209569862271875,0.0415756254512372,1.33238165750511,-0.681027400604152,0.620571329940411,-0.900045782913094,0.309651318876625,0.118175407084332,0.227812270228395,-1.18930438569969,0.0837636991730383,0.588009163095429,-1.5092271780105,0.995898635439402,-0.00639701815793872,-0.3502219106773,1.4288096764155,-0.372525850460849,-1.86657115947184,1.73533502830238,1.4419101995773,-0.0083570684921407,1.3092622868551,0.000392255402139403,-1.45683485419048,-0.844996213277658,-0.60477119564568,-0.0517937465881769,0.821273104631407,1.50318458611856,0.26033919453097,0.521628843921831,-0.525259906808531,-1.07777898882352,-0.340595705086819,-0.0688224810312909,0.024696280077251,1.76093398424226,0.175448866859079,0.138406693673293,-0.206063763800319,-0.435211393481228,0.529893499013784,0.967447102676697,-2.43106301578848,-0.0424933305199116,-1.99868317055989,-2.47557651859656,-0.965041073936391,-0.435050040400478,-0.507170912986636,-1.37604070291559,0.641342899780833,0.15067215935219,0.647532140383024,-1.21460078326391,1.46931171546606,-1.78034791140857,0.39149639637274,0.953942227218803,1.27980367071278,0.263325295304111,-1.03877191179501,-0.962563912956559,0.895371173457089,0.905913690706474],[0.897436443974734,-0.00573164532681636,1.16704995490863,0.212278744761686,0.0756390693607625,0.754902054697905,0.76144129358992,-1.18106963563546,0.971614093926518,1.07397979515228,0.00228724938646607,-1.64234976433326,0.0137546258807806,0.725670508284708,1.57675197073114,1.35809039914424,-0.325871647748838,-0.3861269206941,0.767225424261325,-2.31780402518193,0.113149344738536,-0.986438140434021,-0.147134539796859,1.28448439148749,-0.383636959394146,2.15496081187829,1.75839656708145,0.139217071571821,0.155734123163164,-1.52370702909961,0.0322021354100189,1.34872155124557,0.204123994740862,-1.0292089845585,-0.323708527533709,2.11055763946066,-1.04633453427138,-0.670347954090247,0.193516149386633,-1.30242300544333,-1.44629050477384,-1.15431476244419,1.41747007359662,0.69291577231257,0.010662918127945,2.06557038804559,-0.388325672288275,-0.36419034192449,0.843474231317671,0.49490833300461,-1.23392054328951,0.762352302149601,-1.3505185263397,0.442574467579867,-0.350563853641727,0.0563301812175556,-0.400542639380551,-0.261886516391467,-0.0236227575010055,-2.2007881979132,-0.674403063759589,-1.29173058052973,0.475803221826965,0.827335219933058,0.873568832165635,0.394380294782454,1.16611632231363,0.4055658390999,-1.02561675134655,-0.409869481104284,-0.360985839835809,-0.395929904073761,0.802723332603962,-0.0832605860733349,-0.848222451918073,-0.958012748296243,1.21989344429344,0.740110919883941,0.638189390096503,2.23117958248002,0.502572099498382,0.242474273820953,0.435548208459526,-0.094621679607684,0.215655447098428,-0.812252816450879,-1.48583235406037,0.326842797282787,0.479866307392683,0.327890117053631,-1.08496602629759,-0.395634458955572,1.3903051051168,-1.60729196463155,0.223387338123863,1.88030446287102,-0.490435617409331,-0.652159560023242,-2.14623552357811,-0.272655911965783,-0.119887185805341,0.0851941718069504,-0.888922847792074,-0.619023097099795,-2.27160382388594,-0.332940343231401,0.790762068295594,1.22287449301604,-0.0841670808805475,-0.280100197246299,0.0804089187859711,-1.44163303983825,-0.844153937691885,0.451968281396681,-0.0372871793726484,0.797050686304044,0.0862070947305047,0.453178119204423,2.40891309179385,-0.257394497041917,0.222730991935315,0.921715612021018,-1.026562557289,0.0601875545812316,-0.0462109548540521,2.00245080416641,0.362631462733849,1.40642070543057,0.948271871742641,-1.36857173331829,0.275285415875804,0.150435637675772,-1.02718912980375,0.336830852502647,-0.283652002367926,1.52605035500338,-0.606880172863111,0.546131743480711,-0.556727694549394,-0.873034309250376,1.31528663880689,1.78446876607982,-0.990279345637203,0.835861148061657,-0.31373673113724,1.35833674077208,-1.01376049716464,-1.45622152169513,-2.21366439681935,1.36809181243623,1.05255494099371,-0.200133208981647,-0.70104503996939,-1.36857000567183,0.906312867609053,0.746119004809717,-0.0637196150205952,2.64077334173352,0.299480043479349,0.633560305392727,0.403494472216811,-0.310813331529225,-0.064551325522995,-1.38547973157574,-0.321851305976551,1.07512894501737,1.63355763086559,0.545355036535071,0.981040780631417,0.677723496806557,-0.934297704211023,-0.655192124986513,2.00754089661637,-1.642599752295,-0.655794898200771,-1.67091682158094,0.0176797932619834,0.443834812335425,0.592450475870578,-1.35002942499524,-1.2122559762959,0.720512541975289,-1.10766392471761,0.00980736189576703,-0.0489694612511933,0.181622252484399,-0.726230564980023,-0.221375981974021,-0.100811956251498,-2.10859694691991,-0.99653977356513,-0.56233063150642,-1.71732083633339,1.12132011619491,-0.298901721295536,-1.42249084238895,0.477872217827378,-1.01459978671242,-1.58739635691516,-0.0449164886408528,0.568522654333388,1.11929957116057,0.209794453871911,0.12148943705597,-0.519753896459122,1.11745686001149,0.264161979487866,-0.673089550259263,-1.24832839376201,-1.48219922493508,1.27786969889678,0.184805943535483,1.8259874156138,1.16815051452094,-0.61672730195291,-0.322394861437131,1.04949449244314,0.0334780408105971,-0.414525732628858,2.13691263347971,0.406878782530673,-0.271846445184543,-0.70485138557919,0.978827143582322,-0.455637366397511,-1.01766523847267,-1.0042172511465,-0.370928131496918,-0.324788280107867,0.931055129145747,0.761177377453689,1.23421480095982,0.39759906227067,1.7691864119401,0.28251639048294,-1.47228272096581,-0.57495310051355,0.103337955770286,1.20088432521218,-0.548864120867539,0.220529022327095,0.287313281335527,1.15049740127436,0.182156173509373,-1.4631513114641,-0.181853733817322,0.173904366629008,2.8404671997613,1.69174061952079,1.08623430648753,0.418196859063146,0.0884774075885915,-1.20986414735557,-0.977847393008949,-1.61918982792997,-0.828761148731328,-1.13502040753993,0.166427813458354,0.493653218025049,1.48341814930648,-1.03620322450007,-0.805498033759639,0.910323245770426,-1.82610785899137,0.0220880993478696,0.764153688031616,-1.44136864500189,0.447570869984804,0.837605011503949,-0.33605148306388,-1.55518141860249,-0.103050912783665,0.00805583774877672,-0.315009969755071,-1.68162055786504,1.09628354160676,1.9019263871563,0.105936193407553,-1.99302916302599,-0.817563155881552,-0.275072254573816,0.739786544860332,2.26080486076798,0.611347288300198,0.280472484062912,1.05480267934489,-0.535926011532244,-0.65411710197921,-0.667624284669041,-0.830131605593433,-0.898602600677749,0.210689648627174,-0.171373192038917,-1.04207955113279,-1.39983916108506,-1.18826069198896,0.000232222921050851,-0.0837553075088113,0.194207066116152,0.75102126113346,0.410800406419069,-0.353952321267116,0.137270603685272,0.134278109285541,-0.946741688547103,-0.690202557406568,-0.226586568377231,-2.295170284884,0.357237436614488,2.0946190357201,0.364700060189584,-0.74341870791951,1.56179611323678,2.03377758758448,1.2460179024942,-1.42099684010023,-0.44684982880818,-1.39457063597948,0.0252938896685038,-0.555549342698711,0.520734692673534,-1.70462826400679,-0.228701670688881,1.46480251942872,0.537775926841082,1.02959669724962,-0.164586151663018,-0.998887304104237,-1.16856228004476,-0.629795896206124,-0.292296677827613,-0.0746619911395676,2.26883470845661,0.74242242267835,0.492647339443131,-1.20592239840538,0.046788028928779,-0.0271566286081555,-0.588748203825553,-1.63346882700324,0.763887005722043,-0.55587124215814,-0.307960625569751,-0.490324100073416,-2.48889335457985,-1.48525581311031,-1.28318884113507,0.650363297073962,0.725634608205837,-0.253792180787994,-0.363108609156972,-0.862619879818016,0.707101894801644,-1.00074120334557,-0.579715764378039,-0.309506212311133,-0.644623344437331,-0.48226370222881,1.07014316738104,0.415875535083515,0.39392439234349,0.94587325567182,-1.29235625778983,-1.79188244367212,-0.450005328461572,-0.337299118269094,0.471717207876577,-0.0806262024754574,-0.429423941760035,2.11240262152498,1.49314121785428,-0.0119341175638423,0.0930683305875455,-2.27287700560812,0.686709980392871,-1.12152093185963,-1.94105211520962,-1.64628336271747,0.546076711873654,-0.241456111843842,-1.9859703748598,0.188370567636213,-0.387172358511775,-0.245821991678211,0.336092719533541,-0.176675606906826,-0.983450525312458,0.0263331730511371,-0.598229408675216,0.633472529020336,1.32232956138309,1.77568659737415,-1.85301418705185,0.137043462497044,-0.436172615418694,-0.211437688064912,0.26587430868543,0.680831378632687,0.401729594685733,-2.06860589546355,-0.643207362901433,0.986688180126404,0.196731504117861,0.218392274406099,0.241547941109122,-0.237220903812885,1.2136263869808,-1.17063477513449,-0.376062441184638,-0.134599736324994,1.05171161458092,-2.0995793784622,0.232927372083904,-1.27243328897669,1.48687028013173,1.80801651037462,0.872258403661893,0.545262217737305,-0.472344008135935,0.420561298756812,1.30481626926328,-1.59346667122739,-0.977552881862514,0.318611288958536,0.592645923682101,0.416768607831035,1.22419296828023,-1.08097025227261,0.485188048724992,0.781323179590413,0.841710738566319,1.29815193694786,-0.108163548132662,-1.21170302906668,1.69439885101922,-0.0358600093017944,-0.362023172095019,1.24736237065224,0.504138792325278,-0.791285885032277,0.512417446510058,-0.155787542962674,-1.6365008740736,1.06257093292241,-0.725677177791562,-0.523727246726672,-0.102489695814917,-0.192014713487202,0.389899759848729,-1.19049745353822,0.0196424720453605,0.910495047769913,-1.11910972225237,1.57764553125073,-0.771095427578319,0.00838612656673177,0.435431877706022,0.376746762626986,-0.539321751540015,-2.3119608239648,0.0306969911549525,-0.459322654358363,-0.297137431857868,1.55145196896432,0.265261830994994,2.14749940393432,0.997135361104125,1.23125365659883,-0.272522742619088,0.376655817229603,1.73272381000392,-0.312046161995568,-0.483466819257821,0.112300557005462,0.28670875365059,2.79949745645602,0.539270016708426,-1.36818720052372,-0.54260221199038,1.79702635575515,-0.0202644530717131,-1.30350953179318,1.13890683611307,-0.182946540054623,-0.829085675346292,0.982685132078231,0.766332254393808,0.377303699161878,0.595480209916612,0.250006820325454,-0.754960937987928,0.542900868153819,-1.33546151278129,0.74235910316326,-0.390564147345363,0.55959537285123,0.772780175880703,1.24547502352742,-1.59710870061554,0.71210107228969,-0.703736449970977,1.2907266639852,0.0115706572727028,2.83924327114345,1.3627236800189,-0.11777034816561,-0.0293839593523177,0.142810004567991,1.0219860944428,0.101634126238224,-0.212879401274969,-1.0907636153293,0.252470265027553,0.352987986146158,1.00880150062922,1.78735471688111,-1.10707312357342,0.426404130081515,1.83338346929716,-0.554775001812926,1.07951120228445,-0.296342028774632,2.61121783338565,2.55441174938835,-0.367128123646815,-0.841402784270501,0.952241573466095,-0.435496072285758,0.248708531587646,-0.43927415467065,-0.865887315855812,1.11939329737872,-0.923414980339588,-0.763693267367149,0.536630964170417,0.188892605021888,1.57355688165237,-0.273365195549106,-0.938960210282744,0.100221179173179,0.721069265823925,-0.34243537689116,1.29969000763806,1.38792984240825,0.796259357429992,-0.0900989310358469,1.29625071329073,-0.688046638201698,1.14860020279796,-1.67072810455328,-1.55314269813796,-1.2589456242836,1.38178299108832,-1.34804099406872,-0.979247852705683,0.0823077617877281,-1.33030750400801,-0.327002539563617,0.43023025095553,0.491576951569503,0.766348157449777,-0.33340609919642,-1.60204813843976,0.392911623928855,1.71801254420035,1.22347908341701,-0.352315103229946,1.23367646348511,-1.50256178896702,2.01214188954758,-1.20115751658138,-1.55926370553399,-0.588363565097399,-0.416313443693095,0.227732900550809,-0.0270828624440895,1.35641718981622,-0.744272338907745,0.643924661114426,-0.899856991151598,0.446811182715322,0.0848028561313896,0.176501740118866,-1.15160014518823,0.108340795695337,0.665621416980549,-1.5424387666305,1.07640323361577,-0.0201286241787315,-0.290913632161526,1.51242647531108,-0.384238130223423,-1.88633210118264,1.83531236700317,1.48142036366956,0.105157864339243,1.29821094027813,0.110438538353298,-1.48451750001151,-0.780295329691326,-0.751223862405793,-0.0234270493949506,0.904972312142703,1.59953932103168,0.235471053064394,0.521125664545547,-0.531527890026723,-1.00631379029691,-0.354196918558056,-0.0447354192767007,-0.0194858343144599,1.75795675834546,0.170322862594368,0.0842906841774678,-0.236958851954988,-0.49400429635072,0.530806509085963,0.914898606151127,-2.42347199790927,-0.0851871420800538,-1.93693714643409,-2.48668078337466,-0.962686096643631,-0.482774592684947,-0.515682081219001,-1.3375222293987,0.647555343076353,0.138636258402414,0.658106091348476,-1.24611580606341,1.34889216905492,-1.75013055203202,0.447338059004643,1.02207539910965,1.30146644408026,0.213313558700655,-1.00399455210898,-0.945411835582604,0.876959574750515,0.978999472420586],[200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>y<\/th>\n      <th>y_pred<\/th>\n      <th>iter<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":25,"columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script><img src="rtorch-minimal-book_files/figure-html/dt-250-iters-2.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We see the formation of a line between the values and prediction, which means we are getting closer at finding the right algorithm, in this particular case, weights and bias.</p>
</div>
<div id="iterations-1" class="section level4" number="12.4.4.5">
<h4><span class="header-section-number">12.4.4.5</span> 500 iterations</h4>
<p>Let’s try one more time with 500 iterations:</p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-1" aria-hidden="true" tabindex="-1"></a>df_500 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">iterations =</span> <span class="dv">500</span>)</span>
<span id="cb528-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-2" aria-hidden="true" tabindex="-1"></a><span class="fu">datatable</span>(df_500, <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">25</span>))</span>
<span id="cb528-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_500, <span class="fu">aes</span>(<span class="at">x =</span> y_pred, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb528-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb528-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>()</span></code></pre></div>
<p><div id="htmlwidget-5fc805ffa30354cf995b" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-5fc805ffa30354cf995b">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640"],[0.83994026831249,-0.00261469246496478,1.17331508976272,0.113064550776486,0.0103083101232198,0.741725300575136,0.757130556967624,-1.19798639091765,0.977469466953348,0.958784411472249,0.0208831428895932,-1.64816158891565,-0.0500028243495128,0.733813527145886,1.58085339989783,1.37525327872787,-0.386739371622718,-0.360212335648884,0.757263768677633,-2.33097748052543,-0.280231384151112,-0.958825812111196,-0.0248397757648495,1.0914318283759,-0.458192050288552,1.88579895675686,1.8374918101401,-0.102590503582433,0.158376834397865,-1.69505820821086,0.0630930916536182,1.3418620933296,0.173417858163622,-0.972149178523607,-0.312413823093578,2.16446071258402,-1.08569798526669,-0.570029361135791,0.180937256764405,-1.26229551939497,-1.36950465873129,-1.15493391741737,1.40785825831085,0.781023621423221,0.0780186960888742,2.11273169799433,-0.382805598728771,-0.332249209115966,0.833881240069712,0.59587340242551,-1.10900749622674,0.768900089169295,-1.35001396009045,0.483618658476077,-0.286807646207213,0.141828938278373,-0.374918514638461,-0.231434565088581,-0.00992419102991136,-2.11812756250465,-0.523762933650836,-1.32458158157352,0.431681037219444,0.925057143386304,0.93777525245556,0.546488768273407,1.11596946687424,0.570940155822875,-1.05140854030057,-0.318810634218592,-0.289448897409276,-0.418406412091964,0.781141791150043,-0.114891413907268,-0.845416526254121,-0.91845936893655,1.20789854386747,0.741312355103571,0.634427251618938,2.20585574787314,0.356161490453256,0.267242739519324,0.484831528360182,-0.165738361704601,0.189930678043718,-0.93651989042175,-1.45410391289995,0.233684800337273,0.502157215971716,0.28920149072855,-1.22551276737874,-0.385846452553303,1.42231442741027,-1.7116822994054,0.155053435413657,1.75487195590539,-0.480695737266194,-0.744335584259619,-2.13245053322139,-0.364322343399947,-0.233973774563064,0.111681118770621,-0.88914370304214,-0.722085375742162,-2.32991274785241,-0.479068930295669,0.789931238294979,1.06380136135981,-0.0798134682221333,-0.374168001227348,0.122704935131071,-1.43690716269235,-0.853944502445023,0.456289787735743,-0.0142516296486563,0.816751922214445,0.0849881110641102,0.446428132382165,2.42338976222122,-0.246995607668513,0.276176450241468,0.930116509469416,-1.02544718711831,0.104887979909664,-0.0142328430202254,2.03937753638296,0.373422932290684,1.43684984339667,0.948776626114924,-1.31827952307809,0.292905851185605,0.147308597580878,-1.03373092754513,0.419607277988786,-0.261092633509276,1.6071495149865,-0.647906145700861,0.657156199635267,-0.57098265859795,-0.818600322951857,1.31069089175351,1.81375842080092,-0.943433957369921,0.857269379202826,-0.263283789177259,1.31296346802011,-0.955590491841724,-1.56443546642085,-2.17226562414852,1.45591099037169,1.03814194129823,-0.197772383407226,-0.650297223745001,-1.33005360476313,0.921942504799078,0.752209085763665,-0.0361295533712819,2.63443405115608,0.287654631188146,0.655186314323695,0.400648171873719,-0.307311020383442,-0.0880147158656964,-1.41821667150147,-0.313515957169452,1.02977246630598,1.63557919370632,0.471406593501004,0.996474278664091,0.682501811766753,-0.971734007624617,-0.654540506542262,1.99500285811451,-1.64399263553111,-0.663557865235524,-1.68572596581282,-0.00487618116822535,0.458801236543731,0.5824022309138,-1.36287855122539,-1.27186820648675,0.714902868782501,-1.0165666698022,-0.0235973711331876,-0.0710369673541219,0.142253138818817,-0.675694865931503,-0.256829936236055,-0.101224481444322,-2.15395839517849,-0.964817551233336,-0.58900570028774,-1.77786400328839,1.14147095872602,-0.32772078240833,-1.35670287649227,0.415691882771873,-0.925932387905733,-1.61418322141891,-0.0867796005444939,0.601846649732015,1.12863379512717,0.184978687339329,0.156848912253547,-0.484058595314498,1.12522934475228,0.243459583230952,-0.634978878232694,-1.25090253023824,-1.43563909830578,1.13586385748573,0.215411392403429,1.86239774513426,1.11499996333691,-0.674626192034018,-0.434131329001274,1.03699553226858,-0.0366741372856878,-0.402271366973104,2.10246137335328,0.459018936796291,-0.25503953202401,-0.672610058718924,1.09980827026172,-0.410925835278326,-0.896790320270794,-1.03109344053961,-0.262748015975908,-0.333069074621615,0.977294210489194,0.678715733607743,1.25873526914719,0.42400531072313,1.79340828861104,0.294284737340495,-1.51557096175814,-0.566556739572426,0.0695601296483909,1.20367672097015,-0.549833339055226,0.120064475138402,0.307536795611604,1.21556005041361,0.12602924786391,-1.47214013475447,-0.318687822897892,0.279546357353257,2.70523892501022,1.68587064051207,1.1239941108417,0.424088292374247,0.104431077381569,-1.18021245837732,-0.916706926774142,-1.56221497392067,-0.851852947077733,-1.11863086095433,0.148312190622533,0.502631432428189,1.58774800017961,-1.10166271639973,-0.777449867610656,0.942870568479463,-1.9339574431339,0.00698050744702491,0.673684269774716,-1.39732869525216,0.283501034389694,0.874755970337202,-0.417645129576752,-1.36874292450096,-0.0747812970509281,0.0105724695173432,-0.0456309112554566,-1.47339841259021,1.33700556433092,1.94209421947211,0.284457070328279,-1.98482664413377,-0.50080737939018,-0.274676881900125,0.710938213804541,2.25851536328219,0.618237686663071,0.230927765626917,1.10122470908834,-0.572603422146998,-0.53302471140162,-0.717679874784871,-0.877038791682033,-0.748485888179382,0.184771684002625,-0.22231743531996,-1.00929695552901,-1.37047063999704,-1.07025776684948,-0.0066305826821924,0.00463777722911298,0.200669539977365,0.809383668396162,0.287278665935646,-0.303985246895298,0.139672450093511,0.127722862850529,-0.95565825905712,-0.799909006767028,-0.202940490973971,-2.41551300196281,0.3750531988884,2.0938362702894,0.605430623566889,-0.816263936823711,1.4935718662945,2.15379218446882,1.28063423391778,-1.16859150067533,-0.495931078057793,-1.12572347427127,-0.0289018780637711,-0.491217346285856,0.582465462265955,-1.73320910486715,-0.250616430567373,1.37773646638411,0.505183750595998,1.03329001588251,-0.148129803407604,-1.02024054280369,-1.16747702799092,-0.698262649012394,-0.29088420462642,-0.0873481684419898,2.28367061778488,0.799274120229441,0.496859660507907,-1.16771232655097,0.0560084744790094,0.0697664131694416,-0.63172486335937,-1.61579250963407,0.822541288820032,-0.585096534350738,-0.316981493648855,-0.445729843878749,-2.50838624530302,-1.39899463359675,-1.31102892051884,0.750402144358772,0.703404121659897,-0.25083983221813,-0.25095095379119,-0.872342684631607,0.628192536989848,-0.936217675649455,-0.550371466231215,-0.26970983693769,-0.688604841006888,-0.390605966140334,1.06988684039523,0.498849800595456,0.333419121051892,0.910556713584374,-1.27892253213922,-1.84182185333004,-0.512035156980371,-0.364119099300724,0.459286557995962,-0.0437559190006266,-0.453340737020902,2.02055656551793,1.47116567306761,-0.0115038122635589,0.131722917926454,-2.28429446135232,0.669356739346067,-1.14593193034066,-1.91291968267857,-1.67212741707599,0.54073309384294,-0.256724509418031,-1.89134380794797,0.161915204911539,-0.458809181262073,-0.194078077908355,0.296793193074685,-0.0250807732722297,-1.05794861180844,0.24484382118802,-0.648883034347149,0.618876699768757,1.44663045545316,1.76518871577172,-1.92334252201028,0.202212741132606,-0.342486128124399,-0.119772048017177,0.266562321266478,0.722029306985646,0.403631540478367,-1.9875531289521,-0.781171921052648,0.987488573315664,0.308690446320804,0.106148749253679,0.185085607568157,-0.390160876288962,1.25429513802065,-1.35013386465017,-0.342974766571647,-0.232894537629491,0.980332519980097,-2.11322741735,0.23024995771627,-1.45705269393795,1.38625292848739,1.6457152752694,0.886358263746316,0.296335125360452,-0.442944664529715,0.274627958703436,1.37404495680148,-1.61754075111507,-1.01356309656133,0.34151499813101,0.60561927302074,0.466358958410998,1.18788000370773,-0.998222009747409,0.472411048704897,0.802363134564957,0.778863518671328,1.33312992214265,-0.11483784787192,-1.24014458172761,1.68097845650274,-0.156295579276488,-0.36167688476951,1.12990338986994,0.544741774675208,-0.739519141431302,0.348344512782951,-0.10457968657704,-1.54862790298088,0.991859873430099,-0.712951101315977,-0.688713554688238,-0.019645395217261,-0.4276668324237,0.485554217337533,-1.22769785113341,-0.0839624525226322,0.929646496319533,-1.04399669558851,1.5862211735349,-0.762919277781499,-0.0482214954780706,0.482863741201961,0.272647805689435,-0.529870425645623,-2.32824908375099,0.0361073596021704,-0.451810115323141,-0.282254053923175,1.54911580277808,0.253374436546217,2.1590870111341,1.00793354498743,1.2238963859792,-0.262656713944417,0.371188568837448,1.84975076615656,-0.32057748245843,-0.484757190293574,0.119115296354525,0.311425735299618,2.86340591290978,0.561681683625529,-1.36773128004884,-0.521412125803238,1.8438836271771,0.15291957051073,-1.32938108961612,1.12345625118363,-0.0988379062841592,-0.781398448382246,1.13052436749178,0.767681931298354,0.525356584083132,0.582232899579348,0.333066984200484,-0.806258638926797,0.556552894435339,-1.30665318755998,0.785186326876701,-0.394106214647196,0.550730858543516,0.774230360594257,1.25782444024924,-1.59645579014647,0.726215347670778,-0.63694860577194,1.27493610125473,-0.0352439186929309,2.89168261447142,1.37501515427315,-0.0491456451207457,-0.0546139127595145,0.260510410587632,0.981880273079814,0.14277683851885,-0.254490023068397,-1.08396906594294,0.271948629915079,0.377547175371029,0.981224651122629,1.80754059091588,-1.1073823023655,0.482139414365283,1.8188538137154,-0.58749687491936,1.1093609884837,-0.297608176884006,2.57784620418014,2.52204599693865,-0.384840198277502,-0.855562334499144,0.938512269689899,-0.468196363866475,0.260008292451711,-0.462982716485726,-0.886275855262118,1.12411879590505,-0.909935517062699,-0.751334836930073,0.514366756631972,0.195860128347231,1.55471787782953,-0.253467892483979,-0.944270533093175,0.0655037556538774,0.669046506691992,-0.329793629158233,1.30135750702321,1.40114405750616,0.796269800703752,-0.10853585230126,1.29063556898416,-0.693149828347284,1.15340154198528,-1.67650319533252,-1.49429593747768,-1.273469908539,1.31161289723836,-1.31181443286064,-0.950226359402931,0.144663430311268,-1.37563897316271,-0.229895425421889,0.419692024617746,0.516239563506011,0.740669495728343,-0.361952001881317,-1.60995844987436,0.327757025857471,1.6432950871785,1.20535256388868,-0.353510280844418,1.21473389654699,-1.53240226188522,1.92233286176372,-1.09253954985412,-1.59184155325035,-0.64646780319668,-0.426883748541837,0.209569862271875,0.0415756254512372,1.33238165750511,-0.681027400604152,0.620571329940411,-0.900045782913094,0.309651318876625,0.118175407084332,0.227812270228395,-1.18930438569969,0.0837636991730383,0.588009163095429,-1.5092271780105,0.995898635439402,-0.00639701815793872,-0.3502219106773,1.4288096764155,-0.372525850460849,-1.86657115947184,1.73533502830238,1.4419101995773,-0.0083570684921407,1.3092622868551,0.000392255402139403,-1.45683485419048,-0.844996213277658,-0.60477119564568,-0.0517937465881769,0.821273104631407,1.50318458611856,0.26033919453097,0.521628843921831,-0.525259906808531,-1.07777898882352,-0.340595705086819,-0.0688224810312909,0.024696280077251,1.76093398424226,0.175448866859079,0.138406693673293,-0.206063763800319,-0.435211393481228,0.529893499013784,0.967447102676697,-2.43106301578848,-0.0424933305199116,-1.99868317055989,-2.47557651859656,-0.965041073936391,-0.435050040400478,-0.507170912986636,-1.37604070291559,0.641342899780833,0.15067215935219,0.647532140383024,-1.21460078326391,1.46931171546606,-1.78034791140857,0.39149639637274,0.953942227218803,1.27980367071278,0.263325295304111,-1.03877191179501,-0.962563912956559,0.895371173457089,0.905913690706474],[0.839932083113229,-0.00259619575570724,1.17331875830589,0.113064853997152,0.0103188408022312,0.741730549571095,0.757139315220673,-1.1979585642083,0.977472444600714,0.958780004495488,0.0209043778917284,-1.64817402225043,-0.0500123823042407,0.733826408637011,1.58084379965554,1.37529194360209,-0.386758689325045,-0.360229939684986,0.75726596971024,-2.33097394681097,-0.28023045729139,-0.958896373504533,-0.0247948874747308,1.09135671877051,-0.458233065262505,1.88574293845085,1.83748538301571,-0.10269992208068,0.158405783847478,-1.69502511639939,0.0630936664527901,1.34160813963699,0.173548139921491,-0.972376097928475,-0.312463456287841,2.16409325051556,-1.08577887826851,-0.57028789678145,0.180972504748574,-1.26224450313448,-1.36950062446783,-1.15494789238635,1.40786462474275,0.780996277883688,0.0780012128835708,2.11271991948848,-0.38281333192004,-0.332286779965352,0.83387514747181,0.595894760761364,-1.10901558012019,0.76889765357855,-1.35000699101211,0.483613872486434,-0.286816095207973,0.141814757334077,-0.374908317643377,-0.231457800767783,-0.00993505878796319,-2.11811637248354,-0.523760472269744,-1.32451166383348,0.431617828221304,0.925129854529925,0.937815581227895,0.546667033934277,1.11596160336218,0.571012747407542,-1.05139158044448,-0.318833937329815,-0.289450131337257,-0.418459527782833,0.781183380744015,-0.1149353328362,-0.845410111277191,-0.918561707122719,1.20789792574915,0.741276696600633,0.634434905822908,2.20587145424331,0.356171415690241,0.267280033337379,0.48478693722155,-0.165691443963463,0.189975025790473,-0.936440990539855,-1.45409590225692,0.233745739025089,0.502176454824307,0.289190010569165,-1.22548216865263,-0.385807730207461,1.42226130045688,-1.71162285591952,0.155044458412901,1.75503816113058,-0.480722488857238,-0.744304527358111,-2.13246177835875,-0.364333842791981,-0.233970596383521,0.1116552876897,-0.889142659818381,-0.722113112123722,-2.32990048286897,-0.479104114510719,0.789919021068577,1.0637834803655,-0.0798101922004832,-0.374171319370674,0.122698258480503,-1.43685071466214,-0.853964975576879,0.456336199334345,-0.0142109637053025,0.816793898996973,0.085028488103422,0.446528776544943,2.42339999017534,-0.247038844219709,0.276162201768619,0.930090356923472,-1.02540698614981,0.104848658466018,-0.0142222739042879,2.03928705968095,0.373436572710254,1.43683521212533,0.948795154781408,-1.31829417508879,0.292902269705186,0.147265447664105,-1.03369488968684,0.419601802757468,-0.261162368350372,1.6071471870927,-0.647923730709571,0.657055310952635,-0.570982301172996,-0.818555322700843,1.31069933692464,1.81366947360739,-0.943343708080052,0.857176148739541,-0.263359586071546,1.31275783833103,-0.955585470275064,-1.56458519654681,-2.17227423027208,1.45594946795327,1.03812355282464,-0.197778432257518,-0.650260810605454,-1.3301046528089,0.921943613382478,0.752099897059493,-0.0361035298042234,2.63442460113075,0.287654542337149,0.655203442279387,0.400669025844581,-0.307231297204811,-0.0880933318525797,-1.41809584491349,-0.31347025836409,1.02993899352873,1.6355704175802,0.471511110548705,0.996478130401157,0.682477793970827,-0.971708681426662,-0.65450279742376,1.99494025044558,-1.64394404461995,-0.663526676792189,-1.68555668161936,-0.00489117748333534,0.458902497940064,0.582408678848566,-1.36291538884019,-1.27188088771661,0.714905835134157,-1.01655910086838,-0.023614495267573,-0.0710216477946553,0.142202031986045,-0.675672283372061,-0.256807070402076,-0.101221701228051,-2.1539764756439,-0.964812687483203,-0.589022829042261,-1.77784923392804,1.14145209968898,-0.32772935335484,-1.35675349495688,0.415693384037447,-0.925950582336065,-1.61418987119824,-0.0867621696327845,0.601844691761713,1.12868261468677,0.184951361817587,0.156884087063474,-0.484045795498496,1.12529073867861,0.243476753083698,-0.63494120665051,-1.25091171530579,-1.43564313901365,1.13585625836996,0.21538252089311,1.86241874634674,1.11497613857237,-0.674655227172929,-0.434182962583379,1.03698751936033,-0.0367039940475243,-0.402276938339191,2.10245597019278,0.459003897168055,-0.255092979231915,-0.672555892991442,1.09970638415548,-0.410934296895169,-0.896927707406917,-1.03108492017272,-0.262795193602412,-0.333063577476883,0.977312470976463,0.678685792946892,1.25861755454376,0.424121489924674,1.79324076170629,0.294240135071998,-1.5158785674958,-0.566524647798012,0.0693937683248302,1.20366235962209,-0.549742457546945,0.120060059969118,0.307503184028818,1.2155918362671,0.125998694075395,-1.47214389956603,-0.318762097526322,0.279535230776256,2.70517688596651,1.68585793615506,1.12401712670926,0.424093564788534,0.104591454299854,-1.1803343663806,-0.9165578632289,-1.56216995140367,-0.851593623792724,-1.11860132721183,0.148466612255834,0.502630070090145,1.58769701225829,-1.10168452104253,-0.777557268527186,0.94297763713765,-1.93407049045019,0.00692057792853618,0.673501428642007,-1.39732872099199,0.28336236433147,0.874774846339632,-0.417592865434722,-1.36873507442414,-0.0747806381563088,0.0105670939898062,-0.0456454506980926,-1.47339031594463,1.3369923400225,1.94208753069958,0.284473239588879,-1.98482822470336,-0.500809398163266,-0.274658253565713,0.710968303939779,2.25847856696051,0.618289303483217,0.230944591563618,1.10131934305154,-0.572617841726025,-0.532996778264847,-0.717675343037506,-0.877046331783928,-0.748479521260477,0.184867023215944,-0.222414506563568,-1.00917489355167,-1.37048704727327,-1.07000231729438,-0.00663336055369079,0.00468660480072924,0.200654729342952,0.809368627472216,0.287257428583889,-0.30399518488168,0.139702775234314,0.127690399550364,-0.955663527846331,-0.800013385064343,-0.202922402465428,-2.41551072743718,0.375046038453612,2.09383587710961,0.605416442531965,-0.816271658677344,1.49356884637333,2.15381199342426,1.28064156968017,-1.168598311555,-0.495932350273373,-1.125713409468,-0.0288895742693098,-0.491227223720013,0.58247613764048,-1.7331639032851,-0.250684215939097,1.3778068059772,0.505232091444856,1.03344194804982,-0.148151960767482,-1.0201681040633,-1.16748356024614,-0.698289569016374,-0.290896524373165,-0.0873317580266177,2.28364441039829,0.799326310170926,0.49688121972365,-1.16765257942774,0.0559936855539941,0.0698015445148464,-0.63173073696285,-1.61583870723533,0.822540677698406,-0.585089658572308,-0.316985451621544,-0.445728454376432,-2.50837277206513,-1.39899267456777,-1.31102568256418,0.7504161406968,0.703408601835637,-0.25084213980174,-0.250931718918228,-0.872286941566416,0.628123948856479,-0.936167049851122,-0.550332514169838,-0.269581811595275,-0.688621863247731,-0.390548157583153,1.06988544057171,0.498809805707962,0.333414509511587,0.910620921461017,-1.27894154128712,-1.8417688154546,-0.511969510571442,-0.364093769403294,0.459335451829347,-0.0436102154757418,-0.453324557347184,2.02050507263822,1.47112945884264,-0.01150076205309,0.131772114413859,-2.28432527727909,0.669358831485658,-1.14605297558258,-1.91288142600314,-1.67214035243929,0.540739622447057,-0.256718780231093,-1.89131452765758,0.161941974508011,-0.458867473747127,-0.193982920139973,0.296794522358105,-0.0248717035830808,-1.05800715709841,0.244842455042338,-0.648886524970113,0.618878350387298,1.44658230968513,1.7651238305995,-1.92325119655138,0.202106893006015,-0.342513020363006,-0.120001239541129,0.266597248662135,0.721959385110193,0.403632345609537,-1.98751645656086,-0.78114948482437,0.987511150562872,0.308657764402252,0.10617926535495,0.185103579063779,-0.390073357767986,1.25428576795615,-1.35011219547237,-0.342969454539152,-0.232890312324374,0.980309810905929,-2.11327743840579,0.230304772156279,-1.45711508349211,1.38619773665506,1.64561503913805,0.886359250819342,0.296244989645419,-0.442950260031099,0.274643969106965,1.37404928864476,-1.61752327770182,-1.01358475029738,0.341567502478824,0.605615379863707,0.466413049443659,1.18787818614339,-0.998218282516348,0.472406493704238,0.802360667546239,0.77887146982322,1.33310984714731,-0.114831462913569,-1.24017202598153,1.68100590739297,-0.156333907840262,-0.361676092127597,1.12991666296382,0.544753678603083,-0.739512713902037,0.348336296506705,-0.104647388052216,-1.54853561567369,0.991697173919937,-0.712979705371183,-0.688954007152619,-0.0196266778239291,-0.427762346447314,0.485538124379978,-1.22765648631766,-0.0839608502125024,0.929455519210983,-1.04384646411627,1.58598957121594,-0.763090172449321,-0.048470581341153,0.482795985361566,0.2722317778418,-0.529927460041417,-2.32804557710721,0.0361073682628383,-0.451793590871513,-0.282264326599812,1.54913992182161,0.253377812105839,2.15913968342838,1.00793569459381,1.22392743399608,-0.262649821659892,0.371167166691489,1.84974230384505,-0.320662836221858,-0.484696685120263,0.119058094537279,0.31138939445094,2.86326145950913,0.561660116396446,-1.36784084165534,-0.521413318443954,1.84390621473116,0.15289559217166,-1.32920019262609,1.12332762670847,-0.0986781375953427,-0.781257016533019,1.13076098631033,0.767731998704861,0.525653835668498,0.582270365708574,0.332974322904491,-0.806319643281139,0.556406090906604,-1.30645395346996,0.784941553255689,-0.394255663470173,0.550288661010657,0.774277749074076,1.25766481539561,-1.59643991317207,0.726281103948247,-0.637004356852955,1.27487275247604,-0.0351556271008753,2.8916079358584,1.37494274194548,-0.0493800350743996,-0.0546139706884006,0.260425143915517,0.981866403207828,0.142788179240085,-0.254500581951128,-1.08393829993632,0.271928503984242,0.377553744140124,0.981267252018687,1.80754902543396,-1.1073733543378,0.482225506317209,1.81886240946361,-0.587533117915399,1.10937264859288,-0.297584327128313,2.57780345389995,2.52209598766427,-0.384831612054292,-0.855467063572686,0.938502054611698,-0.468165071625422,0.260001961733948,-0.463015512560662,-0.886240280794815,1.12421016229096,-0.910080616911732,-0.751136496148282,0.514392069233836,0.196135931220776,1.55467825939084,-0.253373978864419,-0.94429279814457,0.0654709941742517,0.669042865875214,-0.329938333369434,1.3014437111784,1.40097331408528,0.79622974608084,-0.10877819918252,1.29064490262136,-0.69326942351762,1.15340784227071,-1.67646410673975,-1.49427692853007,-1.27351657912146,1.31161127502502,-1.31181787527126,-0.95025150038739,0.144660911087558,-1.37567549067068,-0.229942296364386,0.419676338583079,0.516244299604568,0.740700053876904,-0.361858376494201,-1.61002671446548,0.327904602758787,1.64328297971566,1.20564911021619,-0.35352758276334,1.2147369565725,-1.53246532290859,1.92235481174216,-1.09255242419552,-1.59184466957563,-0.646442121336978,-0.426901372997513,0.209573020769323,0.04152495267983,1.33239795898585,-0.681002143601106,0.620588127452735,-0.900056295829705,0.30965958076666,0.11824576837806,0.227779209299425,-1.18925834469802,0.0837886195450821,0.588093918374064,-1.5092000511935,0.995963990416421,-0.00640702565372919,-0.350223166808981,1.42888382018494,-0.372328001827809,-1.86673663632025,1.73555986051252,1.44199292277243,-0.0079390840720599,1.30925868207325,0.000688096344961586,-1.45685228927633,-0.845075936663584,-0.604768685220023,-0.0517572381826596,0.821246209231971,1.50324445088074,0.26036891656549,0.521692863803707,-0.525250951588685,-1.07770081366461,-0.340590578094535,-0.0688542919744677,0.0247178375095101,1.76096935543214,0.175389864857628,0.13847784708959,-0.20602639910831,-0.435064679043989,0.529863878658708,0.967512772881289,-2.43105901782872,-0.0425199340527227,-1.99866967085235,-2.47551155954987,-0.965082220610995,-0.435002851149399,-0.5071236176341,-1.37597284240595,0.641369841232226,0.150751328397378,0.647535891296364,-1.2146116131246,1.46931446401003,-1.78029882732534,0.39145486509522,0.954031408084298,1.27980677379113,0.263434724733113,-1.03876745265004,-0.962508057624398,0.895370500046994,0.905874674107615],[500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>y<\/th>\n      <th>y_pred<\/th>\n      <th>iter<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":25,"columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script><img src="rtorch-minimal-book_files/figure-html/dt-500-iters-2.png" width="70%" style="display: block; margin: auto;" />
### Complete code for neural network in rTorch</p>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rTorch)</span>
<span id="cb529-2"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb529-3"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tictoc)</span>
<span id="cb529-4"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb529-5"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-5" aria-hidden="true" tabindex="-1"></a><span class="fu">tic</span>()</span>
<span id="cb529-6"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-6" aria-hidden="true" tabindex="-1"></a>device <span class="ot">=</span> torch<span class="sc">$</span><span class="fu">device</span>(<span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb529-7"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-7" aria-hidden="true" tabindex="-1"></a><span class="co"># device = torch.device(&#39;cuda&#39;)  # Uncomment this to run on GPU</span></span>
<span id="cb529-8"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-8" aria-hidden="true" tabindex="-1"></a><span class="fu">invisible</span>(torch<span class="sc">$</span><span class="fu">manual_seed</span>(<span class="dv">0</span>))</span>
<span id="cb529-9"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb529-10"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Properties of tensors and neural network</span></span>
<span id="cb529-11"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-11" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> 64L; D_in <span class="ot">&lt;-</span> 1000L; H <span class="ot">&lt;-</span> 100L; D_out <span class="ot">&lt;-</span> 10L</span>
<span id="cb529-12"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb529-13"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random Tensors to hold inputs and outputs</span></span>
<span id="cb529-14"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-14" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(N, D_in, <span class="at">device=</span>device)</span>
<span id="cb529-15"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-15" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(N, D_out, <span class="at">device=</span>device)</span>
<span id="cb529-16"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-16" aria-hidden="true" tabindex="-1"></a><span class="co"># dimensions of both tensors</span></span>
<span id="cb529-17"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb529-18"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-18" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the weights</span></span>
<span id="cb529-19"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-19" aria-hidden="true" tabindex="-1"></a>w1 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(D_in, H, <span class="at">device=</span>device)   <span class="co"># layer 1</span></span>
<span id="cb529-20"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-20" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">randn</span>(H, D_out, <span class="at">device=</span>device)  <span class="co"># layer 2</span></span>
<span id="cb529-21"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb529-22"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-22" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="ot">=</span> <span class="fl">1e-6</span></span>
<span id="cb529-23"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-23" aria-hidden="true" tabindex="-1"></a><span class="co"># loop</span></span>
<span id="cb529-24"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">500</span>) {</span>
<span id="cb529-25"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Forward pass: compute predicted y, y_pred</span></span>
<span id="cb529-26"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-26" aria-hidden="true" tabindex="-1"></a>  h <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">mm</span>(w1)              <span class="co"># matrix multiplication, x*w1</span></span>
<span id="cb529-27"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-27" aria-hidden="true" tabindex="-1"></a>  h_relu <span class="ot">&lt;-</span> h<span class="sc">$</span><span class="fu">clamp</span>(<span class="at">min=</span><span class="dv">0</span>)   <span class="co"># make elements greater than zero</span></span>
<span id="cb529-28"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-28" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">mm</span>(w2)    <span class="co"># matrix multiplication, h_relu*w2</span></span>
<span id="cb529-29"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb529-30"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-30" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor</span></span>
<span id="cb529-31"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-31" aria-hidden="true" tabindex="-1"></a>  <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></span>
<span id="cb529-32"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-32" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">&lt;-</span> (torch<span class="sc">$</span><span class="fu">sub</span>(y_pred, y))<span class="sc">$</span><span class="fu">pow</span>(<span class="dv">2</span>)<span class="sc">$</span><span class="fu">sum</span>()   <span class="co"># sum((y_pred-y)^2)</span></span>
<span id="cb529-33"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-33" aria-hidden="true" tabindex="-1"></a>  <span class="co"># cat(t, &quot;\t&quot;)</span></span>
<span id="cb529-34"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># cat(loss$item(), &quot;\n&quot;)</span></span>
<span id="cb529-35"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb529-36"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-36" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span>
<span id="cb529-37"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-37" aria-hidden="true" tabindex="-1"></a>  grad_y_pred <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">mul</span>(torch<span class="sc">$</span><span class="fu">scalar_tensor</span>(<span class="fl">2.0</span>), torch<span class="sc">$</span><span class="fu">sub</span>(y_pred, y))</span>
<span id="cb529-38"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-38" aria-hidden="true" tabindex="-1"></a>  grad_w2 <span class="ot">&lt;-</span> h_relu<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_y_pred)        <span class="co"># compute gradient of w2</span></span>
<span id="cb529-39"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-39" aria-hidden="true" tabindex="-1"></a>  grad_h_relu <span class="ot">&lt;-</span> grad_y_pred<span class="sc">$</span><span class="fu">mm</span>(w2<span class="sc">$</span><span class="fu">t</span>())</span>
<span id="cb529-40"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-40" aria-hidden="true" tabindex="-1"></a>  grad_h <span class="ot">&lt;-</span> grad_h_relu<span class="sc">$</span><span class="fu">clone</span>()</span>
<span id="cb529-41"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-41" aria-hidden="true" tabindex="-1"></a>  mask <span class="ot">&lt;-</span> grad_h<span class="sc">$</span><span class="fu">lt</span>(<span class="dv">0</span>)                         <span class="co"># filter values lower than zero </span></span>
<span id="cb529-42"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-42" aria-hidden="true" tabindex="-1"></a>  torch<span class="sc">$</span><span class="fu">masked_select</span>(grad_h, mask)<span class="sc">$</span><span class="fu">fill_</span>(<span class="fl">0.0</span>) <span class="co"># make them equal to zero</span></span>
<span id="cb529-43"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-43" aria-hidden="true" tabindex="-1"></a>  grad_w1 <span class="ot">&lt;-</span> x<span class="sc">$</span><span class="fu">t</span>()<span class="sc">$</span><span class="fu">mm</span>(grad_h)                  <span class="co"># compute gradient of w1</span></span>
<span id="cb529-44"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-44" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb529-45"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-45" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update weights using gradient descent</span></span>
<span id="cb529-46"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-46" aria-hidden="true" tabindex="-1"></a>  w1 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">sub</span>(w1, torch<span class="sc">$</span><span class="fu">mul</span>(learning_rate, grad_w1))</span>
<span id="cb529-47"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-47" aria-hidden="true" tabindex="-1"></a>  w2 <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">sub</span>(w2, torch<span class="sc">$</span><span class="fu">mul</span>(learning_rate, grad_w2))</span>
<span id="cb529-48"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-48" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb529-49"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-49" aria-hidden="true" tabindex="-1"></a><span class="co"># y vs predicted y</span></span>
<span id="cb529-50"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-50" aria-hidden="true" tabindex="-1"></a>df<span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> y<span class="sc">$</span><span class="fu">flatten</span>()<span class="sc">$</span><span class="fu">numpy</span>(), </span>
<span id="cb529-51"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-51" aria-hidden="true" tabindex="-1"></a>                    <span class="at">y_pred =</span> y_pred<span class="sc">$</span><span class="fu">flatten</span>()<span class="sc">$</span><span class="fu">numpy</span>(), <span class="at">iter =</span> <span class="dv">500</span>)</span>
<span id="cb529-52"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-52" aria-hidden="true" tabindex="-1"></a><span class="fu">datatable</span>(df, <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">25</span>))</span>
<span id="cb529-53"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-53" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> y_pred, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb529-54"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-54" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>()</span>
<span id="cb529-55"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb529-56"><a href="neural-networks-using-numpy-r-base-rtorch-and-pytorch.html#cb529-56" aria-hidden="true" tabindex="-1"></a><span class="fu">toc</span>()</span></code></pre></div>
<div id="htmlwidget-fff8ed89c1ad8f04e039" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-fff8ed89c1ad8f04e039">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640"],[0.83994026831249,-0.00261469246496478,1.17331508976272,0.113064550776486,0.0103083101232198,0.741725300575136,0.757130556967624,-1.19798639091765,0.977469466953348,0.958784411472249,0.0208831428895932,-1.64816158891565,-0.0500028243495128,0.733813527145886,1.58085339989783,1.37525327872787,-0.386739371622718,-0.360212335648884,0.757263768677633,-2.33097748052543,-0.280231384151112,-0.958825812111196,-0.0248397757648495,1.0914318283759,-0.458192050288552,1.88579895675686,1.8374918101401,-0.102590503582433,0.158376834397865,-1.69505820821086,0.0630930916536182,1.3418620933296,0.173417858163622,-0.972149178523607,-0.312413823093578,2.16446071258402,-1.08569798526669,-0.570029361135791,0.180937256764405,-1.26229551939497,-1.36950465873129,-1.15493391741737,1.40785825831085,0.781023621423221,0.0780186960888742,2.11273169799433,-0.382805598728771,-0.332249209115966,0.833881240069712,0.59587340242551,-1.10900749622674,0.768900089169295,-1.35001396009045,0.483618658476077,-0.286807646207213,0.141828938278373,-0.374918514638461,-0.231434565088581,-0.00992419102991136,-2.11812756250465,-0.523762933650836,-1.32458158157352,0.431681037219444,0.925057143386304,0.93777525245556,0.546488768273407,1.11596946687424,0.570940155822875,-1.05140854030057,-0.318810634218592,-0.289448897409276,-0.418406412091964,0.781141791150043,-0.114891413907268,-0.845416526254121,-0.91845936893655,1.20789854386747,0.741312355103571,0.634427251618938,2.20585574787314,0.356161490453256,0.267242739519324,0.484831528360182,-0.165738361704601,0.189930678043718,-0.93651989042175,-1.45410391289995,0.233684800337273,0.502157215971716,0.28920149072855,-1.22551276737874,-0.385846452553303,1.42231442741027,-1.7116822994054,0.155053435413657,1.75487195590539,-0.480695737266194,-0.744335584259619,-2.13245053322139,-0.364322343399947,-0.233973774563064,0.111681118770621,-0.88914370304214,-0.722085375742162,-2.32991274785241,-0.479068930295669,0.789931238294979,1.06380136135981,-0.0798134682221333,-0.374168001227348,0.122704935131071,-1.43690716269235,-0.853944502445023,0.456289787735743,-0.0142516296486563,0.816751922214445,0.0849881110641102,0.446428132382165,2.42338976222122,-0.246995607668513,0.276176450241468,0.930116509469416,-1.02544718711831,0.104887979909664,-0.0142328430202254,2.03937753638296,0.373422932290684,1.43684984339667,0.948776626114924,-1.31827952307809,0.292905851185605,0.147308597580878,-1.03373092754513,0.419607277988786,-0.261092633509276,1.6071495149865,-0.647906145700861,0.657156199635267,-0.57098265859795,-0.818600322951857,1.31069089175351,1.81375842080092,-0.943433957369921,0.857269379202826,-0.263283789177259,1.31296346802011,-0.955590491841724,-1.56443546642085,-2.17226562414852,1.45591099037169,1.03814194129823,-0.197772383407226,-0.650297223745001,-1.33005360476313,0.921942504799078,0.752209085763665,-0.0361295533712819,2.63443405115608,0.287654631188146,0.655186314323695,0.400648171873719,-0.307311020383442,-0.0880147158656964,-1.41821667150147,-0.313515957169452,1.02977246630598,1.63557919370632,0.471406593501004,0.996474278664091,0.682501811766753,-0.971734007624617,-0.654540506542262,1.99500285811451,-1.64399263553111,-0.663557865235524,-1.68572596581282,-0.00487618116822535,0.458801236543731,0.5824022309138,-1.36287855122539,-1.27186820648675,0.714902868782501,-1.0165666698022,-0.0235973711331876,-0.0710369673541219,0.142253138818817,-0.675694865931503,-0.256829936236055,-0.101224481444322,-2.15395839517849,-0.964817551233336,-0.58900570028774,-1.77786400328839,1.14147095872602,-0.32772078240833,-1.35670287649227,0.415691882771873,-0.925932387905733,-1.61418322141891,-0.0867796005444939,0.601846649732015,1.12863379512717,0.184978687339329,0.156848912253547,-0.484058595314498,1.12522934475228,0.243459583230952,-0.634978878232694,-1.25090253023824,-1.43563909830578,1.13586385748573,0.215411392403429,1.86239774513426,1.11499996333691,-0.674626192034018,-0.434131329001274,1.03699553226858,-0.0366741372856878,-0.402271366973104,2.10246137335328,0.459018936796291,-0.25503953202401,-0.672610058718924,1.09980827026172,-0.410925835278326,-0.896790320270794,-1.03109344053961,-0.262748015975908,-0.333069074621615,0.977294210489194,0.678715733607743,1.25873526914719,0.42400531072313,1.79340828861104,0.294284737340495,-1.51557096175814,-0.566556739572426,0.0695601296483909,1.20367672097015,-0.549833339055226,0.120064475138402,0.307536795611604,1.21556005041361,0.12602924786391,-1.47214013475447,-0.318687822897892,0.279546357353257,2.70523892501022,1.68587064051207,1.1239941108417,0.424088292374247,0.104431077381569,-1.18021245837732,-0.916706926774142,-1.56221497392067,-0.851852947077733,-1.11863086095433,0.148312190622533,0.502631432428189,1.58774800017961,-1.10166271639973,-0.777449867610656,0.942870568479463,-1.9339574431339,0.00698050744702491,0.673684269774716,-1.39732869525216,0.283501034389694,0.874755970337202,-0.417645129576752,-1.36874292450096,-0.0747812970509281,0.0105724695173432,-0.0456309112554566,-1.47339841259021,1.33700556433092,1.94209421947211,0.284457070328279,-1.98482664413377,-0.50080737939018,-0.274676881900125,0.710938213804541,2.25851536328219,0.618237686663071,0.230927765626917,1.10122470908834,-0.572603422146998,-0.53302471140162,-0.717679874784871,-0.877038791682033,-0.748485888179382,0.184771684002625,-0.22231743531996,-1.00929695552901,-1.37047063999704,-1.07025776684948,-0.0066305826821924,0.00463777722911298,0.200669539977365,0.809383668396162,0.287278665935646,-0.303985246895298,0.139672450093511,0.127722862850529,-0.95565825905712,-0.799909006767028,-0.202940490973971,-2.41551300196281,0.3750531988884,2.0938362702894,0.605430623566889,-0.816263936823711,1.4935718662945,2.15379218446882,1.28063423391778,-1.16859150067533,-0.495931078057793,-1.12572347427127,-0.0289018780637711,-0.491217346285856,0.582465462265955,-1.73320910486715,-0.250616430567373,1.37773646638411,0.505183750595998,1.03329001588251,-0.148129803407604,-1.02024054280369,-1.16747702799092,-0.698262649012394,-0.29088420462642,-0.0873481684419898,2.28367061778488,0.799274120229441,0.496859660507907,-1.16771232655097,0.0560084744790094,0.0697664131694416,-0.63172486335937,-1.61579250963407,0.822541288820032,-0.585096534350738,-0.316981493648855,-0.445729843878749,-2.50838624530302,-1.39899463359675,-1.31102892051884,0.750402144358772,0.703404121659897,-0.25083983221813,-0.25095095379119,-0.872342684631607,0.628192536989848,-0.936217675649455,-0.550371466231215,-0.26970983693769,-0.688604841006888,-0.390605966140334,1.06988684039523,0.498849800595456,0.333419121051892,0.910556713584374,-1.27892253213922,-1.84182185333004,-0.512035156980371,-0.364119099300724,0.459286557995962,-0.0437559190006266,-0.453340737020902,2.02055656551793,1.47116567306761,-0.0115038122635589,0.131722917926454,-2.28429446135232,0.669356739346067,-1.14593193034066,-1.91291968267857,-1.67212741707599,0.54073309384294,-0.256724509418031,-1.89134380794797,0.161915204911539,-0.458809181262073,-0.194078077908355,0.296793193074685,-0.0250807732722297,-1.05794861180844,0.24484382118802,-0.648883034347149,0.618876699768757,1.44663045545316,1.76518871577172,-1.92334252201028,0.202212741132606,-0.342486128124399,-0.119772048017177,0.266562321266478,0.722029306985646,0.403631540478367,-1.9875531289521,-0.781171921052648,0.987488573315664,0.308690446320804,0.106148749253679,0.185085607568157,-0.390160876288962,1.25429513802065,-1.35013386465017,-0.342974766571647,-0.232894537629491,0.980332519980097,-2.11322741735,0.23024995771627,-1.45705269393795,1.38625292848739,1.6457152752694,0.886358263746316,0.296335125360452,-0.442944664529715,0.274627958703436,1.37404495680148,-1.61754075111507,-1.01356309656133,0.34151499813101,0.60561927302074,0.466358958410998,1.18788000370773,-0.998222009747409,0.472411048704897,0.802363134564957,0.778863518671328,1.33312992214265,-0.11483784787192,-1.24014458172761,1.68097845650274,-0.156295579276488,-0.36167688476951,1.12990338986994,0.544741774675208,-0.739519141431302,0.348344512782951,-0.10457968657704,-1.54862790298088,0.991859873430099,-0.712951101315977,-0.688713554688238,-0.019645395217261,-0.4276668324237,0.485554217337533,-1.22769785113341,-0.0839624525226322,0.929646496319533,-1.04399669558851,1.5862211735349,-0.762919277781499,-0.0482214954780706,0.482863741201961,0.272647805689435,-0.529870425645623,-2.32824908375099,0.0361073596021704,-0.451810115323141,-0.282254053923175,1.54911580277808,0.253374436546217,2.1590870111341,1.00793354498743,1.2238963859792,-0.262656713944417,0.371188568837448,1.84975076615656,-0.32057748245843,-0.484757190293574,0.119115296354525,0.311425735299618,2.86340591290978,0.561681683625529,-1.36773128004884,-0.521412125803238,1.8438836271771,0.15291957051073,-1.32938108961612,1.12345625118363,-0.0988379062841592,-0.781398448382246,1.13052436749178,0.767681931298354,0.525356584083132,0.582232899579348,0.333066984200484,-0.806258638926797,0.556552894435339,-1.30665318755998,0.785186326876701,-0.394106214647196,0.550730858543516,0.774230360594257,1.25782444024924,-1.59645579014647,0.726215347670778,-0.63694860577194,1.27493610125473,-0.0352439186929309,2.89168261447142,1.37501515427315,-0.0491456451207457,-0.0546139127595145,0.260510410587632,0.981880273079814,0.14277683851885,-0.254490023068397,-1.08396906594294,0.271948629915079,0.377547175371029,0.981224651122629,1.80754059091588,-1.1073823023655,0.482139414365283,1.8188538137154,-0.58749687491936,1.1093609884837,-0.297608176884006,2.57784620418014,2.52204599693865,-0.384840198277502,-0.855562334499144,0.938512269689899,-0.468196363866475,0.260008292451711,-0.462982716485726,-0.886275855262118,1.12411879590505,-0.909935517062699,-0.751334836930073,0.514366756631972,0.195860128347231,1.55471787782953,-0.253467892483979,-0.944270533093175,0.0655037556538774,0.669046506691992,-0.329793629158233,1.30135750702321,1.40114405750616,0.796269800703752,-0.10853585230126,1.29063556898416,-0.693149828347284,1.15340154198528,-1.67650319533252,-1.49429593747768,-1.273469908539,1.31161289723836,-1.31181443286064,-0.950226359402931,0.144663430311268,-1.37563897316271,-0.229895425421889,0.419692024617746,0.516239563506011,0.740669495728343,-0.361952001881317,-1.60995844987436,0.327757025857471,1.6432950871785,1.20535256388868,-0.353510280844418,1.21473389654699,-1.53240226188522,1.92233286176372,-1.09253954985412,-1.59184155325035,-0.64646780319668,-0.426883748541837,0.209569862271875,0.0415756254512372,1.33238165750511,-0.681027400604152,0.620571329940411,-0.900045782913094,0.309651318876625,0.118175407084332,0.227812270228395,-1.18930438569969,0.0837636991730383,0.588009163095429,-1.5092271780105,0.995898635439402,-0.00639701815793872,-0.3502219106773,1.4288096764155,-0.372525850460849,-1.86657115947184,1.73533502830238,1.4419101995773,-0.0083570684921407,1.3092622868551,0.000392255402139403,-1.45683485419048,-0.844996213277658,-0.60477119564568,-0.0517937465881769,0.821273104631407,1.50318458611856,0.26033919453097,0.521628843921831,-0.525259906808531,-1.07777898882352,-0.340595705086819,-0.0688224810312909,0.024696280077251,1.76093398424226,0.175448866859079,0.138406693673293,-0.206063763800319,-0.435211393481228,0.529893499013784,0.967447102676697,-2.43106301578848,-0.0424933305199116,-1.99868317055989,-2.47557651859656,-0.965041073936391,-0.435050040400478,-0.507170912986636,-1.37604070291559,0.641342899780833,0.15067215935219,0.647532140383024,-1.21460078326391,1.46931171546606,-1.78034791140857,0.39149639637274,0.953942227218803,1.27980367071278,0.263325295304111,-1.03877191179501,-0.962563912956559,0.895371173457089,0.905913690706474],[0.839913548534668,-0.00258787236808966,1.17326445801632,0.113034397129909,0.010328938807467,0.741709200645734,0.75713292749352,-1.19798986278421,0.977424502606979,0.958759958483093,0.0208846818971526,-1.6481658577161,-0.0500004390890787,0.733816852493233,1.58085213001206,1.3752564569637,-0.386738162865882,-0.360215028043378,0.757266805188382,-2.33097535807321,-0.28023002988667,-0.958830048080303,-0.0248477772413764,1.09143844152394,-0.458191098754063,1.8858015907083,1.83749124865544,-0.102590480057926,0.158385539801725,-1.69506037342019,0.0630930597667239,1.34186915986899,0.173401451016703,-0.972151247997491,-0.312406983890389,2.16445910566336,-1.08569648917483,-0.570032496333382,0.180933484091626,-1.26230626089632,-1.36951161404477,-1.15492919879107,1.40783665355661,0.781010468073415,0.0780261508670321,2.11272771306668,-0.38280530735894,-0.332246486328671,0.833863022450537,0.595855523401313,-1.1090060035254,0.76890199735417,-1.35001243519999,0.483625347936903,-0.286809284722636,0.141831584335796,-0.374917025431081,-0.23143044686384,-0.00991853679930443,-2.11812429572775,-0.523764095821917,-1.32457803271954,0.431671010964227,0.925051243483281,0.937776607544039,0.546485720184892,1.1159688146404,0.570937983742568,-1.05141552814772,-0.318821841353875,-0.289456255022322,-0.4184026393837,0.781124097674851,-0.114906296443308,-0.845404384973961,-0.918465471100732,1.20790470730208,0.741312997321136,0.634404306680473,2.205842368697,0.356162081891902,0.267242278476639,0.484838848543364,-0.165738090960944,0.189929774045608,-0.936516525117055,-1.45410274136053,0.233684464459277,0.502156743078876,0.289208616904266,-1.22550813594229,-0.385848379847179,1.42231896323353,-1.71167735796408,0.155048857100081,1.75487385402495,-0.480696894815592,-0.744334523387456,-2.13244313948325,-0.3643217812938,-0.233967330641707,0.111675905130696,-0.889138956841418,-0.722070922038786,-2.32992419947619,-0.479068847618944,0.789926395227255,1.06381036870756,-0.0797929029471547,-0.374168305978094,0.122702902704463,-1.43690383100722,-0.853930119491674,0.456289726006624,-0.0142525365281561,0.816753534096156,0.0849888556965208,0.446430305360734,2.42338636157487,-0.246984838156465,0.276169461709885,0.930122372557461,-1.02544982457473,0.104880687397594,-0.0142312370977411,2.03937264357409,0.373424382220172,1.43685159890974,0.948761150596347,-1.31828474448697,0.292904130835028,0.147310228755906,-1.03372595847936,0.419603741978923,-0.261093121762922,1.60715082985328,-0.647905446165803,0.65715537742425,-0.570988478403955,-0.818596896313514,1.3106941623935,1.81375876390488,-0.943422330956501,0.857272464644548,-0.263285825779747,1.31296898137995,-0.955589001636454,-1.56443656629871,-2.1722654036098,1.45592051729529,1.03813591475316,-0.197771438394895,-0.650295567069932,-1.33006295483275,0.921944983058134,0.752202862242171,-0.0361287489137858,2.63443160026457,0.287643664899418,0.655181237804673,0.400658255256705,-0.307315592426832,-0.0880020162674207,-1.41820842579359,-0.313520965017965,1.02978138407386,1.6355802991561,0.471404546107262,0.996484129767346,0.682511347453164,-0.971725498883337,-0.654539503518322,1.99500251452466,-1.64398286961812,-0.663561255120868,-1.68572154256467,-0.00487921401536084,0.458799670689835,0.582414944271797,-1.36287408730987,-1.2718687879881,0.714903784373574,-1.0165711977177,-0.0235987531965408,-0.0710377937191002,0.142251252286172,-0.67569762895276,-0.256832712054913,-0.101223100339618,-2.15396211870299,-0.964814528789441,-0.589003305735094,-1.77785861873181,1.14147638281117,-0.327726590960356,-1.35670310282624,0.415689635465484,-0.925931482618541,-1.61417560726602,-0.0867774320988883,0.60184194841026,1.12863900411943,0.18496591403978,0.156829593061375,-0.484045842753463,1.12522180597565,0.243466571444683,-0.634979653702819,-1.25093404891322,-1.43564618623987,1.13585336844637,0.215408789952281,1.86236861457401,1.11499354896324,-0.674613846505965,-0.434136066892964,1.03700516696678,-0.036677482822209,-0.402283642930383,2.1024476112658,0.459017388966596,-0.255037508111938,-0.672615662271051,1.0998027784206,-0.410916567337178,-0.896789043866147,-1.03108966087779,-0.262753459440319,-0.333082354155625,0.977294566989163,0.678714711203675,1.25873453260772,0.424005522012035,1.79340489830836,0.294283590390465,-1.51557225888784,-0.566557400404079,0.0695595320407776,1.20367387036261,-0.549836130921607,0.120068214856592,0.30753275802381,1.2155817107878,0.126038358748893,-1.47214585638461,-0.318681071889028,0.279543831147098,2.70523537700077,1.68588710316787,1.12400715424794,0.424093764181939,0.10442880364952,-1.18019086680141,-0.916700680556474,-1.56222370703829,-0.851848148224604,-1.11863407938302,0.148310456516264,0.502641050616882,1.58775842779805,-1.10166347373622,-0.777452787892943,0.942871283028801,-1.93395839596361,0.00697944165844788,0.673683215845369,-1.39733008377906,0.28350004124766,0.874756033074307,-0.417645089759688,-1.36875084039565,-0.0747718249421396,0.0106049251341811,-0.0456341952233998,-1.47340356794433,1.33700892029225,1.94210054046657,0.284455741581302,-1.98483760421212,-0.500795846017335,-0.274682319105488,0.710942799998598,2.25852199311311,0.618225026814601,0.230930399839437,1.10121678494004,-0.572604528413567,-0.533027381020736,-0.717696520980224,-0.877043217358511,-0.748486759792479,0.184772917070582,-0.22231942857448,-1.00929671904296,-1.37046752324559,-1.07025398865628,-0.00662872305179008,0.00463523595045956,0.200669464687206,0.809387804591389,0.287274407172283,-0.303984227284829,0.139665046438317,0.127719663743008,-0.95565223441817,-0.799911666563027,-0.202937397192109,-2.41551123775871,0.375043826421404,2.09383508757913,0.60543304047042,-0.816265554773587,1.4935701721758,2.15379401998208,1.2806340218698,-1.16859068601274,-0.495931213746147,-1.12572393407459,-0.0288990953424459,-0.491218887632103,0.582466898056829,-1.73320797759801,-0.250618799517072,1.37773567984023,0.505184915032987,1.03329019729845,-0.148130245792603,-1.02023949078417,-1.16747825427844,-0.698265878861688,-0.290890251105622,-0.0873443690020679,2.28366964719601,0.799259902997782,0.496864026670281,-1.16772217760357,0.05600698897603,0.0697647068177076,-0.631742146486282,-1.61580207222078,0.822533939087455,-0.585100333152734,-0.316977825658504,-0.445738166502278,-2.50838168081985,-1.39899971856609,-1.31102665972544,0.750397787402582,0.703393597871957,-0.250840219213753,-0.250940581783027,-0.872349651947381,0.628185681743929,-0.936203403246118,-0.550382609936751,-0.269708287909124,-0.68861706029858,-0.390596633689495,1.06991737625447,0.498842879413495,0.333423687411573,0.910547593909932,-1.27890538706654,-1.84181643416853,-0.512044551417682,-0.364112159415031,0.459284544496157,-0.0437528247342307,-0.453330370418374,2.02056940972739,1.47116553299097,-0.0115037727679237,0.131720235939468,-2.28429666643908,0.669357358143358,-1.14593428072276,-1.91292028921364,-1.672127371169,0.540730537935232,-0.256728217228571,-1.89135458860763,0.161923713751871,-0.458808944512854,-0.194093116240806,0.296799448278593,-0.0250858760455768,-1.05794672496183,0.244840777016065,-0.648903675172578,0.61887903253417,1.44662866767928,1.76518794652974,-1.92336446086523,0.202209903821851,-0.342476330359001,-0.119777298093467,0.266562781211624,0.722029223647143,0.403626352160515,-1.9875625444099,-0.781176360609118,0.987489122011464,0.308687680324815,0.106141910775847,0.185084946493907,-0.390167832722437,1.25429249183016,-1.35013264792245,-0.342979977151109,-0.232904733402687,0.98032670528168,-2.11323168344834,0.2302247673042,-1.457056169633,1.38624905755793,1.64569692457825,0.886346595917022,0.296336756898135,-0.442929160021663,0.274596538405559,1.37404124355859,-1.6175387305115,-1.01356474565786,0.341505228104672,0.60562524011408,0.466356758396008,1.18788375907902,-0.998226760232953,0.472396053400725,0.802360980598435,0.778870509748275,1.33312667840589,-0.114829350867559,-1.2401348175676,1.68097525412256,-0.156290277464135,-0.361678172554623,1.12990487232395,0.544755032051736,-0.739508991099848,0.34834865237723,-0.104580869913597,-1.54862817948104,0.991866758909846,-0.712953474082072,-0.688709193734127,-0.0196449015765126,-0.42766527306355,0.485561441432814,-1.22769491474389,-0.0839716252514848,0.929638197277135,-1.04402085300721,1.58621339759573,-0.762914884054146,-0.0482290784718753,0.482862627045384,0.27264159614958,-0.529875532795576,-2.32826675978261,0.0360806462079776,-0.451807231003009,-0.282231654136684,1.54909094666738,0.253376396487742,2.15907357038282,1.00792995060731,1.22389204939901,-0.26267716802634,0.371196789466661,1.8497500709629,-0.320579183683798,-0.4847599348631,0.1191144281714,0.311427180205289,2.86340400064894,0.561681519334819,-1.36773194830541,-0.521412008639965,1.84388150572375,0.15293001502948,-1.32938425266374,1.12348191779453,-0.0988281829602115,-0.781407776919397,1.13052377656241,0.767679292536787,0.525366170719711,0.582247007045441,0.333078237828323,-0.806253913669181,0.556553286579042,-1.30663823842805,0.785192805832571,-0.394109488179521,0.550736017122615,0.774229123013747,1.25782733087736,-1.59644565487891,0.726224487026762,-0.636943032494626,1.27493628996675,-0.0352351722542199,2.89168569913248,1.37501391394626,-0.0491438610424562,-0.054614067546243,0.260508947371091,0.9818838776426,0.142782001641322,-0.254494432136232,-1.08397043676275,0.27194809668201,0.377545011206553,0.981224642452629,1.80753970911447,-1.10738323711928,0.48213663924399,1.81885388374148,-0.587497748439374,1.1093648371698,-0.297611822548109,2.57785388240676,2.52205378711065,-0.384846074279068,-0.855558925112117,0.938509156539961,-0.468194320619521,0.260022144909609,-0.462975371184964,-0.886267511491504,1.12411855861377,-0.90992256021266,-0.751326205215332,0.514361263713338,0.195867899225614,1.55471855901034,-0.253468613047414,-0.944263147887314,0.065512711573934,0.669044246503518,-0.329791729755158,1.30135958344473,1.40114612154416,0.796276147217625,-0.108529644667417,1.29064147481605,-0.693154016271812,1.15339541925391,-1.67649515209393,-1.49428576345616,-1.27346874329405,1.31160765947391,-1.31180474649495,-0.950227745203591,0.144669151196315,-1.37564027921596,-0.229892773641041,0.419701736431808,0.516244042092023,0.740671343410696,-0.361956297908675,-1.60995829124144,0.327759850618029,1.64329466122512,1.2053527313696,-0.353511712988163,1.21473502414503,-1.53239547879139,1.92233372338813,-1.09254143389673,-1.59184176797819,-0.646473071142122,-0.426887134080009,0.20957061477219,0.0415732187868088,1.33238173337418,-0.681027767797282,0.620569757615325,-0.900051139392668,0.309648727693022,0.118178476848846,0.227812209133809,-1.18930550256927,0.0837659171870506,0.588010688346111,-1.50922510046875,0.995898384711847,-0.00640223530745254,-0.350218769042258,1.42880693783247,-0.372527085751725,-1.86659338567864,1.73532706858438,1.44191930899079,-0.008357469286707,1.30926540567799,0.000390243507029098,-1.45684440826658,-0.845006655460494,-0.604761649223272,-0.0517938159756923,0.821263567701497,1.50319776681079,0.260336711233325,0.521630241549286,-0.525261686924445,-1.07777290186398,-0.340576770530518,-0.0688274200403303,0.0247002446929292,1.760933436675,0.175464687010913,0.138406627774067,-0.206069204767029,-0.435208818651325,0.529894842249567,0.96744727837318,-2.43106242232854,-0.0424841372214984,-1.99866320136773,-2.47559335223406,-0.965022431354473,-0.435006295305131,-0.507186604629286,-1.37601239819389,0.6413491210446,0.150683139854012,0.647583793686788,-1.21456182324428,1.46932275202168,-1.7803481970684,0.39148735188148,0.953955916805724,1.27979659112732,0.263329463192063,-1.03877570143665,-0.962560646370184,0.895389477400096,0.905910672428008],[500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>y<\/th>\n      <th>y_pred<\/th>\n      <th>iter<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":25,"columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<pre><code>#&gt; 11.503 sec elapsed</code></pre>
<p><img src="rtorch-minimal-book_files/figure-html/rotch-complete-2.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="exercise-2" class="section level2" number="12.5">
<h2><span class="header-section-number">12.5</span> Exercise</h2>
<ol style="list-style-type: decimal">
<li><p>Rewrite the code in <code>rTorch</code> but including and plotting the loss at each iteration</p></li>
<li><p>On the neural network written in <code>PyTorch</code>, code, instead of printing a long table, print the table by pages that we could navigate using vertical and horizontal bars. Tip: read the PyThon data structure from R and plot it with <code>ggplot2</code></p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="rainfall-prediction-with-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-step-by-step-neural-network-in-rtorch.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
